<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[一个不认识的进程]]></title>
    <url>%2F2019%2F06%2F08%2F%E4%B8%80%E4%B8%AA%E4%B8%8D%E8%AE%A4%E8%AF%86%E7%9A%84%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[今天登陆服务器，看到了一个不认识的进程，root 8581 1 0 Feb14 ? 00:33:01 /usr/bin/python2 -Es /usr/sbin/tuned -l -P 这是个啥，google了一下，好像是跟性能优化有关的进程。 Tuned本质上是一个Linux环境中的后台进程，在运行过程中依据配置内容监控调整系统。监控程序会根据监控的配置内容，监控进程会持续监控结果反馈，调整优化策略。 查看下配置文件[root@data9 tuned]# cat /etc/tuned/tuned-main.conf 12345678910111213141516171819202122232425262728293031323334# Global tuned configuration file.# Whether to use daemon. Without daemon it just applies tuning. It is# not recommended, because many functions don't work without daemon,# e.g. there will be no D-Bus, no rollback of settings, no hotplug,# no dynamic tuning, ...daemon = 1# Dynamicaly tune devices, if disabled only static tuning will be used.dynamic_tuning = 0# How long to sleep before checking for events (in seconds)# higher number means lower overhead but longer response time.sleep_interval = 1# Update interval for dynamic tunings (in seconds).# It must be multiply of the sleep_interval.update_interval = 10# Recommend functionality, if disabled "recommend" command will be not# available in CLI, daemon will not parse recommend.conf but will return# one hardcoded profile (by default "balanced").recommend_command = 1# Whether to reapply sysctl from the e.g /etc/sysctl.conf, /etc/sysctl.d, ...# If enabled these sysctls will be re-appliead after Tuned sysctls are# applied, i.e. Tuned sysctls will not override system sysctls.reapply_sysctl = 1# Default priority assigned to instancesdefault_instance_priority = 0# Udev buffer sizeudev_buffer_size = 1MB 管理tuned-adm命令是管理tuned组件的主要接口。通过list命令，可以查看当前应用和备选的所有Profile内容。123456789101112[root@data9 tuned]# tuned-adm listAvailable profiles:- balanced - General non-specialized tuned profile- desktop - Optimize for the desktop use-case- latency-performance - Optimize for deterministic performance at the cost of increased power consumption- network-latency - Optimize for deterministic performance at the cost of increased power consumption, focused on low latency network performance- network-throughput - Optimize for streaming network throughput, generally only necessary on older CPUs or 40G+ networks- powersave - Optimize for low power consumption- throughput-performance - Broadly applicable tuning that provides excellent performance across a variety of common server workloads- virtual-guest - Optimize for running inside a virtual guest- virtual-host - Optimize for running KVM guestsCurrent active profile: balanced 通过tuned-adm命令的profile参数，可以保证系统动态的进行调整参数。 参考https://www.linuxidc.com/Linux/2015-05/118005.htmhttps://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/power_management_guide/tuned]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你知道Disruptor吗？]]></title>
    <url>%2F2019%2F06%2F08%2F%E4%BD%A0%E7%9F%A5%E9%81%93Disruptor%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[官方githubhttps://github.com/LMAX-Exchange/disruptor 几个重要概念 RingBuffer：环形的缓冲区，消息事件信息的载体。曾经 RingBuffer 是 Disruptor 中的最主要的对象，但从3.0版本开始，其职责被简化为仅仅负责对通过 Disruptor 进行交换的数据（事件）进行存储和更新。在一些更高级的应用场景中，Ring Buffer 可以由用户的自定义实现来完全替代。 Event：定义生产者和消费者之间进行交换的数据类型。 EventFactory：创建事件的工厂类接口，由用户实现，提供具体的事件 EventHandler：事件处理接口，由用户实现，用于处理事件 ThreadFactory：这是一个线程工厂，用于我们Disruptor中生产者消费的时候需要的线程。 EventTranslator:实现这个接口可以将我们的其他数据结构转换为在Disruptor中流通的Event。 应用场景Disruptor是高性能的进程内线程间的数据交换框架，特别适合日志类的处理。 比如https://github.com/alipay/sofa-tracer,蚂蚁金服 团队开源的分布式链路追踪项目，其中日志处理部分就是使用了Disruptor。 包括Apache Storm、Camel、Log4j2等等知名的框架都在内部集成了Disruptor用来替代jdk的队列，以此来获得高性能。 源码分析待分析。。。。 三大特点 CAS：利用CAS进行队列中的一些下标设置，减少了锁的冲突 消除伪共享：在Disruptor中采用了Padding的方式 RingBuffer：采用了环形数组进行保存数据 参考http://www.kailing.pub/article/index/arcid/208.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你知道Gossip协议吗？]]></title>
    <url>%2F2019%2F06%2F08%2F%E4%BD%A0%E7%9F%A5%E9%81%93Gossip%E5%8D%8F%E8%AE%AE%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[Gossip该协议的应用的地方：对于单机实例，我们使用 Redis 自带的哨兵（Sentinel）集群对实例进行状态监控与 Failover。Sentinel 是 Redis 自带的高可用组件，将 Redis 注册到由多个 Sentinel 组成的 Sentinel 集群后，Sentinel 会对 Redis 实例进行健康检查，当 Redis 发生故障后，Sentinel 会通过 Gossip 协议进行故障检测，确认宕机后会通过一个简化的 Raft 协议来提升 Slave 成为新的 Master 过程Redis 集群是去中心化的，彼此之间状态同步靠 gossip 协议通信，集群的消息有以下几种类型： Meet。通过「cluster meet ip port」命令，已有集群的节点会向新的节点发送邀请，加入现有集群。 Ping。节点每秒会向集群中其他节点发送 ping 消息，消息中带有自己已知的两个节点的地址、槽、状态信息、最后一次通信时间等。 Pong。节点收到 ping 消息后会回复 pong 消息，消息中同样带有自己已知的两个节点信息。 Fail。节点 ping 不通某节点后，会向集群所有节点广播该节点挂掉的消息。其他节点收到消息后标记已下线。 由于去中心化和通信机制，Redis Cluster 选择了最终一致性和基本可用。例如当加入新节点时(meet)，只有邀请节点和被邀请节点知道这件事，其余节点要等待 ping 消息一层一层扩散。除了 Fail 是立即全网通知的，其他诸如新节点、节点重上线、从节点选举成为主节点、槽变化等，都需要等待被通知到。 因此，由于 gossip 协议，Redis Cluster 对服务器时间的要求较高，否则时间戳不准确会影响节点判断消息的有效性。另外节点数量增多后的网络开销也会对服务器产生压力。因此官方推荐最大节点数为 1000。 原理Gossip是一种去中心化、容错并保证最终一致性的协议。Background：分布式环境Gossip是为了解决分布式遇到的问题而设计的。由于服务和数据分布在不同的机器上，节点之间的每次交互都伴随着网络延迟、网络故障等的性能问题。可见，分布式系统会比单机系统遇到更多的难题。如CAP理论 所描述的，CAP三个因素在分布式的条件下只能满足两个。对于分布式系统来说，分区容忍性是其的基本要求。因为分布式系统的设计初衷就是利用集群多集的能力去处理单机无法解决的问题。分区容忍性（可扩展性）通过scale up和scale out实现的，也就是通过升级硬件或者增加机器来提升分布式系统的性能。这么说，可扩展性和可用性是相关联的。可扩展性好的系统，其可用性一般会比较高。所以分布式系统的所有问题基本都是在一致性和可用性之间进行协调和平衡。在工程实践中的经验如下：一般来说，交易系统类的业务对一致性的要求比较高，一般会采用ACID模型来保证数据的强一致性，所以其可用性和扩展性就比较差。而其他大多数业务系统一般不需要保证强一致性，只要最终一致就可以了，它们一般采用BASE模型，用最终一致性的思想来设计分布式系统，从而使得系统可以达到很高的可用性和扩展性。一致性可以通过信息在分布式环境下分发来保证，而分发的方式和速度则决定一致性的程度。从客户端的角度来讲：一致性包含三种状态：强一致性、弱一致性、最终一致性（弱一致性的特例）。在下图的一致性光谱中我们可以看出，弱一致性性是异步冗余，读写操作的响应更加快；而强一致性一般都是同步冗余的，所以伴随着性能的下降。 而最终一致性还有其他变种：因果一致性（有逻辑关系的操作能读到更新值）、读你所写一致性（Read-your-writes Consistency，A用户操作只保证自己的后续操作能读到更新值）、会话一致性（保证整个会话期间的读写一致性）、单调一致性（单用户的操作顺序一致）。SWIM：最终一致性前面提到Gossip解决的问题就是在分布式环境下信息高效分发的问题，这个问题的解决决定着系统的一致性程度。而Gossip协议是基于一种叫做SWIM的协议（ S calable W eakly-consistent I nfection-style Process Group M embership Protocol）。SWIM是一种无中心的分布式协议，各个节点之间通过p2p实现信息交流同步各节点状态的方法。看名字也知道这是一种弱一致性的实现。SWIM协议给每个进程组成员在本地维护一个成员表，记录该组存活的进程。该协议通过失效检测器（Failure Detector）和传播组件（Dissemination Component）来完成工作。SWIM的失效检测器会检测失效的节点并将失效节点的更新信息发送给传播组件。SWIM的传播组件通过多播（multicast）的形式将失效信息传播给组内的其他成员。协议的可扩展性体现在：新成员的加入和退出也以同样的方式进行多播通信。而在基本的时间周期内进行失效检测能够保证在限定的时间范围内完成完备性检查，即每个失效的进程都能最终被检测到（最终一致性）。通过多播方式传输协议消的问题在于效率不好也不可靠，通过在ping和ack消息中捎带成员更新信息能够降低丢包率和减少传输时延。这种传播方式被称为可传导的方式（Infection-style）。Gossip：办公室八卦我们的办公室八卦一般都是从一次交谈开始，只要一个人八卦一下，在有限的时间内办公室的的人都会知道该八卦的信息，这种方式也与病毒传播类似。因此 Gossip也有“病毒感染算法”、“谣言传播算法”之称。Gossip来源于流行病学的研究（括号里就是Gossip协议）：在总数为n+1的人群中，被感染（infected）的人数初始化为1，并向周围传播。（一个节点状态发生变化，并向临近节点发送更新信息） 在每个周期内总有未被感染（uninfected）的人转变成被感染的人，方式委每个被感染的人随机感染b个人。（对于节点状态变化的信息随机发送给b个节点，图例中的b值为2） 经过足够的时间，所有的人都会被感染。（随着时间推移，信息能够传达到所有的节点，下一节会进行简单的证明） 可以看到，协议的核心内容就是节点通过将信息随机发送到b个节点来完成本次信息的传播，其涉及到周期性、配对、交互模式。Gossip的交互模式分为两种：Anti-entropy和Rumor mongering。Anti-entropy：每个节点周期性地随机选择其他节点，然后通过相互交换自己的所有数据来消除两者之间的差异。Rumor mongering：当一个节点有来新信息后，该节点变成活跃状态，并周期性地联系其他节点向其发送新信息。每个节点维护一个自己的信息表 &lt;key, (value, version)&gt; ，即属性的值以及版本号；和一个记录其他节点的信息表 &lt;node, &lt;key, (value, version)&gt;&gt; 。每个节点和系统中的某个节点相互配对成为peer。而节点的信息交换方式主要有3种。Push：拥有状态新信息的节点随机选择联系节点并想起发送自己得到信息。Pull：发起信息交换的节点随机选择联系节点并从对方获取信息。Push-Pull混合模式：发起信息交换的节点向选择的节点发送信息。上述Gossip为什么能够完成状态的同步呢？我们对其做一个简单的分析。Analysis：收敛性证明我们以上一节的Push模式Gossip协议进行分析。在n+1个节点的系统中，每个节点每次随机向其他b个节点进行信息通信，即传播速率：β=bnβ=bn。未获得更新信息的数量为x（初始为n），获得更新信息的节点数为y（初始为1）。在连续时间过程中，x的变化速率dxdt=−βxydxdt=−βxy，即传播速率 乘以 两种类型节点之间可能传播的次数。可以推导出火的更新信息的节点数y=n+11+ne−β(n+1)ty=n+11+ne−β(n+1)t而总时间为 t=clog(n)t=clog⁡(n)，即log(n)轮传播乘以一个常数时间。被感染的数量 y≈(n+1)−1ncb−2y≈(n+1)−1ncb−2。那么当c和b都是独立于n的很小的数值时。Gossip协议能够保证：低延迟：在clog(n)内完成一次信息的更新。虽然不是常数级别的，但是气对数级别增长率在程序世界里是实践上可取的。可靠性：n+1−1ncb−2n+1−1ncb−2会收到新信息。轻量级：每个节点不会发送超过cblog(n)条信息。这样我们不仅证明了Gossip的可靠性，并可以保证其在分布式系统应用的高可用性。注意的是，即使有的节点因宕机而重启或者有新节点加入，但经过一段时间后，这些节点的状态也会与其他节点达成一致。也就是说，Gossip天然具有分布式容错的优点。Application：应用除了改善SWIM协议中的多播方式，Gossip还在很多地方有应用：数据库复制：基于Gossip实现分布数据管理的一般思路是：灾一个节点实现数据更新，通过Gossip算法将更新传播导其他节点。聚合计算：在无中心的系统中，没有中心节点存储全局信息。通过Gossip应用导分布环境下的聚合计算中来保证系统的发送消息的容错。总之，Gossip简单、高效，同时具有很好的可扩展性和鲁棒性，非常适合大规模、动态、资源受限的网络环境。 Gossip应用场景接受最终一致性的领域：失败检测、路由同步、Pub\Sub、动态负载均衡]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka的各种参数]]></title>
    <url>%2F2019%2F06%2F07%2FKafka%E7%9A%84%E5%90%84%E7%A7%8D%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[buffer.memory参数buffer.memory这个是kafka-producer 缓存池（用的这个CopyOnWriteMap数据结构，无锁，高并发）的大小参数，应该设置多少？默认是多少呢？ 比如说每秒300条消息，那么你就需要压测一下，假设内存缓冲就32MB，每秒写300条消息到内存缓冲，是否会经常把内存缓冲写满？经过这样的压测，你可以调试出来一个合理的内存大小。 有个问题？在缓存期间客户端挂掉，数据是不是丢失了。？？？？？？（必然要容忍极端宕机情况下，可能丢失比如几秒钟的数据。？？） batch.size参数不能太小，也不能太大（会有延时）。需要在这里按照生产环境的发消息的速率，调节不同的Batch大小自己测试一下最终出去的吞吐量以及消息的 延迟，设置一个最合理的参数。 linger.ms参数“linger.ms”决定了你的消息一旦写入一个Batch，最多等待这么多时间，他一定会跟着Batch一起发送出去。避免一个Batch迟迟凑不满，导致消息一直积压在内存里发送不出去的情况。 max.request.size参数这个参数决定了每次发送给Kafka服务器请求的最大大小，同时也会限制你一条消息的最大大小也不能超过这个参数设置的值，这个其实可以根据你自己的消息的大小来灵活的调整。 retries”和“retries.backoff.ms”重试机制也就是如果一个请求失败了可以重试几次，每次重试的间隔是多少毫秒。 这个大家适当设置几次重试的机会，给一定的重试间隔即可，比如给100ms的重试间隔 acks持久化机制这个参数可以设置成0、1 和 all。第一种选择是把acks参数设置为0，意思就是我的KafkaProducer在客户端，只要把消息发送出去，不管那条数据有没有在哪怕Partition Leader上落到磁盘，我就不管他了，直接就认为这个消息发送成功了。 第二种选择是设置 acks = 1，意思就是说只要Partition Leader接收到消息而且写入本地磁盘了，就认为成功了，不管他其他的Follower有没有同步过去这条消息了。 这种设置其实是kafka默认的设置，大家请注意，划重点！这是默认的设置 最后一种情况，就是设置acks=all，这个意思就是说，Partition Leader接收到消息之后，还必须要求ISR列表里跟Leader保持同步的那些Follower都要把消息同步过去，才能认为这条消息是写入成功了。 acks=all 就可以代表数据一定不会丢失了吗？当然不是，如果你的Partition只有一个副本，也就是一个Leader，任何Follower都没有，你认为acks=all有用吗？当然没用了，因为ISR里就一个Leader，他接收完消息后宕机，也会导致数据丢失。所以说，这个acks=all，必须跟ISR列表里至少有2个以上的副本配合使用，起码是有一个Leader和一个Follower才可以。这样才能保证说写一条数据过去，一定是2个以上的副本都收到了才算是成功，此时任何一个副本宕机，不会导致数据丢失。 参考https://juejin.im/post/5cdc5c6bf265da038145fd9fhttps://juejin.im/post/5cbf2d58f265da0380436fde]]></content>
      <categories>
        <category>Kakfa</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源项目汇总]]></title>
    <url>%2F2019%2F06%2F06%2F%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[k-v数据库RocksDBhttps://github.com/google/LevelDB 缓存框架spring cachehttp://spring.io/guides/gs/caching/ jetcachehttps://github.com/alibaba/jetcache http容器RestExpressRestExpress虽很轻量但五脏俱全，非常适合一些小工具暴露http的服务。https://github.com/RestExpress/RestExpress Java排查Arthashttps://github.com/alibaba/arthas K8sKubernetes 集群管理软件 Wayne(360公司)https://github.com/Qihoo360/wayne 无服务器容器云平台（国产）https://github.com/goodrain/rainbond Docker镜像java应用docker镜像build工具jibhttps://github.com/GoogleContainerTools/jib skywalking-dockerhttps://hub.docker.com/r/wutang/skywalking-docker/ 小工具内网穿透lanproxy(免费)https://github.com/ffay/lanproxy hosts快速切换工具SwitchHostshttps://github.com/oldj/SwitchHosts 高性能框架Disruptorhttps://github.com/LMAX-Exchange/disruptor JCToolshttps://github.com/JCTools/JCTools Akkahttps://github.com/akka/akka （分布式特性） 分布式ID服务tinyidhttps://github.com/didi/tinyid&gt; Leafhttps://github.com/Meituan-Dianping/Leaf uid-generatorhttps://github.com/baidu/uid-generator DistributedIDhttps://github.com/beyondfengyu/DistributedID api文档管理easy-mockhttps://github.com/easy-mock/easy-mock ApiManagerhttps://github.com/EhsanTang/ApiManager showdochttps://github.com/star7th/showdoc 配置中心apollohttps://github.com/ctripcorp/apollo disconfhttps://github.com/knightliao/disconf spring cloud config 注册中心服务zookpeerhttp://zookeeper.apache.org/ eurekahttps://github.com/Netflix/eureka consulhttps://github.com/hashicorp/consul etcdhttps://github.com/coreos/etcd rpcdubbohttps://github.com/apache/incubator-dubbo motanhttps://github.com/weibocom/motan sofa-rpchttps://github.com/alipay/sofa-rpc mqrpchttps://gitee.com/kailing/springboot-mqrpc Tarshttps://github.com/Tencent/Tars 微服务spring cloudhttp://projects.spring.io/spring-cloud/ ServiceCombhttp://servicecomb.incubator.apache.org/cn/ api网关zuulhttps://github.com/Netflix/zuul konghttps://github.com/Kong/kong orangehttps://github.com/sumory/orange 分布式事务 lcnhttps://github.com/codingapi/tx-lcn/ tcc-transaction&lt;https://github.com/changmingxie/##### tcc-transaction&gt; coolmqhttps://github.com/vvsuperman/coolmq Raincathttps://github.com/yu199195/Raincat fescarhttps://github.com/alibaba/fescar 分布式锁klock Klock是基于redis开发的分布式锁spring-boot starter组件https://gitee.com/kekingcn/spring-boot-klock-starter lock-springhttps://github.com/zouyingchun/lock-spring 分布式任务调度xxl-jobhttps://github.com/xuxueli/xxl-job Elastic-jobhttps://github.com/elasticjob/elastic-job-lite 批处理spring batchhttps://spring.io/projects/spring-batch partitionjobhttps://gitee.com/kailing/partitionjob NewSqltidbhttps://github.com/pingcap/tidb oceanbasehttps://github.com/alibaba/oceanbase cockroachhttps://github.com/cockroachdb/cockroach 数据库中间件cetushttps://github.com/Lede-Inc/cetus DBProxyhttps://github.com/Meituan-Dianping/DBProxy mycathttps://github.com/MyCATApache/Mycat-Server sharding-jdbchttps://github.com/shardingjdbc/sharding-jdbc dblehttps://github.com/actiontech/dble proxysqlhttps://github.com/sysown/proxysql 数据库连接池druidhttps://github.com/alibaba/druid tomcatjdbchttp://tomcat.apache.org/tomcat-8.0-doc/jdbc-pool.html HikariCPhttps://github.com/brettwooldridge/HikariCP 数据访问jpahttps://github.com/spring-projects/spring-data-jpa querydslhttps://github.com/querydsl/querydsl jooqhttps://github.com/jOOQ/jOOQ mangohttps://github.com/jfaster/mango minidaohttps://gitee.com/jeecg/minidao SQL优化SQLAdvisorhttps://github.com/Meituan-Dianping/SQLAdvisor binlog增量日志消费：keking-binloghttps://gitee.com/kekingcn/keking-binlog-distributor mysql-binlog-connector-javahttps://github.com/shyiko/mysql-binlog-connector-java canalhttps://github.com/alibaba/canal pumahttps://github.com/dianping/puma open-replicatorhttps://github.com/whitesock/open-replicator porterhttps://github.com/sxfad/porter (随行付，成品项目) 索引引擎elasticsearchhttps://github.com/elastic/elasticsearch solrhttp://lucene.apache.org/solr/guide/7_3/ lucenehttp://lucene.apache.org/ JRediSearchhttps://github.com/RedisLabs/JRediSearch ftserverhttps://github.com/iboxdb/ftserver 消息中间件RabbitMQhttp://www.rabbitmq.com/getstarted.html ActiveMQhttps://github.com/apache/activemq kafkahttps://github.com/apache/kafka rocketmqhttps://github.com/apache/rocketmq zbushttp://zbus.io/ apache pulsarhttp://pulsar.apache.org/ DevOpsjenkinshttps://jenkins.io hudsonhttp://www.eclipse.org/hudson rundeckhttps://github.com/rundeck/rundeck Hygieiahttps://github.com/capitalone/Hygieia 应用安全openrasphttps://github.com/baidu/openrasp apmskywalkinghttps://github.com/apache/incubator-skywalking pinpointhttps://github.com/naver/pinpoint zipkinhttps://github.com/openzipkin/zipkin cathttps://github.com/dianping/cat sofa-tracer(阿里巴巴)https://github.com/sofastack/sofa-tracer 快速开发，微核心spring boothttp://projects.spring.io/spring-boot/ jfinalhttps://gitee.com/jfinal/jfinal nutzhttps://github.com/nutzam/nutz rapidoidhttps://github.com/rapidoid/rapidoid 日志采集logpipehttps://github.com/calvinwilliams/logpipe logstashhttps://github.com/elastic/logstash sentryhttps://github.com/getsentry/sentry 爬虫相关webmagichttps://gitee.com/flashsword20/webmagic WebCollectorhttps://github.com/CrawlScript/WebCollector jsouphttps://github.com/jhy/jsoup 堡垒机teleporthttps://github.com/tp4a/teleport/ jumpserverhttps://github.com/jumpserver/jumpserver 后台管理脚手架x-boothttps://github.com/Exrick/x-boot（x-font）]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之字节码知识]]></title>
    <url>%2F2019%2F06%2F04%2FJava%E4%B9%8B%E5%AD%97%E8%8A%82%E7%A0%81%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[第一次买了一本掘金小册，花了19.9元，记录下收获。 用xxd 命令以 16 进制的方式查看这个 class 文件。 javap，用来方便的窥探 class 文件内部的细节。javap 有比较多的参数选项，其中-c -v -l -p -s是最常用的。 javap的-l的作用：-l 输出行及局部变量表。 虚拟机常见的实现方式有两种：Stack based 的和 Register(寄存器) based。比如基于 Stack 的虚拟机有Hotspot JVM、.net CLR，这种基于 Stack 实现虚拟机是一种广泛的实现方法。而基于 Register 的虚拟机有 Lua 语言虚拟机 LuaVM 和 Google 开发的安卓虚拟机 DalvikVM。 整个 JVM 指令执行的过程就是局部变量表与操作数栈之间不断 load、store 的过程 一个对象创建的套路是这样的：new、dup、invokespecial，下次遇到同样的指令要形成条件反射。 HSDB 全称是：Hotspot Debugger，是内置的 JVM 工具，可以用来深入分析 JVM 运行时的内部状态。HSDB 位于 JDK 安装目录下的 lib/sa-jdi.jar 中， 启动 HSDB 随着 JDK7 的发布，字节码指令集新增了一个重量级指令 invokedynamic。这个指令为为多语言在 JVM 上百花齐放提供了坚实的技术支撑。 鸭子类型（Duck Typing）：当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子 在鸭子类型中，关注点在于对象的行为，能做什么；而不是关注对象所属的类型，不关注对象的继承关系 MethodHandle 又被称为方法句柄或方法指针， 是java.lang.invoke 包中的 一个类，它的出现使得 Java 可以像其它语言一样把函数当做参数进行传递。MethodHandle 类似于反射中的 Method 类，但它比 Method 类要更加灵活和轻量级。 123456789101112131415public class Foo &#123; public void print(String s) &#123; System.out.println("hello, " + s); &#125; public static void main(String[] args) throws Throwable &#123; Foo foo = new Foo(); MethodType methodType = MethodType.methodType(void.class, String.class); MethodHandle methodHandle = MethodHandles.lookup().findVirtual(Foo.class, "print", methodType); methodHandle.invokeExact(foo, "world"); &#125;&#125;运行输出hello, world Groovy 采用 invokedynamic 指令有哪些好处? 标准化。使用 Bootstrap Method、CallSite、MethodHandle 机制使得动态调用的方式得到统一保持了字节码层的统一和向后兼容。把动态方法的分派逻辑下放到语言实现层，未来版本可以很方便的进行优化、修改高性能。接近原生 Java 调用的性能，也可以享受到 JIT 优化等 关于为什么用 invokedynamic 来实现 Lambda，Oracle 的开发者专门写了一篇文章 Translation of Lambda Expressions，介绍了 Java 8 Lambda 设计时的考虑以及实现方法。 文中提到 Lambda 表达式可以通过内部类、method handle、dynamic proxies 等方式实现，但是这些方法各有优劣。实现 Lambda 表达式需要达成两个目标： 为未来的优化提供最大的灵活性保持类文件字节码格式的稳定使用 invokedynamic 可以很好的兼顾这两个目标。http://cr.openjdk.java.net/~briangoetz/lambda/lambda-translation.html lambda 表达式与普通的匿名内部类的实现方式不一样，在编译阶段只是新增了一个 invokedynamic 指令，并没有在编译期间生成匿名内部类，lambda 表达式的内容会被编译成一个静态方法。在运行时 LambdaMetafactory.metafactory 这个工厂方法来动态生成一个内部类 class，该内部类会调用前面生成的静态方法。 lambda 表达式最终还是会生成一个内部类，只不过不是在编译期间而是在运行时，未来的 JDK 会怎么实现 Lambda 表达式可能还会有变化。 JDK7 引入的 String 的 switch 实现流程分为下面几步： 计算字符串 hashCode使用 lookupswitch 对整型 hashCode 进行分支对相同 hashCode 值的字符串进行最后的字符串匹配执行 case 块代码 finally为什么一定会被执行？因为底层实现它复制了三份。。。 123456789101112public void foo() &#123; try &#123; tryItOut1(); handleFinally(); &#125; catch (MyException1 e) &#123; handleException(e); handleFinally(); &#125; catch (Throwable e) &#123; handleFinally(); throw e; &#125;&#125; finally中有return。如果 finally 中有 return，因为它先于其它的执行，会覆盖其它的返回（包括异常） 第一，try-with-resource 语法并不是简单的在 finally 里中加入了closable.close()方法，因为 finally 中的 close 方法如果抛出了异常会淹没真正的异常；第二，引入了 suppressed 异常的概念，能抛出真正的异常，且会调用 addSuppressed 附带上 suppressed 的异常。 因为编译器必须保证，无论同步代码块中的代码以何种方式结束（正常 return 或者异常退出），代码中每次调用 monitorenter 必须执行对应的 monitorexit 指令。为了保证这一点，编译器会自动生成一个异常处理器，这个异常处理器的目的就是为了同步代码块抛出异常时能执行 monitorexit。这也是字节码中，只有一个 monitorenter 却有两个 monitorexit 的原因 方法级的 synchronized实现原理方法级的同步与上述有所不同，它是由常量池中方法的 ACC_SYNCHRONIZED 标志来隐式实现的。 12345678synchronized public void testMe() &#123;&#125;对应字节码public synchronized void testMe();descriptor: ()Vflags: ACC_PUBLIC, ACC_SYNCHRONIZED JVM 不会使用特殊的字节码来调用同步方法，当 JVM 解析方法的符号引用时，它会判断方法是不是同步的（检查方法 ACC_SYNCHRONIZED 是否被设置）。如果是，执行线程会先尝试获取锁。如果是实例方法，JVM 会尝试获取实例对象的锁，如果是类方法，JVM 会尝试获取类锁。在同步方法完成以后，不管是正常返回还是异常返回，都会释放锁 泛型真的被完全擦除了吗LocalVariableTypeTable 和 Signature 是针对泛型引入的新的属性，用来解决泛型的参数类型识别问题，Signature 最为重要，它的作用是存储一个方法在字节码层面的特征签名，这个属性保存的不是原生类型，而是包括了参数化类型的信息。我们依然可以通过反射的方式拿到参数的类型。所谓的擦除，只是把方法 code 属性的字节码进行擦除。 因为很多情况下，反射只会调用一次，因此 JVM 想了一招，设置了 15 这个 sun.reflect.inflationThreshold 阈值，反射方法调用超过 15 次时（从 0 开始），采用 ASM 生成新的类，保证后面的调用比 native 要快。如果小于 15 次的情况下，还不如生成直接 native 来的简单直接，还不造成额外类的生成、校验、加载。这种方式被称为 「inflation 机制」。inflation 这个单词也比较有意思，它的字面意思是「膨胀；通货膨胀」。 基于 javaagent 来实现的，比如热部署 JRebel、性能调试工具 XRebel、听云、newrelic 等。它能实现的基本功能包括 可以在加载 class 文件之前做拦截，对字节码做修改，可以实现 AOP、调试跟踪、日志记录、性能分析可以在运行期对已加载类的字节码做变更，可以实现热部署等功能。 javaagent 有两个重要的入口类：Premain-Class 和 Agent-Class，分别对应入口函数 premain 和 agentmain，其中 agentmain 可以采用远程 attach API 的方式远程挂载另一个 JVM 进程。 ASM 库是设计模式中访问者模式的典型应用，三大核心类 ClassReader(读取字节码、分析)、ClassVisitor（修改各个节点的字节码）、ClassWriter （ClassWriter 的 toByteArray 方法则把最终修改的字节码以 byte 数组的形式返回） 字节码反编译查看工具 jdgui，luyten字节码浏览工具 jclasslibASMvim、hex editor vim 十六进制查看文件vim -b ./com/jclarity/censum/CensumStartupChecks.class 使用 16 进制模式打开:%!xxd 回到普通模式:%!xxd -r保存退出 如果你去做一个商业版本的软件，有哪些手段可以防止别人用类似的手段破解呢？” 使用自定义的 classloader，原来的 class 文件经过加密处理，直接解压拿到的的 class 文件没法直接查看，在 classloader 里面进行解密然后加载 把核心的逻辑写到 jni 层 页面加载时间、首屏时间、页面渲染时间 我们在 chrome console 里输入 window.performance.timing 就可以拿到详细的各阶段时间 怎么样做嵌码？Java 服务端：使用我们之前介绍过的 javaagent 字节码 instrument 技术进行字节码改写Node.js 阿里有开源 pandora.js 可以参考借鉴安卓：用 gradle 插件在编译期进行 hookiOS：Hook（Method Swizzling） 因为 APM 会产生调用次数放大，一次调用可能产生几十次上百次的的链路调用 trace。因此数据一定要合并上报，减少网络的开销。 这个场景就是 合并上报，指定时间还没达到批量的阈值，有多少条报多少条，针对此场景我写了一个非常简单的工具类，用 BlockingQueue 实现了带超时的批量取 12345678910111213141516171819202122232425262728293031323334private static final int SIZE = 5000;private static final int BATCH_FETCH_ITEM_COUNT = 50;private static final int MAX_WAIT_TIMEOUT = 30;private BlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;&gt;(SIZE); public boolean add(final String logItem) &#123; // 如果队列已满，需要超时等待一段时间，使用此方法queue.offer(logItem, 10, TimeUnit.MILLISECONDS)// 如果队列已满，直接需要返回add失败，使用此方法 return queue.offer(logItem);&#125;public List&lt;String&gt; batchGet() &#123; List&lt;String&gt; bulkData = new ArrayList&lt;&gt;(); batchGet(queue, bulkData, BATCH_FETCH_ITEM_COUNT, MAX_WAIT_TIMEOUT, TimeUnit.SECONDS); return bulkData;&#125;public static &lt;E&gt; int batchGet(BlockingQueue&lt;E&gt; q,Collection&lt;? super E&gt; buffer, int numElements, long timeout, TimeUnit unit) throws InterruptedException &#123; long deadline = System.nanoTime() + unit.toNanos(timeout); int added = 0; while (added &lt; numElements) &#123; // drainTo非常高效，我们先尝试批量取，能取多少是多少，不够的poll来凑 added += q.drainTo(buffer, numElements - added); if (added &lt; numElements) &#123; E e = q.poll(deadline - System.nanoTime(), TimeUnit.NANOSECONDS); if (e == null) &#123; break; &#125; buffer.add(e); added++; &#125; &#125; return added;&#125; 实时告警 实时数据处理可以用时序数据库，也可以用 Redis + Lua 的方式，有告警的情况下可以达到分钟级别的微信、邮件通知，也可以在业务上做告警收敛，避免告警风暴 Disruptor在APM系统中可以用在日志模块，异步写日志，也可以用在agent模块，收集trace日志，在collector端根据traceID聚合日志 跨进程链路调用的实现跨进程调用是采用在调用方注入当前 traceId 和 spanId 来实现的。以调用 Dubbo 为例，Dubbo 真正的远程调用是在 com.alibaba.dubbo.rpc.cluster.support.AbstractClusterInvoker.invoke()函数 123public Result invoke(Invocation invocation) throws RpcException &#123; return new RpcResult(doInvoke(proxy, invocation.getMethodName(), invocation.getParameterTypes(), invocation.getArguments()));&#125; 需要把上述代码改写为12345public Result invoke(Invocation invocation) throws RpcException &#123; invocation.setAttachment("X-APM-TraceId", "traceId-1001"); invocation.setAttachment("X-APM-SpanId", "spanId-A001"); return new RpcResult(doInvoke(proxy, invocation.getMethodName(), invocation.getParameterTypes(), invocation.getArguments()));&#125; 实现一个 APM 系统最核心的部分：字节码改写、ThreadLocal 实现调用栈、跨进程调用。https://github.com/arthur-zhang/geek01/tree/master/javaagent-demo 把 Nginx 加入到 APM 链路中来OpenResty 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。 使用 OpenResty 可以比较灵活的实现添加 header，获取耗时、状态码等信息，用少量的代码就可以把 Nginx 加入到 APM 链路中来 123456789101112131415161718192021222324- 定义Headerslocal X_APM_TRACE_ID = "X-APM-TraceId"local X_APM_SPAN_ID = 'X-APM-SpanId'local X_APM_SPAN_NAME = 'Nginx'-- 生成Nginx Span Idlocal ngx_span_id = string.gsub(uuid(), '-', '')-- 从Header中，获取父Span信息local ngx_span_parent = nilif req_headers ~= nil then ngx_span_parent = req_headers[X_APM_SPAN_ID]end-- 向Header中，写入Nginx Span相关信息local trace_id = req_headers[X_APM_TRACE_ID]if trace_id == nil then trace_id = string.gsub(uuid(), '-', '') ngx.req.set_header(X_APM_TRACE_ID, trace_id)endngx.req.set_header(X_APM_SPAN_ID, ngx_span_id)ngx.req.set_header(X_APM_SPAN_NAME, X_APM_SPAN_NAME)ngx.req.set_header(X_APM_SAMPLED, X_APM_SAMPLED) 好的书籍Java虚拟机规范(Java SE 8版) 这本书我看了很多遍，每次看都有新的收获，强烈推荐 自己动手写Java虚拟机 这本书主要讲的是用 Go 语言来实现 Java 虚拟机，需要你有一点点的 Go 语言基础，我对着这本书敲完了，然后用 Kotlin 重写了一遍，真正了解了 class 文件解析、字节码指令运行的详细细节，也推荐大家看一看 深入理解Java虚拟机:JVM高级特性与最佳实践 这本神书不用多说，很经典，笔试面试必备 JRockit权威指南 深入理解JVM 这本书也是了解 JVM 非常不多的书籍，里面提到了很多调优相关的东西 ASM guidehttps://asm.ow2.io/asm4-guide.pdf 掘金小册地址https://juejin.im/book/5c25811a6fb9a049ec6b23ee]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《JAVA并发编程实践》（译）笔记]]></title>
    <url>%2F2019%2F06%2F02%2F%E8%AF%BB%E3%80%8AJAVA%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%E3%80%8B%EF%BC%88%E8%AF%91%EF%BC%89%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[多数Servlet都可以实现为无状态的，降低了线程安全的负担，只有当servlet要为不同的请求记录一些信息时，才需要线程安全。 耗时的计算或操作，执行这些操作期间不要占用锁，比如IO 不要让this引用在构造期间逸出 SynchronousQueue这类队列只有在消费者充足的时候比较合适，他们总能为下一个任务做好准备。 可以使用Semaphore把任何容器转化为有界的阻塞容器 Exchanger是非常有用的，比如一个线程向缓冲写入一个数据，另一个线程消费这个数据，就可以使用Exchanger进行会面。 中段通常是实现取消最明智的选择 Java中的中断 使用finally块和显式close方法的结合来管理资源，会比使用finalizer起到更好地作用，尽量不要使用finalizer。 扩展ThreadPoolExecutor这个的设计是可扩展的，有函数钩子，beforeExecute、afterExecute和terminate。 如何解决活锁加入随机性。以太协议在重复发生冲突时，同样包含一个指数的撤退协议，减少了拥塞和多个冲突的机站重复失败的风险。 自旋等待适合短期的等待，而挂起适合长时间等待。 减少锁的竞争（三种方式）减少锁持有的时间、减少频率、用协调机制替代独占锁 垃圾回收在技术上是不可能强制垃圾回收执行的，System.gc仅仅是建议JVM在一个合适的时候进行垃圾回收。 静态分析工具FindBugs http://findbugs.sourceforege.net]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《实战Java高并发程序设计》笔记]]></title>
    <url>%2F2019%2F06%2F02%2F%E8%AF%BB%E3%80%8A%E5%AE%9E%E6%88%98Java%E9%AB%98%E5%B9%B6%E5%8F%91%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E3%80%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[一句说解释活锁 两个线程都主动释放资源给对方，就会导致这个资源在两个线程间来回跳动，谁也拿不到资源执行，就类似于两个人互相谦让。 并发级别阻塞（synchroized、lock）、无饥饿（公平锁与非公平锁）、无障碍（直接去操作，发生冲突回滚，类似，类似乐观锁）、无锁（CAS）、无等待（RCU Read Copy Update 读不限制，修改副本） 指令重排 指令重排是为了减少中断，CPU是流水线模型。哪些指令不能重排：Happens before规则 线程状态线程的等待在等什么？wait的等待线程在等notify，join的等待线程在等目标线程的终止。 启动线程要用start(),start创建一个新的线程并运行run方法。如果只用run（），代表在当前线程执行，只是一个普通的方法。 线程等待Object.wait和Thread.sleep(),都可以让线程等待。wait可以被唤醒，wait释放锁。sleep不释放锁。 守护线程设置守护线程，必须在start()方法前设置，否则线程永远不会停下来。 HashMap的死循环问题在jdk8不会出现 可重入锁重入锁可以提供中断处理的能力。比如一个等待的线程，他在等待锁，他可以收到一个通知，被告知无需等待了。也就是在等待锁的过程中，可以响应等中断。而对于synchroized来讲，就是要么有锁执行，要么无锁等待。重入锁可以是公平的。 ConditionCondition与重入锁配合，Synchronized与wait和notify配合 1234567891011121314151617181920212223242526272829303132333435import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.ReentrantLock;/*** @author 宋建涛 E-mail:1872963677@qq.com* @version 创建时间：2019年6月1日 下午7:47:12* 类说明*/public class ReenterLockCondition implements Runnable &#123; public static ReentrantLock lock = new ReentrantLock(); public static Condition condition = lock.newCondition(); @Override public void run() &#123; try &#123; lock.lock(); condition.await();//在condition上等待 System.out.println("Thread is going on"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; ReenterLockCondition t1 = new ReenterLockCondition(); Thread t= new Thread(t1); t.start(); Thread.sleep(2000); //通知线程t继续执行 lock.lock(); //先获取相关的锁 condition.signal(); //告诉等待在condotion上的线程可以继续执行了 lock.unlock(); &#125;&#125; 信号量 锁一次只能要求一个线程操控共享资源，而信号量可以允许多个。比如你要让5个线程5个线程的执行，可以设置一个信号量为5. 线程阻塞工具类LockSupport，可以在线程内任意位置让线程阻塞。与Object.wait（）相比，他不需要先获得某个对象的锁。使用park函数时，可以为当前线程设置一个阻塞对象，这个阻塞对象会出现在线程Dump中，更加容易分析问题。 线程池的队列如果使用无界队列来存储任务，要注意如果任务创建和处理的速度相差很大，可能会造成OOM。 ForkJoin 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.ArrayList;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;import java.util.concurrent.RecursiveTask;/*** @author 宋建涛 E-mail:1872963677@qq.com* @version 创建时间：2019年6月1日 下午8:32:32* 类说明*///继承RecursiveTask，可以携带返回值public class CountTask extends RecursiveTask&lt;Long&gt; &#123; private static final int THRESHOLD = 10000; private long start; private long end; public CountTask(long start, long end) &#123; this.start = start; this.end = end; &#125; public Long compute() &#123; long sum = 0; boolean canCompute = (end-start)&lt;THRESHOLD; if(canCompute) &#123; for(long i=start;i&lt;=end;i++) &#123; sum +=i; &#125; &#125; else &#123; long step = (start+end)/100; ArrayList&lt;CountTask&gt; subTasks = new ArrayList&lt;CountTask&gt;(); long pos = start; //划分成100个小任务 for(int i=0;i&lt;100;i++) &#123; long lastOne = pos+step; if(lastOne&gt;end) lastOne=end; CountTask subTask = new CountTask(pos, lastOne); pos+=step+1; subTasks.add(subTask); //提交任务 subTask.fork(); &#125; for(CountTask t :subTasks) &#123; sum+=t.join(); &#125; &#125; return sum; &#125; public static void main(String[] args) &#123; ForkJoinPool forkJoinPool = new ForkJoinPool(); CountTask task = new CountTask(0, 200000L); ForkJoinTask&lt;Long&gt; result = forkJoinPool.submit(task); try &#123; long res = result.get(); System.out.println(res); &#125; catch (Exception e) &#123; // TODO: handle exception &#125; &#125;&#125; 逃逸分析锁消除的一项关键技术就是逃逸分析，就是观察一个变量是否会逃出每一个作用域。逃逸分析必须在server模式下进行，可以使用-XX：+DoEscapeAnalysis参数进行逃逸分析。使用-XX：+EliminateLocks参数打开锁消除。 避免死锁使用无锁或可重入锁，通过可重入锁的中断或限时等待。 创建单例不要使用双重检验方式，太垃圾了 不变模式通过回避问题而不是解决问题的态度解决多线程并发访问控制，不变对象是不需要进行同步的。不变模式可以提高系统的并发性能和并发量。 ConcurrentLinkedQueue队列性能很高，因为使用了CAS。但是使用CAS编程很难，但是我们可以利用Disruptor框架。 CPU cache的优化：解决伪共享问题。CPU有个高速缓存，高速缓存读写数据最小的单位是缓存行。如果两个变量放在一个缓存行中，在多线程访问中，可能会让缓存行失效导致效率低下，所以可以通过填充padding的方式来不让这两个变量在同一个缓存行。 冒泡排序、希尔排序都可以并行化 虽然NIO在网络操作中提供了非阻塞的方法，但是IO行为还是同步的，在IO操作准备好时就通知你，但是AIO是IO完成后才通知你。通过回调函数。 读写锁的改进：StampedLock。读写锁用的悲观锁策略，StampedLock用的乐观锁策略。 如何调试并行程序。 Jetty核心代码示例]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之同步队列SynchronousQueue]]></title>
    <url>%2F2019%2F06%2F01%2FJava%E4%B9%8B%E5%90%8C%E6%AD%A5%E9%98%9F%E5%88%97SynchronousQueue%2F</url>
    <content type="text"><![CDATA[怎么理解这个“特殊”的队列一般印象中，阻塞队列都有缓冲，但是这个队列是没有缓冲的，就是说这里边不能存东西。数据是在配对的生产者和消费者线程之间直接传递的，并不会将数据缓冲数据到队列中。可以这样来理解：生产者和消费者互相等待对方，握手（传数据，一个put，一个take），然后一起离开。 什么地方需要这个队列 非常适合于传递性设计，在这种设计中，在一个线程中运行的对象要将某些信息、事件或任务传递给在另一个线程中运行的对象，它就必须与该对象同步。 在线程池里的一个典型应用是Executors.newCachedThreadPool()就使用了SynchronousQueue，这个线程池根据需要（新任务到来时）创建新的线程，如果有空闲线程则会重复使用，线程空闲了60秒后会被回收。 全宇宙的JAVA IT人士应该都知道ThreadPoolExecutor的执行流程： core线程还能应付的,则不断的创建新的线程;core线程无法应付,则将任务扔到队列里面;队列满了(意味着插入任务失败),则开始创建MAX线程,线程数达到MAX后,队列还一直是满的,则抛出RejectedExecutionException.这个执行流程有个小问题,就是当core线程无法应付请求的时候,会立刻将任务添加到队列中,如果队列非常长,而任务又非常多,那么将会有频繁的任务入队列和任务出队列的操作。 根据实际的压测发现,这种操作也是有一定消耗的。其实JAVA提供的SynchronousQueue队列是一个零长度的队列,任务都是直接由生产者递交给消费者,中间没有入队列的过程,可见JAVA API的设计者也是有考虑过入队列这种操作的开销。 扩展ThreadPoolExecutor的一种办法(参考的tomcat的ThreadPoolExecutor源码)只有当达到MAx数量时，才放入队列。。。。而不是超过core数量时https://blog.csdn.net/linsongbin1/article/details/78275283 自己实现SynchronousQueue使用wait和notify实现阻塞算法实现通常在内部采用一个锁来保证多个线程中的put()和take()方法是串行执行的。采用锁的开销是比较大的，还会存在一种情况是线程A持有线程B需要的锁，B必须一直等待A释放锁，即使A可能一段时间内因为B的优先级比较高而得不到时间片运行。所以在高性能的应用中我们常常希望规避锁的使用。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/*** @author 宋建涛 E-mail:1872963677@qq.com* @version 创建时间：2019年6月1日 下午3:23:59* 类说明*/class NativeSynchronousQueue&lt;E&gt; &#123; boolean putting = false; E item = null; public synchronized E take() throws InterruptedException &#123; while (item == null) wait(); E e = item; item = null; notifyAll(); return e; &#125; public synchronized void put(E e) throws InterruptedException &#123; if (e == null) return; while (putting) wait(); putting = true; item = e; notifyAll(); while (item != null) wait(); putting = false; notifyAll(); &#125;&#125;public class NativeSynchronousQueueTest &#123; public static void main(String[] args) throws InterruptedException &#123; final NativeSynchronousQueue&lt;String&gt; queue = new NativeSynchronousQueue&lt;String&gt;(); Thread putThread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("put thread start"); try &#123; queue.put("1"); &#125; catch (InterruptedException e) &#123; &#125; System.out.println("put thread end"); &#125; &#125;); Thread takeThread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("take thread start"); try &#123; System.out.println("take from putThread: " + queue.take()); &#125; catch (InterruptedException e) &#123; &#125; System.out.println("take thread end"); &#125; &#125;); putThread.start(); Thread.sleep(1000); takeThread.start(); &#125;&#125; 信号量实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import java.util.concurrent.Semaphore;/*** @author 宋建涛 E-mail:1872963677@qq.com* @version 创建时间：2019年6月1日 下午3:46:28* 类说明*/class SemaphoreSynchronousQueue&lt;E&gt; &#123; E item = null; Semaphore sync = new Semaphore(0); Semaphore send = new Semaphore(1); Semaphore recv = new Semaphore(0); public E take() throws InterruptedException &#123; recv.acquire(); E x = item; sync.release(); send.release(); return x; &#125; public void put (E x) throws InterruptedException&#123; send.acquire(); item = x; recv.release(); sync.acquire(); &#125;&#125;public class SemaphoreSynchronousQueueTest &#123; public static void main(String[] args) throws InterruptedException &#123; final SemaphoreSynchronousQueue&lt;String&gt; queue = new SemaphoreSynchronousQueue&lt;String&gt;(); Thread putThread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("put thread start"); try &#123; queue.put("1"); &#125; catch (InterruptedException e) &#123; &#125; System.out.println("put thread end"); &#125; &#125;); Thread takeThread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("take thread start"); try &#123; System.out.println("take from putThread: " + queue.take()); &#125; catch (InterruptedException e) &#123; &#125; System.out.println("take thread end"); &#125; &#125;); putThread.start(); Thread.sleep(1000); takeThread.start(); &#125;&#125; Java5实现Java 5的实现相对来说做了一些优化，只使用了一个锁，使用队列代替信号量也可以允许发布者直接发布数据，而不是要首先从阻塞在信号量处被唤醒。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.util.Queue;import java.util.concurrent.locks.AbstractQueuedSynchronizer;import java.util.concurrent.locks.ReentrantLock;public class Java5SynchronousQueue&lt;E&gt; &#123; ReentrantLock qlock = new ReentrantLock(); Queue waitingProducers = new Queue(); Queue waitingConsumers = new Queue(); static class Node extends AbstractQueuedSynchronizer &#123; E item; Node next; Node(Object x) &#123; item = x; &#125; void waitForTake() &#123; /* (uses AQS) */ &#125; E waitForPut() &#123; /* (uses AQS) */ &#125; &#125; public E take() &#123; Node node; boolean mustWait; qlock.lock(); node = waitingProducers.pop(); if(mustWait = (node == null)) node = waitingConsumers.push(null); qlock.unlock(); if (mustWait) return node.waitForPut(); else return node.item; &#125; public void put(E e) &#123; Node node; boolean mustWait; qlock.lock(); node = waitingConsumers.pop(); if (mustWait = (node == null)) node = waitingProducers.push(e); qlock.unlock(); if (mustWait) node.waitForTake(); else node.item = e; &#125;&#125; java6实现Java 6的SynchronousQueue的实现采用了一种性能更好的无锁算法 — 扩展的“Dual stack and Dual queue”算法。性能比Java5的实现有较大提升。竞争机制支持公平和非公平两种：非公平竞争模式使用的数据结构是后进先出栈(Lifo Stack)；公平竞争模式则使用先进先出队列（Fifo Queue），性能上两者是相当的，一般情况下，Fifo通常可以支持更大的吞吐量，但Lifo可以更大程度的保持线程的本地化。 代码实现里的Dual Queue或Stack内部是用链表(LinkedList)来实现的，其节点状态为以下三种情况： 持有数据 – put()方法的元素持有请求 – take()方法空 这个算法的特点就是任何操作都可以根据节点的状态判断执行，而不需要用到锁。 其核心接口是Transfer，生产者的put或消费者的take都使用这个接口，根据第一个参数来区别是入列（栈）还是出列（栈）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * Shared internal API for dual stacks and queues. */ static abstract class Transferer &#123; /** * Performs a put or take. * * @param e if non-null, the item to be handed to a consumer; * if null, requests that transfer return an item * offered by producer. * @param timed if this operation should timeout * @param nanos the timeout, in nanoseconds * @return if non-null, the item provided or received; if null, * the operation failed due to timeout or interrupt -- * the caller can distinguish which of these occurred * by checking Thread.interrupted. */ abstract Object transfer(Object e, boolean timed, long nanos); &#125;TransferQueue实现如下(摘自Java 6源代码)，入列和出列都基于Spin和CAS方法：/** * Puts or takes an item. */ Object transfer(Object e, boolean timed, long nanos) &#123; /* Basic algorithm is to loop trying to take either of * two actions: * * 1. If queue apparently empty or holding same-mode nodes, * try to add node to queue of waiters, wait to be * fulfilled (or cancelled) and return matching item. * * 2. If queue apparently contains waiting items, and this * call is of complementary mode, try to fulfill by CAS'ing * item field of waiting node and dequeuing it, and then * returning matching item. * * In each case, along the way, check for and try to help * advance head and tail on behalf of other stalled/slow * threads. * * The loop starts off with a null check guarding against * seeing uninitialized head or tail values. This never * happens in current SynchronousQueue, but could if * callers held non-volatile/final ref to the * transferer. The check is here anyway because it places * null checks at top of loop, which is usually faster * than having them implicitly interspersed. */ QNode s = null; // constructed/reused as needed boolean isData = (e != null); for (;;) &#123; QNode t = tail; QNode h = head; if (t == null || h == null) // saw uninitialized value continue; // spin if (h == t || t.isData == isData) &#123; // empty or same-mode QNode tn = t.next; if (t != tail) // inconsistent read continue; if (tn != null) &#123; // lagging tail advanceTail(t, tn); continue; &#125; if (timed &amp;amp;&amp;amp; nanos &amp;lt;= 0) // can't wait return null; if (s == null) s = new QNode(e, isData); if (!t.casNext(null, s)) // failed to link in continue; advanceTail(t, s); // swing tail and wait Object x = awaitFulfill(s, e, timed, nanos); if (x == s) &#123; // wait was cancelled clean(t, s); return null; &#125; if (!s.isOffList()) &#123; // not already unlinked advanceHead(t, s); // unlink if head if (x != null) // and forget fields s.item = s; s.waiter = null; &#125; return (x != null)? x : e; &#125; else &#123; // complementary-mode QNode m = h.next; // node to fulfill if (t != tail || m == null || h != head) continue; // inconsistent read Object x = m.item; if (isData == (x != null) || // m already fulfilled x == m || // m cancelled !m.casItem(x, e)) &#123; // lost CAS advanceHead(h, m); // dequeue and retry continue; &#125; advanceHead(h, m); // successfully fulfilled LockSupport.unpark(m.waiter); return (x != null)? x : e; &#125; &#125; &#125; java8实现待补充。。。 参考http://ifeve.com/java-synchronousqueue/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[压测jmeter我的秒杀]]></title>
    <url>%2F2019%2F05%2F31%2F%E5%8E%8B%E6%B5%8Bjmeter%E6%88%91%E7%9A%84%E7%A7%92%E6%9D%80%2F</url>
    <content type="text"><![CDATA[jmeter步骤 启动bin目录下的jmeter.bat 添加一个thread group，设置参数比如1000,0,1（循环一次） 添加一个config element里的http default request 添加sample 里的http request 添加一个聚合报告，run 压测结果throughput才38.1/sec,这也太低了吧。为啥。也就是说一秒钟只能接受38个请求。。改变了下线程池的参数，变为执行10次，即10000次，throughout达到了273.59。也太低。。每次跑，结果都不一样啊，最高1000。 查看dump看了下进程日志，发现了很多信息。 dump文件中描述的线程状态runnable：运行中状态，在虚拟机内部执行，可能已经获取到了锁，可以观察是否有locked字样。blocked：被阻塞并等待锁的释放。wating：处于等待状态，等待特定的操作被唤醒，一般停留在park(), wait(), sleep(),join() 等语句里。time_wating：有时限的等待另一个线程的特定操作。terminated：线程已经退出。 进程区域的划分进入区====拥有区====等待区 进入区（Entry Set）：等待获取对象锁，一旦对象锁释放，立即参与竞争。拥有区（The Owner）：已经获取到锁。等待区（Wait Set）：表示线程通过wait方法释放了对象锁，并在等待区等待被唤醒。 方法调用修饰locked: 成功获取锁waiting to lock：还未获取到锁，在进入去等待；waiting on：获取到锁之后，又释放锁，在等待区等待；parking to wait for：等待许可证； （参考LockSupport.park和unpark操作） 举个栗子：12345678"Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007f94c81d2800 nid=0x13 in Object.wait() [0x00007f94b4cd7000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000006c7226010&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143) - locked &lt;0x00000006c7226010&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164) at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209) 意外发现的小问题 部署在docker里，返回的数据有乱码。我看了一下编码方式是UTF-8啊，没毛病啊。解决办法：在dockerfile里，添加ENV LANG C.UTF-8.重新制作docker镜像，docker run -ti [镜像] 进入容器后执行locale发现编码格式已经被修改为C.UTF-8，之前出现的中文文件名乱码问题也没有了。 在Request Header里的Cookie: Hm_lvt_1fc1803bb58c30947181c3b6b65e7d9d=1552486046; token=22ad4279f56a4fa6a39b6e7ea3b23bdf，其中Hm_lvt是啥，其实这是百度统计的东西，不用管。 参考https://blog.csdn.net/sheldon178/article/details/79543671]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我向github提的第一个issue]]></title>
    <url>%2F2019%2F05%2F31%2F%E6%88%91%E5%90%91github%E6%8F%90%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AAissue%2F</url>
    <content type="text"><![CDATA[缘起平常经常看到github上有人提issue，今天我也提了一个。是一个kafka镜像起来之后存在僵尸进程的问题。https://github.com/wurstmeister/kafka-docker/issues/497 解决方案作者说需要引入一个dumb-init方案来解决这个场景，我说为啥不用&lt;https://github.com/phusion/baseimage-docker/blob/rel-0.9.16/image/bin/my_init &gt;这个解决方案，作者说The Phusion solution requires Python - which seems like a lot of extra baggage to pull in (100MBs vs &lt; 1Mb) dumb-init的解决方案https://github.com/Yelp/dumb-init#why-you-need-an-init-system 一篇很好的文章https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/ phusion的解决方案https://github.com/phusion/baseimage-docker/blob/rel-0.9.16/image/bin/my_init这个是用python来写的，脚本源码在这，我想弄明白他的原理，为什么他能避免这个问题呢？ 其实最主要的就是一个正确的init进程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341#!/usr/bin/python3 -u （-u是干嘛的啊，还不知道）import os, os.path, sys, stat, signal, errno, argparse, time, json, reKILL_PROCESS_TIMEOUT = 5KILL_ALL_PROCESSES_TIMEOUT = 5LOG_LEVEL_ERROR = 1LOG_LEVEL_WARN = 1LOG_LEVEL_INFO = 2LOG_LEVEL_DEBUG = 3log_level = Noneterminated_child_processes = &#123;&#125;class AlarmException(Exception): passdef error(message): if log_level &gt;= LOG_LEVEL_ERROR: sys.stderr.write("*** %s\n" % message)def warn(message): if log_level &gt;= LOG_LEVEL_WARN: sys.stderr.write("*** %s\n" % message)def info(message): if log_level &gt;= LOG_LEVEL_INFO: sys.stderr.write("*** %s\n" % message)def debug(message): if log_level &gt;= LOG_LEVEL_DEBUG: sys.stderr.write("*** %s\n" % message)def ignore_signals_and_raise_keyboard_interrupt(signame): signal.signal(signal.SIGTERM, signal.SIG_IGN) signal.signal(signal.SIGINT, signal.SIG_IGN) raise KeyboardInterrupt(signame)def raise_alarm_exception(): raise AlarmException('Alarm')def listdir(path): try: result = os.stat(path) except OSError: return [] if stat.S_ISDIR(result.st_mode): return sorted(os.listdir(path)) else: return []def is_exe(path): try: return os.path.isfile(path) and os.access(path, os.X_OK) except OSError: return Falsedef import_envvars(clear_existing_environment = True, override_existing_environment = True): new_env = &#123;&#125; for envfile in listdir("/etc/container_environment"): name = os.path.basename(envfile) with open("/etc/container_environment/" + envfile, "r") as f: # Text files often end with a trailing newline, which we # don't want to include in the env variable value. See # https://github.com/phusion/baseimage-docker/pull/49 value = re.sub('\n\Z', '', f.read()) new_env[name] = value if clear_existing_environment: os.environ.clear() for name, value in new_env.items(): if override_existing_environment or not name in os.environ: os.environ[name] = valuedef export_envvars(to_dir = True): shell_dump = "" for name, value in os.environ.items(): if name in ['HOME', 'USER', 'GROUP', 'UID', 'GID', 'SHELL']: continue if to_dir: with open("/etc/container_environment/" + name, "w") as f: f.write(value) shell_dump += "export " + shquote(name) + "=" + shquote(value) + "\n" with open("/etc/container_environment.sh", "w") as f: f.write(shell_dump) with open("/etc/container_environment.json", "w") as f: f.write(json.dumps(dict(os.environ)))_find_unsafe = re.compile(r'[^\w@%+=:,./-]').searchdef shquote(s): """Return a shell-escaped version of the string *s*.""" if not s: return "''" if _find_unsafe(s) is None: return s # use single quotes, and put single quotes into double quotes # the string $'b is then quoted as '$'"'"'b' return "'" + s.replace("'", "'\"'\"'") + "'"# Waits for the child process with the given PID, while at the same time# reaping any other child processes that have exited (e.g. adopted child# processes that have terminated).（主要逻辑在这）def waitpid_reap_other_children(pid): global terminated_child_processes status = terminated_child_processes.get(pid) if status: # A previous call to waitpid_reap_other_children(), # with an argument not equal to the current argument, # already waited for this process. Return the status # that was obtained back then. del terminated_child_processes[pid] return status done = False status = None while not done: try: this_pid, status = os.waitpid(-1, 0) if this_pid == pid: done = True else: # Save status for later. terminated_child_processes[this_pid] = status except OSError as e: if e.errno == errno.ECHILD or e.errno == errno.ESRCH: return None else: raise return statusdef stop_child_process(name, pid, signo = signal.SIGTERM, time_limit = KILL_PROCESS_TIMEOUT): info("Shutting down %s (PID %d)..." % (name, pid)) try: os.kill(pid, signo) except OSError: pass signal.alarm(time_limit) try: try: waitpid_reap_other_children(pid) except OSError: pass except AlarmException: warn("%s (PID %d) did not shut down in time. Forcing it to exit." % (name, pid)) try: os.kill(pid, signal.SIGKILL) except OSError: pass try: waitpid_reap_other_children(pid) except OSError: pass finally: signal.alarm(0)def run_command_killable(*argv): filename = argv[0] status = None pid = os.spawnvp(os.P_NOWAIT, filename, argv) try: status = waitpid_reap_other_children(pid) except BaseException as s: warn("An error occurred. Aborting.") stop_child_process(filename, pid) raise if status != 0: if status is None: error("%s exited with unknown status\n" % filename) else: error("%s failed with status %d\n" % (filename, os.WEXITSTATUS(status))) sys.exit(1)def run_command_killable_and_import_envvars(*argv): run_command_killable(*argv) import_envvars() export_envvars(False)def kill_all_processes(time_limit): info("Killing all processes...") try: os.kill(-1, signal.SIGTERM) except OSError: pass signal.alarm(time_limit) try: # Wait until no more child processes exist. done = False while not done: try: os.waitpid(-1, 0) except OSError as e: if e.errno == errno.ECHILD: done = True else: raise except AlarmException: warn("Not all processes have exited in time. Forcing them to exit.") try: os.kill(-1, signal.SIGKILL) except OSError: pass finally: signal.alarm(0)def run_startup_files(): # Run /etc/my_init.d/* for name in listdir("/etc/my_init.d"): filename = "/etc/my_init.d/" + name if is_exe(filename): info("Running %s..." % filename) run_command_killable_and_import_envvars(filename) # Run /etc/rc.local. if is_exe("/etc/rc.local"): info("Running /etc/rc.local...") run_command_killable_and_import_envvars("/etc/rc.local")def start_runit(): info("Booting runit daemon...") pid = os.spawnl(os.P_NOWAIT, "/usr/bin/runsvdir", "/usr/bin/runsvdir", "-P", "/etc/service") info("Runit started as PID %d" % pid) return piddef wait_for_runit_or_interrupt(pid): try: status = waitpid_reap_other_children(pid) return (True, status) except KeyboardInterrupt: return (False, None)def shutdown_runit_services(): debug("Begin shutting down runit services...") os.system("/usr/bin/sv down /etc/service/*")def wait_for_runit_services(): debug("Waiting for runit services to exit...") done = False while not done: done = os.system("/usr/bin/sv status /etc/service/* | grep -q '^run:'") != 0 if not done: time.sleep(0.1)def install_insecure_key(): info("Installing insecure SSH key for user root") run_command_killable("/usr/sbin/enable_insecure_key")def main(args): import_envvars(False, False) export_envvars() if args.enable_insecure_key: install_insecure_key() if not args.skip_startup_files: run_startup_files() runit_exited = False exit_code = None if not args.skip_runit: runit_pid = start_runit() try: exit_status = None if len(args.main_command) == 0: runit_exited, exit_code = wait_for_runit_or_interrupt(runit_pid) if runit_exited: if exit_code is None: info("Runit exited with unknown status") exit_status = 1 else: exit_status = os.WEXITSTATUS(exit_code) info("Runit exited with status %d" % exit_status) else: info("Running %s..." % " ".join(args.main_command)) pid = os.spawnvp(os.P_NOWAIT, args.main_command[0], args.main_command) try: exit_code = waitpid_reap_other_children(pid) if exit_code is None: info("%s exited with unknown status." % args.main_command[0]) exit_status = 1 else: exit_status = os.WEXITSTATUS(exit_code) info("%s exited with status %d." % (args.main_command[0], exit_status)) except KeyboardInterrupt: stop_child_process(args.main_command[0], pid) raise except BaseException as s: warn("An error occurred. Aborting.") stop_child_process(args.main_command[0], pid) raise sys.exit(exit_status) finally: if not args.skip_runit: shutdown_runit_services() if not runit_exited: stop_child_process("runit daemon", runit_pid) wait_for_runit_services()# Parse options.parser = argparse.ArgumentParser(description = 'Initialize the system.')parser.add_argument('main_command', metavar = 'MAIN_COMMAND', type = str, nargs = '*', help = 'The main command to run. (default: runit)')parser.add_argument('--enable-insecure-key', dest = 'enable_insecure_key', action = 'store_const', const = True, default = False, help = 'Install the insecure SSH key')parser.add_argument('--skip-startup-files', dest = 'skip_startup_files', action = 'store_const', const = True, default = False, help = 'Skip running /etc/my_init.d/* and /etc/rc.local')parser.add_argument('--skip-runit', dest = 'skip_runit', action = 'store_const', const = True, default = False, help = 'Do not run runit services')parser.add_argument('--no-kill-all-on-exit', dest = 'kill_all_on_exit', action = 'store_const', const = False, default = True, help = 'Don\'t kill all processes on the system upon exiting')parser.add_argument('--quiet', dest = 'log_level', action = 'store_const', const = LOG_LEVEL_WARN, default = LOG_LEVEL_INFO, help = 'Only print warnings and errors')args = parser.parse_args()log_level = args.log_levelif args.skip_runit and len(args.main_command) == 0: error("When --skip-runit is given, you must also pass a main command.") sys.exit(1)# Run main function.signal.signal(signal.SIGTERM, lambda signum, frame: ignore_signals_and_raise_keyboard_interrupt('SIGTERM'))signal.signal(signal.SIGINT, lambda signum, frame: ignore_signals_and_raise_keyboard_interrupt('SIGINT'))signal.signal(signal.SIGALRM, lambda signum, frame: raise_alarm_exception())try: main(args)except KeyboardInterrupt: warn("Init system aborted.") exit(2)finally: if args.kill_all_on_exit: kill_all_processes(KILL_ALL_PROCESSES_TIMEOUT)]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程中的大小端字节序]]></title>
    <url>%2F2019%2F05%2F27%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%A4%A7%E5%B0%8F%E7%AB%AF%E5%AD%97%E8%8A%82%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[什么是大小端字节序端模式：字节序分为大端字节序和小端字节序，也就是字节在内存中的顺序。举个栗子：12345678910111213141516171819小端字节序：低字节存放于内存低地址，高字节存放于内存高位地址。 如一个数据0x12345678 0x0029f458 0x78 0x0029f459 0x56 0x0029f45a 0x34 0x0029f45b 0x12 内存地址的大小顺序为0x0029f458&lt; 0x0029f459&lt;0x0029f45a&lt;0x0029f45b 数据的大小顺序为0x78&lt;0x56&lt;0x34&lt;0x12 小数据在小的内存地址中，所以为小端字节序。 大端字节序和以上就相反。 注意：网络字节序都是大端字节序。这是为什么呢？？？？？ 因为TCP、UDP、IP协议规定了，把接受到的第一个字节当做高位字节处理，但是对于发送方来说呢，发送的第一个字节在内存的低地址处，所以就相当于高位对应着地位，这样就是大端字节序了。如果发送方和接收方平台不一样，必须进行转换。 各自优缺点 大端模式优点： 符号位在所表示的数据的内存的第一个字节中，便于快速判断数据的正负和大小 小端模式优点： 内存的低地址处存放低字节，所以在强制转换数据时不需要调整字节的内容（注解：比如把int的4字节强制转换成short的2字节时，就直接把int数据存储的前两个字节给short就行，因为其前两个字节刚好就是最低的两个字节，符合转换逻辑）；CPU做数值运算时从内存中依顺序依次从低位到高位取数据进行运算，直到最后刷新最高位的符号位，这样的运算方式会更高效 如何判断当前平台是哪种？可以通过写程序的方式，通过取比如一个int的第一个字节来判断。123456789101112131415#include&lt;stdio.h&gt;int main()&#123; int a = 1;//这里为了方便，以1为例 char*p = (char*)(&amp;a); if (*p == 1) &#123; printf("little endian\n");//小端存储 &#125; else &#123; printf("big endian\n");//大端存储 &#125; return 0;&#125; 如何转换 在c语言中， 有htonl、ntohl、htons、ntohs等函数。h代表host，n代表net。一段调用hton的时候，在另一端接收到的时候，只要对应的调用ntoh。那就是不会有问题的。主机cpu的结构决定了自己本端是大端还是小端。如果自己本端本身本身就是大端字节序，那么就收到网络上的数据之后，就算执行ntoh函数，也是不做任何的处理。因为就是相当于大端转化为大端了。ntoh函数内部会判断主机本身是大端还是小端序。 参考https://blog.csdn.net/qq_22945931/article/details/79649008https://blog.csdn.net/melody_1016/article/details/81910873]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-IOC-容器源码解析]]></title>
    <url>%2F2019%2F05%2F27%2FSpring-IOC-%E5%AE%B9%E5%99%A8%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[今天读到了一篇写Spring源码的文章，非常长但也非常好，记录一下。以便后期查看。https://www.javadoop.com/post/spring-ioc]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参加dubbo开发者日]]></title>
    <url>%2F2019%2F05%2F26%2F%E5%8F%82%E5%8A%A0dubbo%E5%BC%80%E5%8F%91%E8%80%85%E6%97%A5%2F</url>
    <content type="text"><![CDATA[今天参加了一个dubbo开发者日，听说有小马哥就去了。上午讲了三个分享。第一个介绍了springcloud alibaba生态；第二个介绍了seata分布式事务，为什么引入seata呢？因为xa协议本来设计就不是适用于分布式的，xa加了很多很多的锁，但是又因为大部分情况是不需要加那么多锁的 ，所以seate的处理方式就是尝试把锁打碎，这样性能就能上去了。另外，seata也可以兼容at模式，tcc模式，saga模式甚至xa模式。其中有人提了个问题就是，如何保障tc（seata包含tm事务管理者、tc事务协调者和rm资源管理者）的高可用，老师提示可以利用分布式kv存储，这样即使某个存储挂了，也能工作。还有一个问题是，undo日志执行失败了怎么办，老师提示可以利用全局锁，会申请一个锁来保证全局事务，如果全局锁不释放，别的是操控不了的。seata的设计理念好像是基于xid全局事务id，tm确定事务边界，也就是说事务什么时候开始，什么时候结束。通过与tc的协调合作，保障每个分支事务正确执行，如果哪个分支事务执行失败可以进行回滚。 自己对分布式事务的理解 我们可以通过消息实现分布式事务方案，比如这样的一个场景，某笔订单成功后，为用户加一定的积分。但是需要注意的是如何保证消息的可靠性。 还可以通过补偿来实现，比基于消息的要难。阿里GTS/fescar本质上也是这补偿的编程模型，只不过补偿代码自动生成，无需业务干预，同时接管应用数据源，禁止业务修改处于全局事务状态中的记录。因此，其关于读场景的适用性，可参考补偿。但其在写的适用场景由于引入了全局事务时的写锁，其写适用性介于 TCC以及补偿之间 。 还可以基于TCC。TCC实际上是最为复杂的一种情况，其能处理所有的业务场景，但无论出于性能上的考虑，还是开发复杂度上的考虑，都应该尽量避免该类事务。 还可以基于SAGA，SAGA可以看做一个异步的、利用队列实现的补偿事务。 不同业务场景应按需引入不同的事务形态。 实现分布式事务的关键点： 重试记录：通过数据记录保存。重试机制：定时任务或者消息队列自带的重试。幂等：通过状态机加数据库行锁。https://juejin.im/post/5cfdbe06e51d45775653676d下午有4个分享，其中有一个是sentinel 1.6网关限流新特性介绍。 还有小马哥的分享是Apache Dubbo服务自省设计与实现。小马哥的博客在这里https://mercyblitz.github.io/about/。其中主要提到了dubbo2.7 把配置分成了三个部分，服务信息，元数据信息等，这样分散开始为了减轻zk的压力，dubbo3.0还会结合k8s。还提到了一个优化点是而且也简化了url的参数，在2.7.3版本会提供一种轻量的服务注册机制，减小粒度。提到了zk的缺点，为什么性能上不去呢，因为zk一下子起那么多路径，肯定上不去。而且zk是个CP系统，本身就不是很适合做注册中心。最后一个老师的分享是为什么把注册中心从zk迁移到nacos。]]></content>
      <categories>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一直卡死的实验]]></title>
    <url>%2F2019%2F05%2F25%2F%E4%B8%80%E7%9B%B4%E5%8D%A1%E6%AD%BB%E7%9A%84%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[今天我做了个实验报错了,是把数据从一方传到远程的kafka里。发送方启动server.jar,kafka的接收方也启动server.jar。 java -jar server-v1.0.jar -d C:\Users\Administrator\Desktop\test\ -s 172.21.6.57 -t taxi 传输了不久之后，报错了。报错日志如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384## A fatal error has been detected by the Java Runtime Environment:## EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x0000000002be853a, pid=11956, tid=0x0000000000001e10## JRE version: Java(TM) SE Runtime Environment (8.0_171-b11) (build 1.8.0_171-b11)# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.171-b11 mixed mode windows-amd64 compressed oops)# Problematic frame:# J 2149 C2 sun.nio.ch.IOUtil.write(Ljava/io/FileDescriptor;[Ljava/nio/ByteBuffer;IILsun/nio/ch/NativeDispatcher;)J (509 bytes) @ 0x0000000002be853a [0x0000000002be7e40+0x6fa]## Failed to write core dump. Minidumps are not enabled by default on client versions of Windows## If you would like to submit a bug report, please visit:# http://bugreport.java.com/bugreport/crash.jsp#--------------- T H R E A D ---------------Current thread (0x0000000019b3e000): JavaThread "nioEventLoopGroup-2-1" [_thread_in_Java, id=7696, stack(0x0000000019f50000,0x000000001a050000)]siginfo: ExceptionCode=0xc0000005, writing address 0x0000000033960040Registers:RAX=0x00000000d36695f0, RBX=0x0000000000000008, RCX=0x000000001e25ebc0, RDX=0x00000000d7ee5360RSP=0x000000001a04ef50, RBP=0x00000000d36695f0, RSI=0x000000000000b5a4, RDI=0x0000000000000000R8 =0x000000000045bf8c, R9 =0x00000000ded44a00, R10=0x0000000033960040, R11=0x0000000000000039R12=0x0000000000000000, R13=0x00000000d5c00000, R14=0x00000000d47d9430, R15=0x0000000019b3e000RIP=0x0000000002be853a, EFLAGS=0x0000000000010202Top of Stack: (sp=0x000000001a04ef50)0x000000001a04ef50: 0000000000000001 00000000000000200x000000001a04ef60: 000000001a04efb8 00000000000000000x000000001a04ef70: 0000000000800000 00000000ca036da00x000000001a04ef80: 00000000819b6ea8 00000000000000000x000000001a04ef90: 0000b5a40000b5a4 00000000817b09c00x000000001a04efa0: 0000000019b3e000 00000000819b6c000x000000001a04efb0: 00000000819b6ce0 00000000d36695c00x000000001a04efc0: 00000000819bc618 00000000819bc5780x000000001a04efd0: 00000000819bc5a8 00000000819bc6180x000000001a04efe0: 0000000000000001 0000000002cfda2c0x000000001a04eff0: 00000000df8e3998 00000001400731a80x000000001a04f000: 00000000818cf500 00000000df8eac080x000000001a04f010: 00000001df8e3998 00000000ca036da00x000000001a04f020: 0000b5a40014578f 00000000819b6dd80x000000001a04f030: 00000000819b6ee0 00000000df8e39980x000000001a04f040: 000000012803e413 0000000000000001 Instructions: (pc=0x0000000002be853a)0x0000000002be851a: 00 28 0f 85 47 17 00 00 48 63 c9 49 03 49 10 c50x0000000002be852a: f9 7e cb 83 fb 04 0f 84 d7 0f 00 00 4c 8b 52 180x0000000002be853a: 49 89 0a 4d 63 db 4d 89 5a 08 41 ba 01 00 00 000x0000000002be854a: c4 e1 f9 7e c0 e9 10 fb ff ff 0f 1f 84 00 00 00 Register to memory mapping:RAX=0x00000000d36695f0 is an oop[Ljava.nio.ByteBuffer; - klass: 'java/nio/ByteBuffer'[] - length: 4571020RBX=0x0000000000000008 is an unknown valueRCX=0x000000001e25ebc0 is an unknown valueRDX=0x00000000d7ee5360 is an oopsun.nio.ch.AllocatedNativeObject - klass: 'sun/nio/ch/AllocatedNativeObject'RSP=0x000000001a04ef50 is pointing into the stack for thread: 0x0000000019b3e000RBP=0x00000000d36695f0 is an oop[Ljava.nio.ByteBuffer; - klass: 'java/nio/ByteBuffer'[] - length: 4571020RSI=0x000000000000b5a4 is an unknown valueRDI=0x0000000000000000 is an unknown valueR8 =0x000000000045bf8c is an unknown valueR9 =0x00000000ded44a00 is an oopjava.nio.DirectByteBuffer - klass: 'java/nio/DirectByteBuffer'R10=0x0000000033960040 is an unknown valueR11=0x0000000000000039 is an unknown valueR12=0x0000000000000000 is an unknown valueR13=0x00000000d5c00000 is an oop[I - klass: &#123;type array int&#125; - length: 4571020R14=0x00000000d47d9430 is an oop[I - klass: &#123;type array int&#125; - length: 4571020R15=0x0000000019b3e000 is a threadStack: [0x0000000019f50000,0x000000001a050000], sp=0x000000001a04ef50, free space=1019kNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)C 0x0000000002be853a--------------- P R O C E S S ---------------Java Threads: ( =&gt; current thread ) 0x000000001a5b9800 JavaThread "Thread-1" [_thread_blocked, id=15172, stack(0x000000001cf70000,0x000000001d070000)] 0x000000001a5c9000 JavaThread "kafka-producer-network-thread | producer-1" daemon [_thread_in_native, id=18268, stack(0x000000001ccf0000,0x000000001cdf0000)] 0x000000001a5f8800 JavaThread "threadDeathWatcher-3-1" daemon [_thread_blocked, id=15976, stack(0x000000001bbe0000,0x000000001bce0000)]=&gt;0x0000000019b3e000 JavaThread "nioEventLoopGroup-2-1" [_thread_in_Java, id=7696, stack(0x0000000019f50000,0x000000001a050000)] 0x0000000018965000 JavaThread "Service Thread" daemon [_thread_blocked, id=10604, stack(0x0000000019380000,0x0000000019480000)] 0x00000000188f8000 JavaThread "C1 CompilerThread2" daemon [_thread_blocked, id=19388, stack(0x0000000019280000,0x0000000019380000)] 0x00000000188f4000 JavaThread "C2 CompilerThread1" daemon [_thread_blocked, id=9220, stack(0x0000000019160000,0x0000000019260000)] 0x000000001762f000 JavaThread "C2 CompilerThread0" daemon [_thread_blocked, id=1804, stack(0x0000000018e20000,0x0000000018f20000)] 0x00000000188c8800 JavaThread "Attach Listener" daemon [_thread_blocked, id=9820, stack(0x0000000018f50000,0x0000000019050000)] 0x0000000017620000 JavaThread "Signal Dispatcher" daemon [_thread_blocked, id=8704, stack(0x0000000018d00000,0x0000000018e00000)] 0x000000001760a000 JavaThread "Finalizer" daemon [_thread_blocked, id=6588, stack(0x00000000187c0000,0x00000000188c0000)] 0x00000000175c3000 JavaThread "Reference Handler" daemon [_thread_blocked, id=6064, stack(0x0000000018460000,0x0000000018560000)] 0x000000000020e800 JavaThread "main" [_thread_blocked, id=19140, stack(0x0000000002610000,0x0000000002710000)]Other Threads: 0x00000000175bb800 VMThread [stack: 0x00000000185e0000,0x00000000186e0000] [id=8132] 0x000000001896e800 WatcherThread [stack: 0x0000000019640000,0x0000000019740000] [id=14692]VM state:not at safepoint (normal execution)VM Mutex/Monitor currently owned by a thread: NoneHeap: PSYoungGen total 461312K, used 173083K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 71% used [0x00000000d5c00000,0x00000000e0506c90,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1381668K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5949270,0x00000000d5c00000) Metaspace used 17921K, capacity 18118K, committed 18432K, reserved 1064960K class space used 2194K, capacity 2284K, committed 2304K, reserved 1048576KCard table byte_map: [0x0000000011ad0000,0x0000000011ed0000] byte_map_base: 0x00000000116c6000Marking Bits: (ParMarkBitMap*) 0x00000000531208c0 Begin Bits: [0x00000000128d0000, 0x0000000014880000) End Bits: [0x0000000014880000, 0x0000000016830000)Polling page: 0x0000000000140000CodeCache: size=245760Kb used=5665Kb max_used=6455Kb free=240094Kb bounds [0x0000000002710000, 0x0000000002d70000, 0x0000000011710000] total_blobs=2312 nmethods=1836 adapters=388 compilation: enabledCompilation events (10 events):Event: 418.857 Thread 0x00000000188f8000 2213 3 io.netty.util.Recycler$Stack::pushLater (93 bytes)Event: 418.857 Thread 0x000000001762f000 2214 4 io.netty.util.Recycler::access$2100 (4 bytes)Event: 418.857 Thread 0x000000001762f000 nmethod 2214 0x000000000285ae50 code [0x000000000285af80, 0x000000000285afd8]Event: 418.858 Thread 0x00000000188f8000 nmethod 2213 0x0000000002bd32d0 code [0x0000000002bd35c0, 0x0000000002bd5008]Event: 418.861 Thread 0x00000000188f4000 2215 4 io.netty.util.Recycler$WeakOrderQueue::reserveSpace (46 bytes)Event: 418.862 Thread 0x00000000188f4000 nmethod 2215 0x0000000002b1f2d0 code [0x0000000002b1f400, 0x0000000002b1f478]Event: 418.862 Thread 0x000000001762f000 2216 4 io.netty.util.Recycler$Stack::pushLater (93 bytes)Event: 418.863 Thread 0x00000000188f4000 2217 4 io.netty.util.Recycler$WeakOrderQueue::add (84 bytes)Event: 418.867 Thread 0x00000000188f4000 nmethod 2217 0x00000000029588d0 code [0x0000000002958a20, 0x0000000002958d18]Event: 418.893 Thread 0x000000001762f000 nmethod 2216 0x0000000002842710 code [0x0000000002842900, 0x0000000002843e50]GC Heap History (10 events):Event: 392.153 GC heap before&#123;Heap before GC invocations=219 (full 62): PSYoungGen total 461312K, used 231300K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 95% used [0x00000000d5c00000,0x00000000e3de1358,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1384162K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5bb8b50,0x00000000d5c00000) Metaspace used 17944K, capacity 18158K, committed 18432K, reserved 1064960K class space used 2200K, capacity 2294K, committed 2304K, reserved 1048576KEvent: 395.350 GC heap afterHeap after GC invocations=219 (full 62): PSYoungGen total 461312K, used 222710K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 92% used [0x00000000d5c00000,0x00000000e357d948,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1384162K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5bb8b50,0x00000000d5c00000) Metaspace used 17944K, capacity 18158K, committed 18432K, reserved 1064960K class space used 2200K, capacity 2294K, committed 2304K, reserved 1048576K&#125;Event: 395.357 GC heap before&#123;Heap before GC invocations=220 (full 63): PSYoungGen total 461312K, used 241664K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 100% used [0x00000000d5c00000,0x00000000e4800000,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1384162K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5bb8b50,0x00000000d5c00000) Metaspace used 17944K, capacity 18158K, committed 18432K, reserved 1064960K class space used 2200K, capacity 2294K, committed 2304K, reserved 1048576KEvent: 400.346 GC heap afterHeap after GC invocations=220 (full 63): PSYoungGen total 461312K, used 169771K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 70% used [0x00000000d5c00000,0x00000000e01cae08,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1384088K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5ba6170,0x00000000d5c00000) Metaspace used 17944K, capacity 18158K, committed 18432K, reserved 1064960K class space used 2200K, capacity 2294K, committed 2304K, reserved 1048576K&#125;Event: 400.471 GC heap before&#123;Heap before GC invocations=221 (full 64): PSYoungGen total 461312K, used 233668K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 96% used [0x00000000d5c00000,0x00000000e4031180,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1384088K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5ba6170,0x00000000d5c00000) Metaspace used 17944K, capacity 18158K, committed 18432K, reserved 1064960K class space used 2200K, capacity 2294K, committed 2304K, reserved 1048576KEvent: 403.654 GC heap afterHeap after GC invocations=221 (full 64): PSYoungGen total 461312K, used 225026K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 93% used [0x00000000d5c00000,0x00000000e37c0b78,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1384088K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5ba6170,0x00000000d5c00000) Metaspace used 17944K, capacity 18158K, committed 18432K, reserved 1064960K class space used 2200K, capacity 2294K, committed 2304K, reserved 1048576K&#125;Event: 403.654 GC heap before&#123;Heap before GC invocations=222 (full 65): PSYoungGen total 461312K, used 225026K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 93% used [0x00000000d5c00000,0x00000000e37c0b78,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1384088K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5ba6170,0x00000000d5c00000) Metaspace used 17944K, capacity 18158K, committed 18432K, reserved 1064960K class space used 2200K, capacity 2294K, committed 2304K, reserved 1048576KEvent: 414.327 GC heap afterHeap after GC invocations=222 (full 65): PSYoungGen total 461312K, used 224508K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 92% used [0x00000000d5c00000,0x00000000e373f278,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1384121K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5bae6e8,0x00000000d5c00000) Metaspace used 17918K, capacity 18118K, committed 18432K, reserved 1064960K class space used 2194K, capacity 2284K, committed 2304K, reserved 1048576K&#125;Event: 414.506 GC heap before&#123;Heap before GC invocations=223 (full 66): PSYoungGen total 461312K, used 241664K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 100% used [0x00000000d5c00000,0x00000000e4800000,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1384124K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5baf358,0x00000000d5c00000) Metaspace used 17921K, capacity 18118K, committed 18432K, reserved 1064960K class space used 2194K, capacity 2284K, committed 2304K, reserved 1048576KEvent: 418.168 GC heap afterHeap after GC invocations=223 (full 66): PSYoungGen total 461312K, used 39793K [0x00000000d5c00000, 0x0000000100000000, 0x0000000100000000) eden space 241664K, 16% used [0x00000000d5c00000,0x00000000d82dc588,0x00000000e4800000) from space 219648K, 0% used [0x00000000f2980000,0x00000000f2980000,0x0000000100000000) to space 225280K, 0% used [0x00000000e4800000,0x00000000e4800000,0x00000000f2400000) ParOldGen total 1384448K, used 1381668K [0x0000000081400000, 0x00000000d5c00000, 0x00000000d5c00000) object space 1384448K, 99% used [0x0000000081400000,0x00000000d5949270,0x00000000d5c00000) Metaspace used 17921K, capacity 18118K, committed 18432K, reserved 1064960K class space used 2194K, capacity 2284K, committed 2304K, reserved 1048576K&#125;Deoptimization events (10 events):Event: 414.331 Thread 0x0000000019b3e000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x0000000002d62914 method=io.netty.buffer.PoolChunkList.free(Lio/netty/buffer/PoolChunk;J)Z @ 13Event: 414.409 Thread 0x0000000019b3e000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x0000000002c2c29c method=io.netty.util.Recycler$Stack.pushNow(Lio/netty/util/Recycler$DefaultHandle;)V @ 44Event: 414.409 Thread 0x0000000019b3e000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x0000000002c20a14 method=io.netty.util.Recycler$Stack.pushNow(Lio/netty/util/Recycler$DefaultHandle;)V @ 44Event: 414.466 Thread 0x0000000019b3e000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x0000000002b60620 method=io.netty.buffer.PoolArena.freeChunk(Lio/netty/buffer/PoolChunk;JLio/netty/buffer/PoolArena$SizeClass;)V @ 96Event: 418.187 Thread 0x0000000019b3e000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x0000000002cc8478 method=io.netty.buffer.PoolArena.freeChunk(Lio/netty/buffer/PoolChunk;JLio/netty/buffer/PoolArena$SizeClass;)V @ 96Event: 418.230 Thread 0x0000000019b3e000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x0000000002ac941c method=io.netty.buffer.PoolArena.freeChunk(Lio/netty/buffer/PoolChunk;JLio/netty/buffer/PoolArena$SizeClass;)V @ 96Event: 418.274 Thread 0x0000000019b3e000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x0000000002b5c524 method=io.netty.buffer.PoolArena.freeChunk(Lio/netty/buffer/PoolChunk;JLio/netty/buffer/PoolArena$SizeClass;)V @ 96Event: 418.321 Thread 0x0000000019b3e000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00000000029c4fc8 method=io.netty.buffer.PoolArena.freeChunk(Lio/netty/buffer/PoolChunk;JLio/netty/buffer/PoolArena$SizeClass;)V @ 96Event: 418.839 Thread 0x0000000019b3e000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x0000000002b20190 method=io.netty.channel.ChannelOutboundBuffer.remove0(Ljava/lang/Throwable;Z)Z @ 6Event: 418.856 Thread 0x0000000019b3e000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x0000000002c3361c method=io.netty.util.Recycler$WeakOrderQueue.add(Lio/netty/util/Recycler$DefaultHandle;)V @ 36Classes redefined (0 events):No eventsInternal exceptions (10 events):Event: 0.675 Thread 0x000000000020e800 Exception &lt;a 'java/lang/NoClassDefFoundError': org/slf4j/impl/StaticLoggerBinder&gt; (0x00000000d72ac3c0) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u171\10807\hotspot\src\share\vm\classfile\systemDictionary.cpp, line 199]Event: 0.683 Thread 0x000000000020e800 Exception &lt;a 'java/lang/NoClassDefFoundError': org/apache/log4j/Priority&gt; (0x00000000d72bb238) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u171\10807\hotspot\src\share\vm\classfile\systemDictionary.cpp, line 199]Event: 0.700 Thread 0x000000000020e800 Exception &lt;a 'java/lang/ClassCastException': sun.misc.Cleaner cannot be cast to java.lang.Runnable&gt; (0x00000000d73afc40) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u171\10807\hotspot\src\share\vm\interpreter\interpreterRuntime.cpp, lineEvent: 0.701 Thread 0x000000000020e800 Exception &lt;a 'java/lang/NoClassDefFoundError': javassist/ClassPath&gt; (0x00000000d73ba370) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u171\10807\hotspot\src\share\vm\classfile\systemDictionary.cpp, line 199]Event: 0.973 Thread 0x000000000020e800 Exception &lt;a 'java/lang/NoSuchFieldError': method resolution failed&gt; (0x00000000d7666800) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u171\10807\hotspot\src\share\vm\prims\methodHandles.cpp, line 1167]Event: 0.975 Thread 0x000000000020e800 Exception &lt;a 'java/lang/NoSuchFieldError': method resolution failed&gt; (0x00000000d7673ec0) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u171\10807\hotspot\src\share\vm\prims\methodHandles.cpp, line 1167]Event: 312.070 Thread 0x000000001a5c9000 Exception &lt;a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$identity$2(Ljava/lang/Object;)Ljava/lang/Object;&gt; (0x00000000e3466588) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u171\10807\hotspot\src\share\vm\interpreter\linkResolvEvent: 319.814 Thread 0x000000001a5c9000 Implicit null exception at 0x0000000002c4de6e to 0x0000000002c4f2e9Event: 414.327 Thread 0x0000000019b3e000 Exception &lt;a 'java/lang/OutOfMemoryError'&gt; (0x00000000816f6ef8) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u171\10807\hotspot\src\share\vm\gc_interface/collectedHeap.inline.hpp, line 159]Event: 418.839 Thread 0x0000000019b3e000 Implicit null exception at 0x0000000002b1f835 to 0x0000000002b20171Events (10 events):Event: 418.274 Thread 0x0000000019b3e000 DEOPT UNPACKING pc=0x000000000275583b sp=0x000000001a04eef0 mode 2Event: 418.321 Thread 0x0000000019b3e000 Uncommon trap: trap_request=0xffffff65 fr.pc=0x00000000029c4fc8Event: 418.321 Thread 0x0000000019b3e000 DEOPT PACKING pc=0x00000000029c4fc8 sp=0x000000001a04eee0Event: 418.321 Thread 0x0000000019b3e000 DEOPT UNPACKING pc=0x000000000275583b sp=0x000000001a04eec8 mode 2Event: 418.839 Thread 0x0000000019b3e000 Uncommon trap: trap_request=0xffffff65 fr.pc=0x0000000002b20190Event: 418.839 Thread 0x0000000019b3e000 DEOPT PACKING pc=0x0000000002b20190 sp=0x000000001a04f120Event: 418.839 Thread 0x0000000019b3e000 DEOPT UNPACKING pc=0x000000000275583b sp=0x000000001a04f080 mode 2Event: 418.856 Thread 0x0000000019b3e000 Uncommon trap: trap_request=0xffffff65 fr.pc=0x0000000002c3361cEvent: 418.856 Thread 0x0000000019b3e000 DEOPT PACKING pc=0x0000000002c3361c sp=0x000000001a04f080Event: 418.856 Thread 0x0000000019b3e000 DEOPT UNPACKING pc=0x000000000275583b sp=0x000000001a04f010 mode 2Dynamic libraries:0x000000013fff0000 - 0x0000000140027000 F:\java\jdk1.8\bin\java.exe0x0000000076ff0000 - 0x000000007719a000 C:\Windows\SYSTEM32\ntdll.dll0x0000000076ed0000 - 0x0000000076fef000 C:\Windows\system32\kernel32.dll0x000007fefcd20000 - 0x000007fefcd8a000 C:\Windows\system32\KERNELBASE.dll0x000007fefebf0000 - 0x000007fefeccb000 C:\Windows\system32\ADVAPI32.dll0x000007fefeae0000 - 0x000007fefeb7f000 C:\Windows\system32\msvcrt.dll0x000007fefeb80000 - 0x000007fefeb9f000 C:\Windows\SYSTEM32\sechost.dll0x000007fefe340000 - 0x000007fefe46d000 C:\Windows\system32\RPCRT4.dll0x0000000076dd0000 - 0x0000000076eca000 C:\Windows\system32\USER32.dll0x000007fefe0b0000 - 0x000007fefe117000 C:\Windows\system32\GDI32.dll0x000007fefee30000 - 0x000007fefee3e000 C:\Windows\system32\LPK.dll0x000007fefe1c0000 - 0x000007fefe28b000 C:\Windows\system32\USP10.dll0x000007fefb460000 - 0x000007fefb654000 C:\Windows\WinSxS\amd64_microsoft.windows.common-controls_6595b64144ccf1df_6.0.7601.18837_none_fa3b1e3d17594757\COMCTL32.dll0x000007fefe7a0000 - 0x000007fefe811000 C:\Windows\system32\SHLWAPI.dll0x000007fefee00000 - 0x000007fefee2e000 C:\Windows\system32\IMM32.DLL0x000007fefecd0000 - 0x000007fefedd9000 C:\Windows\system32\MSCTF.dll0x000000006a760000 - 0x000000006a832000 F:\java\jdk1.8\jre\bin\msvcr100.dll0x0000000052900000 - 0x00000000531a0000 F:\java\jdk1.8\jre\bin\server\jvm.dll0x000007fef9550000 - 0x000007fef9559000 C:\Windows\system32\WSOCK32.dll0x000007fefeba0000 - 0x000007fefebed000 C:\Windows\system32\WS2_32.dll0x000007fefe290000 - 0x000007fefe298000 C:\Windows\system32\NSI.dll0x000007fefad40000 - 0x000007fefad7b000 C:\Windows\system32\WINMM.dll0x000007fefbda0000 - 0x000007fefbdac000 C:\Windows\system32\VERSION.dll0x00000000771c0000 - 0x00000000771c7000 C:\Windows\system32\PSAPI.DLL0x0000000073540000 - 0x000000007354f000 F:\java\jdk1.8\jre\bin\verify.dll0x0000000070720000 - 0x0000000070749000 F:\java\jdk1.8\jre\bin\java.dll0x0000000070e00000 - 0x0000000070e16000 F:\java\jdk1.8\jre\bin\zip.dll0x000007fefd0a0000 - 0x000007fefde2a000 C:\Windows\system32\SHELL32.dll0x000007fefdeb0000 - 0x000007fefe0ac000 C:\Windows\system32\ole32.dll0x000007fefcce0000 - 0x000007fefccef000 C:\Windows\system32\profapi.dll0x0000000073550000 - 0x000000007355d000 F:\java\jdk1.8\jre\bin\management.dll0x0000000070de0000 - 0x0000000070dfa000 F:\java\jdk1.8\jre\bin\net.dll0x000007fefc450000 - 0x000007fefc4a5000 C:\Windows\system32\mswsock.dll0x000007fefc440000 - 0x000007fefc447000 C:\Windows\System32\wship6.dll0x000007fefb1e0000 - 0x000007fefb1f5000 C:\Windows\system32\NLAapi.dll0x000007fef1050000 - 0x000007fef1065000 C:\Windows\system32\napinsp.dll0x000007fef1030000 - 0x000007fef1049000 C:\Windows\system32\pnrpnsp.dll0x000007fefc2d0000 - 0x000007fefc32b000 C:\Windows\system32\DNSAPI.dll0x000007fefa5c0000 - 0x000007fefa5cb000 C:\Windows\System32\winrnr.dll0x000007fefa2a0000 - 0x000007fefa2b0000 C:\Windows\system32\wshbth.dll0x000007fefbe70000 - 0x000007fefbe77000 C:\Windows\System32\wshtcpip.dll0x000007fefa970000 - 0x000007fefa997000 C:\Windows\system32\IPHLPAPI.DLL0x000007fefa950000 - 0x000007fefa95b000 C:\Windows\system32\WINNSI.DLL0x000007fef8b50000 - 0x000007fef8b58000 C:\Windows\system32\rasadhlp.dll0x000007fef9e90000 - 0x000007fef9ee3000 C:\Windows\System32\fwpuclnt.dll0x0000000070b80000 - 0x0000000070b91000 F:\java\jdk1.8\jre\bin\nio.dll0x000007fefc4b0000 - 0x000007fefc4c8000 C:\Windows\system32\CRYPTSP.dll0x000007fefc1b0000 - 0x000007fefc1f7000 C:\Windows\system32\rsaenh.dll0x000007fefd080000 - 0x000007fefd09e000 C:\Windows\system32\USERENV.dll0x000007fefcb40000 - 0x000007fefcb4f000 C:\Windows\system32\CRYPTBASE.dll0x000007fef9cf0000 - 0x000007fef9d08000 C:\Windows\system32\dhcpcsvc.DLL0x000007fef9cd0000 - 0x000007fef9ce1000 C:\Windows\system32\dhcpcsvc6.DLL0x000007fef67e0000 - 0x000007fef6905000 C:\Windows\system32\dbghelp.dllVM Arguments:java_command: server-v1.0.jar -d C:\Users\Administrator\Desktop\test\ -s 172.21.6.57 -t taxijava_class_path (initial): server-v1.0.jarLauncher Type: SUN_STANDARDEnvironment Variables:JAVA_HOME=F:\java\jdk1.8CLASSPATH=.;F:\java\jdk1.8\lib\dt.jar;F:\java\jdk1.8\lib\tools.jar;PATH=F:\apache-maven-3.3.9\bin;F:\java\jdk1.8\bin;F:\java\jdk1.8\jre\bin;C:\Program Files\MySQL\MySQL Server 5.7\bin;C:\Program Files (x86)\Microsoft SQL Server\90\Tools\binn\;C:\Windows\system32;C:\Windows;C:\Windows\system32\wbem; F:\java\jdk1.8\bin;F:\java\jdk1.8\jre\bin;C:\Program Files (x86)\Windows Kits\8.1\Windows Performance Toolkit\;C:\Program Files\Microsoft SQL Server\110\Tools\Binn\;E:\Software\Node.js\;E:\Go\bin;E:\Software\CMDer\bin;D:\software\python3\Scripts\;D:\software\python3\;D:\software\codeblocks\MinGW\bin;C:\Users\Administrator\AppData\Roaming\npmUSERNAME=AdministratorOS=Windows_NTPROCESSOR_IDENTIFIER=Intel64 Family 6 Model 60 Stepping 3, GenuineIntel--------------- S Y S T E M ---------------OS: Windows 7 , 64 bit Build 7601 (6.1.7601.23889)CPU:total 4 (initial active 4) (2 cores per cpu, 2 threads per core) family 6 model 60 stepping 3, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, lzcnt, ht, tsc, tscinvbit, bmi1, bmi2Memory: 4k page, physical 8302960k(1130368k free), swap 16604060k(7792288k free)vm_info: Java HotSpot(TM) 64-Bit Server VM (25.171-b11) for windows-amd64 JRE (1.8.0_171-b11), built on Mar 28 2018 16:06:12 by "java_re" with MS VC++ 10.0 (VS2010)time: Fri Apr 05 19:10:48 2019elapsed time: 419 seconds (0d 0h 6m 59s) 原因：典型的内存错误 。发生了FullGC，老年代不够用了。那么应该如果调整JVM参数，不让他报错呢？光增大老年代大小就可以了吗？万一内存就这么大怎么办？ 按说不应该用这么多内存啊，是不是发生了内存泄漏？怎么查看这个代码存不存在内存泄漏的情况? 我们都知道发生内存泄漏的情况会很多，比如 （1）静态集合类，例如HashMap和Vector。如果这些容器为静态的，由于它们的声明周期与程序一致，那么容器中的对象在程序结束之前将不能被释放，从而造成内存泄漏。 （2）各种连接，例如数据库的连接、网络连接以及IO连接等。 （3）监听器。在Java语言中，往往会使用到监听器。通常一个应用中会用到多个监听器，但在释放对象的同时往往没有相应的删除监听器，这也可能导致内存泄漏。 （4）变量不合理的作用域。一般而言，如果一个变量定义的作用域大于其使用范围，很有可能会造成内存泄漏，另一方面如果没有及时地把对象设置为Null，很有可能会导致内存泄漏的放生，如下：1234567891011class Server&#123;private String msg;public void receiveMsg()&#123;readFromNet();//从网络接收数据保存在msg中saveDB()//把msg保存到数据库中｝ 从上面的代码中，通过readFromNet()方法接收的消息保存在变量msg中，然后调用saveDB()方法把msg的内容保存到数据库中，此时msg已经没用了，但是由于msg的声明周期与对象的声明周期相同，此时msg还不能被回收，因此造成了内存泄漏。对于这个问题，有如下两种解决方案：第一种方法，由于msg的作用范围只在receiveMsg()方法内，因此可以把msg定义为这个方法的局部变量，当方法结束后，msg的声明周期就会结束，此时垃圾回收器就会可以回收msg的内容了；第二种方法，在使用完msg后就设置为null，这样垃圾回收器也会自动回收msg内容所占用的内存空间。 （5）单例模式可能会造成内存泄漏 利用工具进行检查利用jconsole发现（直接在cmd里输入jconsole就能出来），老年代确实用满了，大概是1.6G的样子，名称 = ‘PS MarkSweep’, 收集 = 73, 总花费时间 = 5 分钟，可以发现光老年代的GC时间达到了5分钟，这不行啊。总共运行7分钟程序，光GC时间就5分钟。然后我调大了老年代大小，设置成了堆大小是4G，新生代是400m。但是运行了一段时间后，内存占用又上升到了3.2G左右，这个这么占内存吗？ 利用jvisualvm.exe 可以查看JVM参数，但是不知道为啥这个程序看不见。。。显示出来一个 - 就。通过java命令行加入 JVM参数后，就显示出来了。 通过这个工具发现了有一些对象，年代数特别大，58.怎么回事呢？还有一个疑问，当类加载数直线上升时，代表了啥？（类加载数从1000到5000） 会不会和ByteBuffer有关因为在代码中用到了ByteBuffer，所以怀疑是不是和这个有关。代码中有一段是这样的 12345randomAccessFile = new RandomAccessFile(f, "r");FileChannel channel = randomAccessFile.getChannel();ByteBuffer buffer = ByteBuffer.allocate(1048576);int bytesRead = channel.read(buffer);ByteBuffer stringBuffer = ByteBuffer.allocate(1024); 我们都知道写NIO程序经常使用ByteBuffer来读取或者写入数据，那么使用ByteBuffer.allocate(capability)还是使用ByteBuffer.allocteDirect(capability)来分配缓存了？第一种方式是分配JVM堆内存，属于GC管辖范围，由于需要拷贝所以速度相对较慢；第二种方式是分配OS本地内存，不属于GC管辖范围，由于不需要内存拷贝所以速度相对较快，但是内存分配起来比较慢，所以并非不论什么时候allocateDirect的操作效率都是最高的。但是当操作数据量非常小时，两种分配方式操作使用时间基本是同样的（大小是1024000之前都差不多），第一种方式有时可能会更快，可是当数据量非常大时，用ByteBuffer.allocteDirect(capability)这个就比较快了。 无意收获今天读了一篇博客，http://www.ityouknow.com/arch/2017/02/12/a-script-caused-bloody-case.html.其中，有这个一段，“重新启动观察之后mimor gc的频率确实有所下降，测试大约过了3小时候之后又反馈tomcat down掉了，继续分析启动参数配置的时候发现了这么一句-XX:-+DisableExplicitGC,显示的禁止了System.gc(),但是使用了java.nio的大量框架中使用System.gc()来执行gc期通过full gc来强迫已经无用的DirectByteBuffer对象释放掉它们关联的native memory,如果禁用会导致OOM,随即怀疑是否是这个参数引发的问题，在启动参数中去掉它。”大意是我没有禁用，所以不能GC，所以OOM。明白了。。。。。如果我们的应用中使用了java nio中的direct memory，那么使用-XX:+DisableExplicitGC一定要小心，存在潜在的内存泄露风险。 但是这个程序的内存跑到后期的时候，占用的内存还是很大，老年代GC时间很长，虽然没有OOM了，但是这样也不行啊。。。 ByteBuffer操作介绍https://www.cnblogs.com/jiduoduo/p/6397454.html 小插曲因为这个程序占用了CPU 几乎100%，造成了虚拟机抱了这些123456789101112131415161718192021[root@rabbitmq _posts]# Message from syslogd@rabbitmq at May 25 20:39:18 ... kernel:NMI watchdog: BUG: soft lockup - CPU#0 stuck for 33s! [rtkit-daemon:736]Message from syslogd@rabbitmq at May 25 20:39:39 ... kernel:NMI watchdog: BUG: soft lockup - CPU#0 stuck for 35s! [scsi_eh_1:275]Message from syslogd@rabbitmq at May 25 20:39:39 ... kernel:NMI watchdog: BUG: soft lockup - CPU#0 stuck for 21s! [pickup:4845]Message from syslogd@rabbitmq at May 25 20:39:39 ... kernel:NMI watchdog: BUG: soft lockup - CPU#0 stuck for 22s! [dhclient:871]Message from syslogd@rabbitmq at May 25 20:39:39 ... kernel:NMI watchdog: BUG: soft lockup - CPU#0 stuck for 22s! [master:2807]Message from syslogd@rabbitmq at May 25 20:39:39 ... kernel:NMI watchdog: BUG: soft lockup - CPU#0 stuck for 29s! [abrt-hook-ccpp:5402]Message from syslogd@rabbitmq at May 25 20:39:58 ... kernel:NMI watchdog: BUG: soft lockup - CPU#0 stuck for 21s! [abrt-hook-ccpp:5402] 原因是占用过多CPU，造成内核软死锁（soft lockup）。Soft lockup名称解释：所谓，soft lockup就是说，这个bug没有让系统彻底死机，但是若干个进程（或者kernel thread）被锁死在了某个状态（一般在内核区域），很多情况下这个是由于内核锁的使用的问题。 知识点 Code Cache (non-heap):HotSpot Java虚拟机包括一个用于编译和保存本地代码（native code）的内存，叫做“代码缓存区”（code cache） （这里大概是6、7M的大小） 参考https://blog.csdn.net/jsqfengbao/article/details/44787267https://www.cnblogs.com/mengfanrong/p/3984841.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix之疑]]></title>
    <url>%2F2019%2F05%2F25%2Fzabbix%E4%B9%8B%E7%96%91%2F</url>
    <content type="text"><![CDATA[我们的云平台CC节点上zabbix_server进程非常多啊，多达129个，我咋觉得有点不正常呢。12[root@cc /]# ps -ef | grep zabbix_server | wc -l129 这么多进程需要占用多少内存啊得，1for pid in `ps -C zabbix_server -o pid --no-heading`; do grep Pss /proc/$pid/smaps;done|awk '&#123;sum+=$2&#125;;END&#123;print sum/1024/1024"G"&#125;' 一看，还行啊，才用了0.103609G。也就是103m。这么多进程才用了这么点内存，emmmm。。。。。 统计内存要用Pss，不要用Rss用ps命令统计所有zabbix_server进程的rss值进行累加得出占用总内存，此种方法不对，会导致结果偏大。很多人通过累加 “ps aux” 命令显示的 RSS 列来统计全部进程总共占用的物理内存大小，这是不对的。RSS(resident set size)表示常驻内存的大小，但是由于不同的进程之间会共享内存，所以把所有进程RSS进行累加的方法会重复计算共享内存，得到的结果是偏大的。 正确的方法是累加 /proc/[1-9]*/smaps 中的 Pss 。/proc//smaps 包含了进程的每一个内存映射的统计值，详见proc(5)的手册页。Pss(Proportional Set Size)把共享内存的Rss进行了平均分摊，比如某一块100MB的内存被10个进程共享，那么每个进程就摊到10MB。这样，累加Pss就不会导致共享内存被重复计算了。 Zabbix-Server 的处理进程: watchdog：负责监视所有的进程的状态的，一旦某个进程关闭，watchdog将会通知zabbix重新启动该进程 housekeeper：负责清理过期的数据 aleter：报警使用 poller：拉取数据 httppoller：监控web页面的专业拉取数据工具 discoverer: 发现资源 pinger：使用Fping/Ping等工具探测主机是否在线 db_config_syncer：数据库配置同步 db_date_syncer：数据库数据同步 nodewatcher：监控各个节点 timer：计时器 escalator：报警升级 zabbix_get的使用使用zabbix_get获取cpu负载，还可以获取其他信息。。。在CC主机上，执行./zabbix_get -s 172.21.4.101 -p 10050 -k &quot;system.cpu.load[all,avg15]&quot; 参考https://www.jianshu.com/p/b67cbd507ac3]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Epoll惊群问题]]></title>
    <url>%2F2019%2F05%2F25%2FEpoll%E6%83%8A%E7%BE%A4%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[strace命令strace是个功能强大的Linux调试分析诊断工具，可用于跟踪程序执行时进程系统调用(system call)和所接收的信号，尤其是针对源码不可读或源码无法再编译的程序。 在Linux系统中，用户程序运行在一个沙箱(sandbox)里，用户进程不能直接访问计算机硬件设备。当进程需要访问硬件设备(如读取磁盘文件或接收网络数据等)时，必须由用户态模式切换至内核态模式，通过系统调用访问硬件设备。strace可跟踪进程产生的系统调用，包括参数、返回值和执行所消耗的时间。若strace没有任何输出，并不代表此时进程发生阻塞；也可能程序进程正在自己的沙箱里执行某些不需要与系统其它部分发生通信的事情。strace从内核接收信息，且无需以任何特殊方式来构建内核。 strace命令格式如下： strace [-dffhiqrtttTvVxx] [-a column] [-e expr] [-o file] [-p pid] [-s strsize] [-u username] [-E var=val] [command [arg ...]] 或 strace -c [-e expr] [-O overhead] [-S sortby] [-E var=val] [command [arg ...]] 通过不同的选项开关，strace提供非常丰富的跟踪功能。最简单的应用是，跟踪可执行程序运行时的整个生命周期，将所调用的系统调用的名称、参数和返回值输出到标准错误输出stderr(即屏幕)或-o选项所指定的文件。注意，命令(command)必须位于选项列表之后。 啥也没有如果我们用 strace 跟踪一个进程，输出结果很少，是不是说明进程很空闲？其实试试 ltrace，可能会发现别有洞天。记住有内核态和用户态之分。 我第一开始执行strace -fF -o b.txt -e trace=accpet ./spider啥也没有，以为是上边的原因，结果其实是因为你的代码里没有accept相关的调用，所以肯定没有了啊，那我不写的话就相当于记录所有的东西。 命令说明https://www.cnblogs.com/clover-toeic/p/3738156.html 惊群问题https://blog.csdn.net/dog250/article/details/80837278 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/epoll.h&gt;#include &lt;netdb.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/wait.h&gt;#include &lt;time.h&gt;#include &lt;signal.h&gt;#define COUNT 1int mode = 0;int slp = 0;int pid[COUNT] = &#123;0&#125;;int count = 0;void server(int epfd) &#123; struct epoll_event *events; int num, i; struct timespec ts; events = calloc(64, sizeof(struct epoll_event)); while (1) &#123; int sd, csd; struct sockaddr in_addr; num = epoll_wait(epfd, events, 64, -1); if (num &lt;= 0) &#123; continue; &#125; /* ts.tv_sec = 0; ts.tv_nsec = 1; if(nanosleep(&amp;ts, NULL) != 0) &#123; perror(&quot;nanosleep&quot;); exit(1); &#125; */ // 用于测试ET模式下丢事件的情况 if (slp) &#123; sleep(slp); &#125; sd = events[0].data.fd; socklen_t in_len = sizeof(in_addr); csd = accept(sd, &amp;in_addr, &amp;in_len); if (csd == -1) &#123; // 打印这个说明中了epoll LT惊群的招了。 printf(&quot;shit xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:%d\n&quot;, getpid()); continue; &#125; // 本进程一共成功处理了多少个请求。 count ++; printf(&quot;get client:%d\n&quot;, getpid()); close(csd); &#125;&#125;static void siguser_handler(int sig)&#123; // 在主进程被Ctrl-C退出的时候，每一个子进程均要打印自己处理了多少个请求。 printf(&quot;pid:%d count:%d\n&quot;, getpid(), count); exit(0);&#125;static void sigint_handler(int sig)&#123; int i = 0; // 给每一个子进程发信号，要求其打印自己处理了多少个请求。 for (i = 0; i &lt; COUNT; i++) &#123; kill(pid[i], SIGUSR1); &#125;&#125;int main (int argc, char *argv[])&#123; int ret = 0; int listener; int c = 0; struct sockaddr_in saddr; int port; int status; int flags; int epfd; struct epoll_event event; if (argc &lt; 4) &#123; exit(1); &#125; // 0为LT模式，1为ET模式 mode = atoi(argv[1]); port = atoi(argv[2]); // 是否在处理accept之前耽搁一会儿，这个参数更容易重现问题 slp = atoi(argv[3]); signal(SIGINT, sigint_handler); listener = socket(PF_INET, SOCK_STREAM, 0); saddr.sin_family = AF_INET; saddr.sin_port = htons(port); saddr.sin_addr.s_addr = INADDR_ANY; bind(listener, (struct sockaddr*)&amp;saddr, sizeof(saddr)); listen(listener, SOMAXCONN); flags = fcntl (listener, F_GETFL, 0); flags |= O_NONBLOCK; fcntl (listener, F_SETFL, flags); epfd = epoll_create(64); if (epfd == -1) &#123; perror(&quot;epoll_create&quot;); abort(); &#125; event.data.fd = listener; event.events = EPOLLIN; if (mode == 1) &#123; event.events |= EPOLLET; &#125; else if (mode == 2) &#123; event.events |= EPOLLONESHOT; &#125; ret = epoll_ctl(epfd, EPOLL_CTL_ADD, listener, &amp;event); if (ret == -1) &#123; perror(&quot;epoll_ctl&quot;); abort(); &#125; for(c = 0; c &lt; COUNT; c++) &#123; int child; child = fork(); if(child == 0) &#123; // 安装打印count值的信号处理函数 signal(SIGUSR1, siguser_handler); server(epfd); &#125; pid[c] = child; printf(&quot;server:%d pid:%d\n&quot;, c+1, child); &#125; wait(&amp;status); sleep(1000000); close (listener);&#125; 参考https://www.cnblogs.com/clover-toeic/p/3738156.htmlhttps://huoding.com/2015/10/16/474]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你知道Raft吗？]]></title>
    <url>%2F2019%2F05%2F25%2F%E4%BD%A0%E7%9F%A5%E9%81%93Raft%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[什么是 paxos 算法以及raft算法paxos：多个proposer发请提议（每个提议有id+value），acceptor接受最新id的提议并把之前保留的提议返回。当超过半数的accetor返回某个提议时，此时要求value修改为propeser历史上最大值，propeser认为可以接受该提议，于是广播给每个acceptor，acceptor发现该提议和自己保存的一致，于是接受该提议并且learner同步该提议。 raft：raft要求每个节点有一个选主的时间间隔，每过一个时间间隔向master发送心跳包，当心跳失败，该节点重新发起选主，当过半节点响应时则该节点当选主机，广播状态，然后以后继续下一轮选主 raft原理的动画演示：http://thesecretlivesofdata.com/raft/ 寻找一种易于理解的一致性算法（扩展版）https://learnblockchain.cn/2019/03/22/easy_raft/#more Java 版本的 Raft(CP) KV 分布式存储https://github.com/stateIs0/lu-raft-kv]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件句柄和文件描述符傻傻分不清楚]]></title>
    <url>%2F2019%2F03%2F29%2F%E6%96%87%E4%BB%B6%E5%8F%A5%E6%9F%84%E5%92%8C%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E5%82%BB%E5%82%BB%E5%88%86%E4%B8%8D%E6%B8%85%E6%A5%9A%2F</url>
    <content type="text"><![CDATA[我的ubuntu16服务器上文件句柄使用量(第一个字段)为10688（文件句柄） 12root@ubuntu16-desktop:/proc# cat /proc/sys/fs/file-nr 10688 0 1597389 但是通过运维常用的lsof命令算了下是283753(文件描述符)，相差甚远。lsof -P -n | wc -l 然后我在云平台上也试了试,结果都是一样的。1234[root@cc ~]# cat /proc/sys/fs/file-nr5664 0 1300000[root@cc ~]# lsof -P -n | wc -l180019 为啥呢？？老师说，可能是使用者不一样，有的是操作系统用的，有的是进程调度用的，有的是特殊命令用的。。。但本质的作用是一样的，都是区分当前唯一的对象文件啥的。 什么地方会分配文件句柄 open系统调用打开文件（path_openat内核函数) 打开一个目录（dentry_open函数) 共享内存attach （do_shmat函数） socket套接字（sock_alloc_file函数） 管道（create_pipe_files函数） epoll/inotify/signalfd等功能用到的匿名inode文件系统（anon_inode_getfile函数) 是不是可能部分共享内存段被attach了好多好多次通过ipcs -m查看一下共享内存的情况,并没有什么问题啊。 123456789101112131415161718root@ubuntu16-desktop:/proc# ipcs -m------ Shared Memory Segments --------key shmid owner perms bytes nattch status 0x00000000 294912 ubuntu16 600 524288 2 dest 0x00000000 589825 ubuntu16 600 16777216 2 0x00000000 557058 ubuntu16 600 524288 2 dest 0x00000000 1343491 ubuntu16 600 524288 2 dest 0x00000000 753668 ubuntu16 600 524288 2 dest 0x00000000 917509 ubuntu16 600 524288 2 dest 0x00000000 1146886 ubuntu16 600 524288 2 dest 0x00000000 1048583 ubuntu16 600 67108864 2 dest 0x00000000 1245192 ubuntu16 600 524288 2 dest 0x00000000 1441801 ubuntu16 600 524288 2 dest 0x00000000 1540106 ubuntu16 600 524288 2 dest 0x00000000 1638411 ubuntu16 600 524288 2 dest 0x00000000 1671180 ubuntu16 600 4194304 2 dest 0x00000000 1802254 ubuntu16 600 524288 2 dest 进一步了解文件描述符和文件句柄 一个描述符上可以监控很多的事件，如果监控的是read，然而write事件到来的时候也会将该描述符唤醒 lsof在用户空间，主要还是从文件描述符的角度来看文件句柄。 简单来说，每个进程都有一个打开的文件表（fdtable)。表中的每一项是struct file类型，包含了打开文件的一些属性比如偏移量，读写访问模式等，这是真正意义上的文件句柄。 文件描述符是一个整数。代表fdtable中的索引位置（下标），指向具体的struct file（文件句柄）。 每一个文件描述符会与一个打开文件相对应，同时，不同的文件描述符也会指向同一个文件。相同的文件可以被不同的进程打开也可以在同一个进程中被多次打开。系统为每一个进程维护了一个文件描述符表，该表的值都是从0开始的，所以在不同的进程中你会看到相同的文件描述符，这种情况下相同文件描述符有可能指向同一个文件，也有可能指向不同的文件。具体情况要具体分析，要理解具体其概况如何，需要查看由内核维护的3个数据结构。 进程级的文件描述符表 系统级的打开文件描述符表 文件系统的i-node表 进程级的描述符表的每一条目记录了单个文件描述符的相关信息。 控制文件描述符操作的一组标志。（目前，此类标志仅定义了一个，即close-on-exec标志） 对打开文件句柄的引用 内核对所有打开的文件的文件维护有一个系统级的描述符表格（open file description table）。有时，也称之为打开文件表（open file table），并将表格中各条目称为打开文件句柄（open file handle）。一个打开文件句柄存储了与一个打开文件相关的全部信息，如下所示： 当前文件偏移量（调用read()和write()时更新，或使用lseek()直接修改） 打开文件时所使用的状态标识（即，open()的flags参数） 文件访问模式（如调用open()时所设置的只读模式、只写模式或读写模式） 与信号驱动相关的设置 对该文件i-node对象的引用 文件类型（例如：常规文件、套接字或FIFO）和访问权限 一个指针，指向该文件所持有的锁列表 文件的各种属性，包括文件大小以及与不同类型操作相关的时间戳 下图展示了文件描述符、打开的文件句柄以及i-node之间的关系，图中，两个进程拥有诸多打开的文件描述符。 在进程A中，文件描述符1和30都指向了同一个打开的文件句柄（标号23）。这可能是通过调用dup()、dup2()、fcntl()或者对同一个文件多次调用了open()函数而形成的。 进程A的文件描述符2和进程B的文件描述符2都指向了同一个打开的文件句柄（标号73）。这种情形可能是在调用fork()后出现的（即，进程A、B是父子进程关系），或者当某进程通过UNIX域套接字将一个打开的文件描述符传递给另一个进程时，也会发生。再者是不同的进程独自去调用open函数打开了同一个文件，此时进程内部的描述符正好分配到与其他进程打开该文件的描述符一样。 此外，进程A的描述符0和进程B的描述符3分别指向不同的打开文件句柄，但这些句柄均指向i-node表的相同条目（1976），换言之，指向同一个文件。发生这种情况是因为每个进程各自对同一个文件发起了open()调用。同一个进程两次打开同一个文件，也会发生类似情况。 有关文件描述符的一些命令ulimit查看进程允许打开的最大文件句柄数：ulimit -n设置进程能打开的最大文件句柄数：ulimit -n xxx ulimit在系统允许的情况下，提供对特定shell可利用的资源的控制。（Provides control over the resources avaliable to the shell and to processes started by it, on systems that allow such control）-H和-S选项设定指定资源的硬限制和软限制。硬限制设定之后不能再添加，而软限制则可以增加到硬限制规定的值。如果-H和-S选项都没有指定，则软限制和硬限制同时设定。限制值可以是指定资源的数值或者hard, soft, unlimited这些特殊值，其中hard代表当前硬限制, soft代表当前软件限制, unlimited代表不限制. 如果不指定限制值, 则打印指定资源的软限制值, 除非指定了-H选项.如果指定了不只一种资源, 则限制名和单位都会在限制值前显示出来. 1234root@ubuntu16-desktop:/proc# ulimit -Sn1024root@ubuntu16-desktop:/proc# ulimit -Hn1048576 limits.conflimits.conf这个文件实在/etc/security/目录下，因此这个文件是处于安全考虑的。limits.conf文件是用于提供对系统中的用户所使用的资源进行控制和限制，对所有用户的资源设定限制是非常重要的，这可以防止用户发起针对处理器和内存数量等的拒绝服务攻击。这些限制必须在用户登录时限制。 limits.conf与ulimit的区别在于前者是针对所有用户的，而且在任何shell都是生效的，即与shell无关，而后者只是针对特定用户的当前shell的设定。在修改最大文件打开数时，最好使用limits.conf文件来修改，通过这个文件，可以定义用户，资源类型，软硬限制等。也可修改/etc/profile文件加上ulimit的设置语句来是的全局生效。 当达到上限时，会报错：too many open files或者遇上Socket/File: Cannot open so many files等。 file-max &amp; file-nr1234root@ubuntu16-desktop:/proc# cat /proc/sys/fs/file-max1597389root@ubuntu16-desktop:/proc# cat /proc/sys/fs/file-nr 10720 0 1597389 该文件指定了可以分配的文件句柄的最大数目（系统全局的可用句柄数目. The value in file-max denotes the maximum number of file handles that the Linux kernel will allocate）。如果用户得到的错误消息审批由于打开文件数已经达到了最大值，从而他们不能打开更多文件，则可能需要增加改之。可将这个值设置成任意多个文件，并且能通过将一个新数字值写入该文件来更改该值。这个参数的默认值和内存大小有关系，可以使用公式：file-max ≈ 内存大小/ 10k. lsoflsof是列出系统所占用的资源（list open files），但是这些资源不一定会占用句柄。比如共享内存、信号量、消息队列、内存映射等，虽然占用了这些资源，但不占用句柄。 常用命令确认系统设置的最大文件句柄数ulimit -a 统计系统中当前打开的总文件句柄数lsof -w | awk &#39;{print $2}&#39; | wc -l-w参数为忽略错误，因为不加这个-w，会显示出很多lsof: no pwd entry for UID 472 根据打开文件的数量降序排列，其中第二列为进程ID： lsof -w | awk &#39;{print $2}&#39; | sort | uniq -c | sort -nr | more 查看当前进程打开了多少文件lsof -w -n|awk &#39;{print $2}&#39;|sort|uniq -c|sort -nr|more | grep 22312 总结 由于进程级文件描述符表的存在，不同的进程中会出现相同的文件描述符，它们可能指向同一个文件，也可能指向不同的文件 两个不同的文件描述符，若指向同一个打开文件句柄，将共享同一文件偏移量。因此，如果通过其中一个文件描述符来修改文件偏移量（由调用read()、write()或lseek()所致），那么从另一个描述符中也会观察到变化，无论这两个文件描述符是否属于不同进程，还是同一个进程，情况都是如此。 要获取和修改打开的文件标志（例如：O_APPEND、O_NONBLOCK和O_ASYNC），可执行fcntl()的F_GETFL和F_SETFL操作，其对作用域的约束与上一条颇为类似。 文件描述符标志（即，close-on-exec）为进程和文件描述符所私有。对这一标志的修改将不会影响同一进程或不同进程中的其他文件描述符 从句柄的概念再看分层设计几个层次问题，首先物理内存和虚拟内存，操作系统管理物理内存，而用户进程使用虚拟内存，操作系统呈现给用户进程的是连续的虚拟内存但是不一定连续的物理内存，物理内存随时在变化，但是对于用户进程来说其虚拟内存地址是不变的；其次是指针和句柄，操作系统为了向用户空间提供若干台虚拟机并且又要管理一些所有进程需要的系统服务，必然不能将内核数据结构呈现给进程，但是进程确实可以使用这种数据，因此句柄就出现了，句柄其实是指针的指针，虽然它不是真正意义上的指针，由于操作系统管理的数据结构易变，并且处于高特权级，所以操作系统不能将之开放给用户空间，用户空间只能通过句柄来调用系统服务，进入内核之后，内核会根据句柄找到真正的内核数据结构，很多实现中句柄就是内核中进程块中一个表的索引字段，因为虚拟机是基于进程的，比如linux的task_struct中的打开文件表，在windows中的做法更是深刻，一切都是对象，因此每个对象都有句柄，进程，线程本身，文件，设备，窗口，…都有句柄，其进程块中有一个句柄表，每一个元素代表一个对象。正是因为windows的一切皆对象特性，其可以操作远程进程，因为进程是一个对象，而对象有句柄，有了句柄就可以操作，win32API中很多接口都是基于句柄的对象操作接口，在linux中似乎只有文件体现为句柄，也只有文件是进程可以共享的，由于没有别的句柄，因此也就不能随意操作别的对象。 句柄隐藏了内核的管理细节，呈现给进程一个平滑的结构，实际上句柄指向的真正的结构的地址是可以变化的，正如一个指针不变的情况下，其指向的数据可以变化，句柄的作用有二：第一，隐藏不必要的操作，作为唯一接口由API只开放可用的安全操作；第二，类似物理内存和虚拟内存的关系，向用户空间提供一个稳定的结构操作把手，将抽象从操作系统提升到进程。在操作系统中，抽象是基于文件，设备的，但是到了进程就统一都成了句柄，体现了在句柄指向的数据结构上的多路复用，按照分层原则，上层可以在下层提供的一个地址格式上多路复用，按照抽象的原则，越往上抽象程度越高，正如TCP和UDP可以复用一个IP地址一样，多个进程的多个句柄可以指向同一个内核数据结构。 句柄是一个层次间通信的通道，每一个进程虚拟机都通过句柄来使用共享的操作系统内核服务，但是这只是一类服务，进程虚拟机并不是靠句柄来使用操作系统的一切服务的，还有一类服务和操作系统一样可以直接使用更底层的硬件机制，比如MMU。前面的一篇文章说过，操作系统给进程虚拟机抽象两类指令，一类是安全的指令，这类指令直接并且始终在cpu-内存之间运行，运行这类指令不需要经过操作系统层，还有一类指令不是仅仅在cpu运行，而是牵扯到了没有提供多道程序环境的IO外设，对于这些资源的访问，进程必须使用操作系统的服务而不能直接使用硬件机制了。对于MMU而言，其实也是一种和句柄类似的抽象，多cpu上同时运行的不同进程可以同时访问相同的虚拟地址(非共享内存)，但是通过MMU转换后的物理地址却不同，这就是多路复用概念的引申。本质上讲，MMU和操作系统内核是属于同一层次的，只不过MMU是硬件固有的机制，而操作系统内核提供一些管理策略，对于句柄而言，不同进程可以使用相同的句柄，但是指向的实际内核数据结构却不一定相同，因为句柄和虚拟内存一样，只在进程内部有意义，句柄是进程中内核数据结构的索引，是为了用户进程访问内核数据结构而设计的，这种设计非常好，规定了用户访问内核数据结构只能在规则下进行，而规则就是以句柄而参数的API的实现，其实就是系统调用的实现。 开放的句柄越多，对操作系统设计的安全要求就越高，只要有句柄，在得到句柄表的情况下就很容易得到实际的数据，在windows上，系统漏洞很多由此而发，而linux上仅仅开放了打开文件这个句柄，类似的漏洞也就很少了。为什么？句柄其实就是指针的指针，只不过这里的指针的含义更加抽象了，不是一般意义上的指针 参考https://blog.csdn.net/dog250/article/details/5302879http://blog.csdn.net/cywosp/article/details/38965239]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>OS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统之中断]]></title>
    <url>%2F2019%2F03%2F29%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E4%B8%AD%E6%96%AD%2F</url>
    <content type="text"><![CDATA[什么是中断中断是一种当今很重要的硬件与cpu通信的方式，主板上集成很多硬件，那么就可以认为会有很多中断，但是cpu的数目往往要少得多，那么肯定会有多个硬件中断竞争一个cpu的情况，任何系统（包括自然界）都不能乱套，肯定会有一定的机制防止事情变得不可控制，这种机制的结果就是使一切变得有序化，出现多竞争一的情况时，最常用的方法就是排队，而排队有很多策略，比如先来先服务，优先级队列，加权优先级队列，多级队列等等。这是硬件方面的情况，那么cpu方 面呢，cpu的中断引脚往往只有一个，这就是说，排队必须在中断到达cpu前进行，所以中断控制器的作用就体现出来了，有了中断控制器，我们就可以把控制 器后面的硬件当成黑盒子了，只要管控制器里面会接受很多中断并且仲裁它们就可以了。 中断优先级和中断线程优先级https://blog.csdn.net/dog250/article/details/5302745 进程上下文 &amp; 中断上下文 进程上下文一般在进程切换中提到，进程控制块PCB,保存着进程的诸多详细信息，当进程要切换时当前进程的寄存器内容以及内存页表的详细信息等等内容，也就是关于描述进程的信息。 当一个进程在执行时,CPU的所有寄存器中的值、进程的状态以及堆栈中的内容被称为该进程的上下文。当内核需要切换到另一个进程时，它需要保存当前进程的所有状态，即保存当前进程的上下文，以便在再次执行该进程时，能够得到切换时的状态执行下去。在LINUX中，当前进程上下文均保存在进程的任务数据结构中。在发生中断时,内核就在被中断进程的上下文中，在内核态下执行中断服务例程。但同时会保留所有需要用到的资源，以便中继服务结束时能恢复被中断进程的执行。 当一个进程运行时 ，产生了一个中断，CPU转而执行中断处理程序 ，虽然CPU当时保存了被中断进程的上下文， 但这和中断处理丝毫没有关系 ， 也就是说，中断处理程序没有进程上下文，但是你却可以得到current的值！ 进程上下文就是表示进程信息的一系列东西，包括各种变量、寄存器以及进程的运行的环境。这样，当进程被切换后，下次再切换回来继续执行，能够知道原来的状态。同理，中断上下文就是中断发生时，原来的进程执行被打断，那么就要把原来的那些变量保存下来，以便中断完成后再恢复。 处理器总处于以下状态中的一种： 内核态，运行于进程上下文，内核代表进程运行于内核空间； 内核态，运行于中断上下文，内核代表硬件运行于内核空间； 用户态，运行于用户空间。 进程上下文：在Linux中，用户程序装入系统形成一个进程的实质是系统为用户程序提供一个完整的运行环境。进程的运行环境是由它的程序代码和程序运行所需要的数据结构以及硬件环境组成的。进程的运行环境主要包括：1.进程空间中的代码和数据、各种数据结构、进程堆栈和共享内存区等。2.环境变量：提供进程运行所需的环境信息。3.系统数据：进程空间中的对进程进行管理和控制所需的信息，包括进程任务结构体以及内核堆栈等。4.进程访问设备或者文件时的权限。5.各种硬件寄存器。6.地址转换信息。 从以上组成情况可以看到，进程的运行环境是动态变化的，尤其是硬件寄存器的值以及进程控制信息是随着进程的运行而不断变化的。在Linux中把系统提供给进程的的处于动态变化的运行环境总和称为进程上下文。 系统中的每一个进程都有自己的上下文。一个正在使用处理器运行的进程称为当前进程(current)。当前进程因时间片用完或者因等待某个事件而阻塞时，进程调度需要把处理器的使用权从当前进程交给另一个进程，这个过程叫做进程切换。此时，被调用进程成为当前进程。在进程切换时系统要把当前进程的上下文保存在指定的内存区域（该进程的任务状态段TSS中），然后把下一个使用处理器运行的进程的上下文设置成当前进程的上下文。当一个进程经过调度再次使用CPU运行时，系统要恢复该进程保存的上下文。所以，进程的切换也就是上下文切换。 在系统内核为用户进程服务时，通常是进程通过系统调用执行内核代码，这时进程的执行状态由用户态转换为内核态。但是，此时内核的运行是为用户进程服务，也可以说内核在代替当前进程执行某种服务功能。在这种情况下，内核的运行仍是进程运行的一部分，所以说这时内核是运行在进程上下文中。内核运行在进程上下文中时可以访问和修改进程的系统数据。此外，若内核运行在进程上下文中需要等待资源和设备时，系统可以阻塞当前进程 中断上下文：硬件通过触发信号，导致内核调用中断处理程序，进入内核空间。这个过程中，硬件的一些变量和参数也要传递给内核，内核通过这些参数进行中断处理。所谓的“中断上下文”，其实也可以看作就是硬件传递过来的这些参数和内核需要保存的一些其他环境（主要是当前被打断执行的进程环境）。中断时，内核不代表任何进程运行，它一般只访问系统空间，而不会访问进程空间，内核在中断上下文中执行时一般不会阻塞。 参考https://blog.csdn.net/dog250/article/details/5302745]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>OS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个诡异的shell脚本问题]]></title>
    <url>%2F2019%2F03%2F29%2F%E4%B8%80%E4%B8%AA%E8%AF%A1%E5%BC%82%E7%9A%84shell%E8%84%9A%E6%9C%AC%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[起因在论坛上看见有人问了一个问题，https://javaweb.io/post/394/reply/1246,其实这个shell脚本很简单，就是根据进程名字得到进程号，但是很诡异的出来了异常结果。 我也自己尝试了一下，直接执行命令确实是返回一个pid，但是在shell脚本里，却是3个pid号。这是为什么呢？ 峰回路转看见了这个https://akaedu.github.io/book/ch31s02.html大概知道了什么原因： 用户在命令行输入命令后，一般情况下Shell会fork并exec该命令，但是Shell的内建命令例外，执行内建命令相当于调用Shell进程中的一个函数，并不创建新的进程。以前学过的cd、alias、umask、exit等命令即是内建命令，凡是用which命令查不到程序文件所在位置的命令都是内建命令，内建命令没有单独的man手册，要在man手册中查看内建命令，应该用man bash-builtins 一句话总结就是：多出来的那两个是shell子进程的pid号 过程原来的错误脚本是这样的1234567891011#!/bin/bashset -xprocess=$1pid=$(ps -ef | grep $process | grep -v grep | awk '&#123;print $2&#125;')echo $pidsleep 5while truedo echo aaaaaaaadone 所以说 在终端输入sh a.sh rcu_sched（我拿这个进程举例）的时候，其实起了三个进程，其中有两个是sh a.sh rcu_sched,这两个是Shell的临时进程。解决办法就是加上grep -v sh（grep的-v参数是取反，也就是说grep -v grep是过滤掉那些带grep关键字进程，） 收获虽然浪费了好长时间才弄懂，但是收获了别的东西 调试shell脚本开启调试功能 通过sh -x 脚本名 #显示脚本执行过程 脚本里set -x选项,轻松跟踪调试shell脚本 首先使用“-n”选项检查语法错误，然后使用“-x”选项跟踪脚本的执行，使用“-x”选项之前，别忘了先定制PS4变量的值来增强“-x”选项的输出信息，至少应该令其输出行号信息(先执行export PS4=’+[$LINENO]’，更一劳永逸的办法是将这条语句加到您用户主目录的.bash_profile文件中去)，这将使你的调试之旅更轻松。也可以利用trap,调试钩子等手段输出关键调试信息，快速缩小排查错误的范围，并在脚本中使用“set -x”及“set +x”对某些代码块进行重点跟踪。这样多种手段齐下，相信您已经可以比较轻松地抓出您的shell脚本中的臭虫了。如果您的脚本足够复杂，还需要更强的调试能力，可以使用shell调试器bashdb，这是一个类似于GDB的调试工具，可以完成对shell脚本的断点设置，单步执行，变量观察等许多功能，使用bashdb对阅读和理解复杂的shell脚本也会大有裨益。关于bashdb的安装和使用，不属于本文范围，您可参阅http://bashdb.sourceforge.net/上的文档并下载试用。 一个脚本检测网站该网站能帮你检测你的脚本有啥问题https://www.shellcheck.net/ Linux Shell脚本实现根据进程名杀死进程Shell脚本源码如下：1234567891011121314#!/bin/sh#根据进程名杀死进程if [ $# -lt 1 ]then echo "缺少参数：procedure_name" exit 1fi PROCESS=`ps -ef|grep $1|grep -v grep|grep -v PPID|awk '&#123; print $2&#125;'`for i in $PROCESSdo echo "Kill the $1 process [ $i ]" kill -9 $idone 交互式 Bash Shell 获取进程 pid在已知进程名(name)的前提下，交互式 Shell 获取进程 pid 有很多种方法，典型的通过 grep 获取 pid 的方法为（这里添加 -v grep是为了避免匹配到 grep 进程）： ps -ef | grep “name” | grep -v grep | awk ‘{print $2}’ 或者不使用 grep（这里名称首字母加[]的目的是为了避免匹配到 awk 自身的进程）： ps -ef | awk ‘/[n]ame/{print $2}’ 如果只使用 x 参数的话则 pid 应该位于第一位： ps x | awk ‘/[n]ame/{print $1}’ 最简单的方法是使用 pgrep： pgrep -f name 如果需要查找到 pid 之后 kill 掉该进程，还可以使用 pkill： pkill -f name 如果是可执行程序的话，可以直接使用 pidof pidof name 获取 Shell 脚本自身进程 pid123456这里涉及两个指令： 1. $$ ：当前 Shell 进程的 pid 2. $! ：上一个后台进程的 pid 可以使用这两个指令来获取相应的进程 pid。例如，如果需要获取某个正在执行的进程的 pid（并写入指定的文件）：myCommand &amp;&amp; pid=$!myCommand &amp; echo $! &gt;/path/to/pid.file注意，在脚本中执行 $! 只会显示子 Shell 的后台进程 pid，如果子 Shell 先前没有启动后台进程，则没有输出。 查看指定进程是否存在在获取到 pid 之后，还可以根据 pid 查看对应的进程是否存在（运行），这个方法也可以用于 kill 指定的进程。12345if ps -p $PID &gt; /dev/nullthen echo "$PID is running" # Do something knowing the pid exists, i.e. the process with $PID is runningfi 使用 Shell 对进程资源进行监控https://www.ibm.com/developerworks/cn/linux/l-cn-shell-monitoring/index.html 父子进程(pid相差1)https://www.cnblogs.com/jian-99/p/7719085.html 参考http://weyo.me/pages/techs/linux-get-pid/]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么Netty使用NIO而不是AIO？]]></title>
    <url>%2F2019%2F03%2F29%2F%E4%B8%BA%E4%BB%80%E4%B9%88Netty%E4%BD%BF%E7%94%A8NIO%E8%80%8C%E4%B8%8D%E6%98%AFAIO%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[先了解下BIO NIO AIOBIO：同步并阻塞，一个连接一个线程，适用于链接数量小且固定的架构。NIO：同步非阻塞：一个请求一个线程，客户端发送的链接请求都会注册到多路复用器上，多路复用器轮训到链接有 io 请求时才启动一个线程进行处理，适用于链接比较多，比较短。以块的方式处理数据。采用多路复用Reactor模式。JDK1.4时引入AIO：异步非阻塞，一个有效请求一个线程，适用于链接数目多且长。基于unix事件驱动，不需要多路复用器对注册通道进行轮询，采用Proactor设计模式。JDK1.7时引入。 为什么Netty使用NIO而不是AIO？ Netty不看重Windows上的使用，在Linux系统上，AIO的底层实现仍使用EPOLL，没有很好实现AIO，因此在性能上没有明显的优势，而且被JDK封装了一层不容易深度优化 Netty整体架构是reactor模型, 而AIO是proactor模型, 混合在一起会非常混乱,把AIO也改造成reactor模型看起来是把epoll绕个弯又绕回来 AIO还有个缺点是接收数据需要预先分配缓存, 而不是NIO那种需要接收时才需要分配缓存, 所以对连接数量非常大但流量小的情况, 内存浪费很多 Linux上AIO不够成熟，处理回调结果速度跟不到处理需求，比如外卖员太少，顾客太多，供不应求，造成处理速度有瓶颈（待验证） 一个比喻：NIO相当于餐做好了自己去取，AIO相当于送餐上门。要吃饭的人是百万，千万级的，送餐员也就几百人。所以一般要吃到饭，是自己去取快呢，还是等着送的更快？目前的外卖流程不是很完善，所以时间上没想的那么靠谱，但是有优化空间，这就是AIO。 作者原话：Not faster than NIO (epoll) on unix systems (which is true)There is no daragram suppportUnnecessary threading model (too much abstraction without usage) Reactor VS Proactor主动和被动以主动写为例： Reactor将handle放到select()，等待可写就绪，然后调用write()写入数据；写完处理后续逻辑； Proactor调用aoi_write后立刻返回，由内核负责写操作，写完后调用相应的回调函数处理后续逻辑；可以看出，Reactor被动的等待指示事件的到来并做出反应；它有一个等待的过程，做什么都要先放入到监听事件集合中等待handler可用时再进行操作； Proactor直接调用异步读写操作，调用完后立刻返回； 实现 Reactor实现了一个被动的事件分离和分发模型，服务等待请求事件的到来，再通过不受间断的同步处理事件，从而做出反应； Proactor实现了一个主动的事件分离和分发模型；这种设计允许多个任务并发的执行，从而提高吞吐量；并可执行耗时长的任务（各个任务间互不影响） 优点 Reactor实现相对简单，对于耗时短的处理场景处理高效； 操作系统可以在多个事件源上等待，并且避免了多线程编程相关的性能开销和编程复杂性； 事件的串行化对应用是透明的，可以顺序的同步执行而不需要加锁； 事务分离：将与应用无关的多路分解和分配机制和与应用相关的回调函数分离开来， Proactor性能更高，能够处理耗时长的并发场景； 缺点 Reactor处理耗时长的操作会造成事件分发的阻塞，影响到后续事件的处理； Proactor实现逻辑复杂；依赖操作系统对异步的支持，目前实现了纯异步操作的操作系统少，实现优秀的如windows IOCP，但由于其windows系统用于服务器的局限性，目前应用范围较小；而Unix/Linux系统对纯异步的支持有限，应用事件驱动的主流还是通过select/epoll来实现； 适用场景 Reactor：同时接收多个服务请求，并且依次同步的处理它们的事件驱动程序； Proactor：异步接收和同时处理多个服务请求的事件驱动程序； 为什么Redis是单线程？ 反应堆模型开发效率上比起直接使用IO复用要高，它通常是单线程的，设计目标是希望单线程使用一颗CPU的全部资源，但也有附带优点，即每个事件处理中很多时候可以不考虑共享资源的互斥访问。可是缺点也是明显的，现在的硬件发展，已经不再遵循摩尔定律，CPU的频率受制于材料的限制不再有大的提升，而改为是从核数的增加上提升能力，当程序需要使用多核资源时，反应堆模型就会悲剧，为何呢？如果程序业务很简单，例如只是简单的访问一些提供了并发访问的服务，就可以直接开启多个反应堆，每个反应堆对应一颗CPU核心，这些反应堆上跑的请求互不相关，这是完全可以利用多核的。例如Nginx这样的http静态服务器。如果程序比较复杂，例如一块内存数据的处理希望由多核共同完成，这样反应堆模型就很难做到了，需要昂贵的代价，引入许多复杂的机制。所以，大家就可以理解像redis、nodejs这样的服务，为什么只能是单线程，为什么memcached简单些的服务确可以是多线程。 参考https://www.cnblogs.com/xiexj/p/6874654.htmlhttp://www.taohui.org.cn/2016/01/27/%E9%AB%98%E6%80%A7%E8%83%BD%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B6-reactor%E5%8F%8D%E5%BA%94%E5%A0%86%E4%B8%8E%E5%AE%9A%E6%97%B6%E5%99%A8%E7%AE%A1%E7%90%86/]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你真的懂TCP和UDP？]]></title>
    <url>%2F2019%2F03%2F29%2F%E4%BD%A0%E7%9C%9F%E7%9A%84%E6%87%82TCP%E5%92%8CUDP%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[UDP协议疑难杂症全景解析https://blog.csdn.net/dog250/article/details/6896949 TCP协议疑难杂症全景解析https://blog.csdn.net/dog250/article/details/6612496 为什么QQ用的是UDP协议而不是TCP协议？说法1登陆采用TCP协议和HTTP协议，你和好友之间发送消息，主要采用UDP协议，内网传文件采用了P2P技术。总来的说：1.登陆过程，客户端client 采用TCP协议向服务器server发送信息，HTTP协议下载信息。登陆之后，会有一个TCP连接来保持在线状态。2.和好友发消息，客户端client采用UDP协议，但是需要通过服务器转发。腾讯为了确保传输消息的可靠，采用上层协议来保证可靠传输。如果消息发送失败，客户端会提示消息发送失败，并可重新发送。3.如果是在内网里面的两个客户端传文件，QQ采用的是P2P技术，不需要服务器中转。 说法2QQ既有UDP也有TCP！不管UDP还是TCP，最终登陆成功之后，QQ都会有一个TCP连接来保持在线状态。这个TCP连接的远程端口一般是80，采用UDP方式登陆的时候，端口是8000。 UDP协议是无连接方式的协议，它的效率高，速度快，占资源少，但是其传输机制为不可靠传送，必须依靠辅助的算法来完成传输控制。QQ采用的通信协议以UDP为主，辅以TCP协议。由于QQ的服务器设计容量是海量级的应用，一台服务器要同时容纳十几万的并发连接，因此服务器端只有采用UDP协议与客户端进行通讯才能保证这种超大规模的服务。 QQ客户端之间的消息传送也采用了UDP模式，因为国内的网络环境非常复杂，而且很多用户采用的方式是通过代理服务器共享一条线路上网的方式，在这些复杂的情况下，客户端之间能彼此建立起来TCP连接的概率较小，严重影响传送信息的效率。而UDP包能够穿透大部分的代理服务器，因此QQ选择了UDP作为客户之间的主要通信协议。 采用UDP协议，通过服务器中转方式。因此，现在的IP侦探在你仅仅跟对方发送聊天消息的时候是无法获取到IP的。大家都知道，UDP 协议是不可靠协议，它只管发送，不管对方是否收到的，但它的传输很高效。但是，作为聊天软件，怎么可以采用这样的不可靠方式来传输消息呢？于是，腾讯采用了上层协议来保证可靠传输：如果客户端使用UDP协议发出消息后，服务器收到该包，需要使用UDP协议发回一个应答包。如此来保证消息可以无遗漏传输。之所以会发生在客户端明明看到“消息发送失败”但对方又收到了这个消息的情况，就是因为客户端发出的消息服务器已经收到并转发成功，但客户端由于网络原因没有收到服务器的应答包引起的。 http://www.52im.net/thread-279-1-1.html 现在早期的腾讯QQ也同样面临C10K问题，只不过他们是用了UDP这种原始的包交换协议来实现的，绕开了这个难题，当然过程肯定是痛苦的。如果当时有epoll技术，他们肯定会用TCP。众所周之，后来的手机QQ、微信都采用TCP协议。 QQ通信原理聊天消息通信。采用UDP协议，通过服务器中转方式。因此，现在的IP侦探在你仅仅跟对方发送聊天消息的时候是无法获取到IP的。大家都知道，UDP协议是不可靠协议，它只管发送，不管对方是否收到的，但它的传输很高效。但是，作为聊天软件，怎么可以采用这样的不可靠方式来传输消息呢？于是，腾讯采用了上层协议来保证可靠传输：如果客户端使用UDP协议发出消息后，服务器收到该包，需要使用UDP协议发回一个应答包。如此来保证消息可以无遗漏传输。之所以会发生在客户端明明看到“消息发送失败”但对方又收到了这个消息 的情况，就是因为客户端发出的消息服务器已经收到并转发成功，但客户端由于网络原因没有收到服务器的应答包引起的。 发送消息的时候是UDP打洞,登陆的时候使用HTTPhttps://blog.csdn.net/gcc_sky/article/details/18663069用UDP协议通讯时怎样得知目标机是否获得了数据包？大家都知道，UDP协议通信必须在同一个局域网内UDP包中包括一个TimeStamp,如果接收者收到,将TimeStamp返回.在UDP之上自定义一个通讯协议：每个数据包中包含一个唯一标识，可以用编号也可以用时间；接收端收到数据包后回发一个数据包，包含收到的这个唯一标识；发送端在预定时间内没有收到回执则自动重发，重发一定次数后仍未收到回执则认为发送失败。 迅雷是UDP? UDP打洞和TCP打洞？就是udp的，视频传输数量大，速度要求高，所以用udp，tcp如果出现错误要重传，这是不符合效率要求的，你说的tcp连接的是ftp服务http://www.cnblogs.com/LeoWong/archive/2009/09/25/1574265.html server最大tcp连接数：server通常固定在某个本地端口上监听，等待client的连接请求。不考虑地址重用（unix的SO_REUSEADDR选项）的情况下，即使server端有多个ip，本地监听端口也是独占的，因此server端tcp连接4元组中只有remote ip（也就是client ip）和remote port（客户端port）是可变的，因此最大tcp连接为客户端ip数×客户端port数，对IPV4，不考虑ip地址分类等因素，最大tcp连接数约为2的32次方（ip数）×2的16次方（port数），也就是server端单机最大tcp连接数约为2的48次方。 C10m的问题Robert Graham的结论是：OS的内核不是解决C10M问题的办法，恰恰相反OS的内核正是导致C10M问题的关键所在。这也就意味着： 不要让OS内核执行所有繁重的任务：将数据包处理、内存管理、处理器调度等任务从内核转移到应用程序高效地完成，让诸如Linux这样的OS只处理控制层，数据层完全交给应用程序来处理。 最终就是要设计这样一个系统，该系统可以处理千万级别的并发连接，它在200个时钟周期内处理数据包，在14万个时钟周期内处理应用程序逻辑。由于一次主存储器访问就要花费300个时钟周期，所以这是最大限度的减少代码和缓存丢失的关键。 面向数据层的系统可以每秒处理1千万个数据包，面向控制层的系统，每秒只能处理1百万个数据包。这似乎很极端，请记住一句老话：可扩展性是专业化的，为了做好一些事情，你不能把性能问题外包给操作系统来解决，你必须自己做。http://www.52im.net/thread-568-1-1.html 解决C10M问题的思路总结综上所述，解决C10M问题的关键主要是从下面几个方面入手： 网卡问题：通过内核工作效率不高解决方案：使用自己的驱动程序并管理它们，使适配器远离操作系统。 CPU问题：使用传统的内核方法来协调你的应用程序是行不通的。解决方案：Linux管理前两个CPU，你的应用程序管理其余的CPU，中断只发生在你允许的CPU上。 内存问题：内存需要特别关注，以求高效。解决方案：在系统启动时就分配大部分内存给你管理的大内存页。 以Linux为例，解决的思咯就是将控制层交给Linux，应用程序管理数据。应用程序与内核之间没有交互、没有线程调度、没有系统调用、没有中断，什么都没有。 然而，你有的是在Linux上运行的代码，你可以正常调试，这不是某种怪异的硬件系统，需要特定的工程师。你需要定制的硬件在数据层提升性能，但是必须是在你熟悉的编程和开发环境上进行。 从C10K到C10M高性能网络应用的理论探索http://www.52im.net/thread-578-1-1.html IO模型http://www.52im.net/thread-1935-1-1.html 线程模型http://www.52im.net/thread-1939-1-1.html]]></content>
      <categories>
        <category>TCP/IP</category>
      </categories>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器可视化监控中心搭建]]></title>
    <url>%2F2019%2F03%2F23%2FDocker%E5%AE%B9%E5%99%A8%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9B%91%E6%8E%A7%E4%B8%AD%E5%BF%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[背景我的服务器上跑了大概15个容器，比如kafka，zookeeper，ES等，我想监控他们怎么办？ 准备镜像 adviser：负责收集容器的随时间变化的数据 influxdb：负责存储时序数据 grafana：负责分析和展示时序数据 从下镜像到放弃（最终还是没成功）解决 Docker pull 出现的net/http: TLS handshake timeout 的一个办法使用国内的Docker仓库daocloud：(没尝试)12$ &quot;DOCKER_OPTechoS=\&quot;\$DOCKER_OPTS --registry-mirror=http://f2d6cb40.m.daocloud.io\&quot;&quot; | sudo tee -a /etc/default/docker$ sudo service docker restart 遇到一个问题docker: Error response from daemon: Get https://registry-1.docker.io/v2/: x509: certificate is valid for goldopen.org, www.goldopen.org, not registry-1.docker.io. 经查资料，说1I suspect it&apos;s a man-in-the-middle proxy that modifies the SSL request. There are some instructions for configuring a proxy to be used by the docker daemon here; https://github.com/moby/moby/issues/33154 我试了一下这玩意openssl s_client -connect registry-1.docker.io:443 返回了 以下内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990CONNECTED(00000003)depth=4 C = US, O = "Starfield Technologies, Inc.", OU = Starfield Class 2 Certification Authorityverify return:1depth=3 C = US, ST = Arizona, L = Scottsdale, O = "Starfield Technologies, Inc.", CN = Starfield Services Root Certificate Authority - G2verify return:1depth=2 C = US, O = Amazon, CN = Amazon Root CA 1verify return:1depth=1 C = US, O = Amazon, OU = Server CA 1B, CN = Amazonverify return:1depth=0 CN = goldopen.orgverify return:1---Certificate chain 0 s:/CN=goldopen.org i:/C=US/O=Amazon/OU=Server CA 1B/CN=Amazon 1 s:/C=US/O=Amazon/OU=Server CA 1B/CN=Amazon i:/C=US/O=Amazon/CN=Amazon Root CA 1 2 s:/C=US/O=Amazon/CN=Amazon Root CA 1 i:/C=US/ST=Arizona/L=Scottsdale/O=Starfield Technologies, Inc./CN=Starfield Services Root Certificate Authority - G2 3 s:/C=US/ST=Arizona/L=Scottsdale/O=Starfield Technologies, Inc./CN=Starfield Services Root Certificate Authority - G2 i:/C=US/O=Starfield Technologies, Inc./OU=Starfield Class 2 Certification Authority---Server certificate-----BEGIN CERTIFICATE-----MIIFdDCCBFygAwIBAgIQAhgLcb8KjmLQqtrwjZIfSTANBgkqhkiG9w0BAQsFADBGMQswCQYDVQQGEwJVUzEPMA0GA1UEChMGQW1hem9uMRUwEwYDVQQLEwxTZXJ2ZXIgQ0EgMUIxDzANBgNVBAMTBkFtYXpvbjAeFw0xODA4MzEwMDAwMDBaFw0xOTA5MzAxMjAwMDBaMBcxFTATBgNVBAMTDGdvbGRvcGVuLm9yZzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAJD0no7+tthdbiSHNU76RGaGvIJ1OiwnHdwP37x86XlOfUkpd3eiCvCcZ02xHhK7EDrI2pW6qKLQ7lMheK6yjlqEjT8tbQt2KUOebabRGIoRKEYREbH917UUuS1zI5r64LR0kOfytNuKVkC5yCk8RXnh1xx2ZtqC4eBIFlNQzCEeJ9rS1hkPfF36+pEJkxVCluz3/RDDHoEBl8r/fxmcDzjUEI3fPIyEM3XGGKv52WPBGsoG7l/Lmdm7qSEiXaLjRvi3EWfq48lDPcYSEFTeFq47k+s6Ua1K4xbdTVPVuvyLDwqGyF0HAefLDDRQEJ/dZx6NOapD3RjppeMBNHoteVMCAwEAAaOCAoswggKHMB8GA1UdIwQYMBaAFFmkZgZSoHuVkjyjlAcnlnRb+T3QMB0GA1UdDgQWBBReRu9vDv2ZhjPTej4AAZBjGRGT+jApBgNVHREEIjAgggxnb2xkb3Blbi5vcmeCEHd3dy5nb2xkb3Blbi5vcmcwDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMBBggrBgEFBQcDAjA7BgNVHR8ENDAyMDCgLqAshipodHRwOi8vY3JsLnNjYTFiLmFtYXpvbnRydXN0LmNvbS9zY2ExYi5jcmwwIAYDVR0gBBkwFzALBglghkgBhv1sAQIwCAYGZ4EMAQIBMHUGCCsGAQUFBwEBBGkwZzAtBggrBgEFBQcwAYYhaHR0cDovL29jc3Auc2NhMWIuYW1hem9udHJ1c3QuY29tMDYGCCsGAQUFBzAChipodHRwOi8vY3J0LnNjYTFiLmFtYXpvbnRydXN0LmNvbS9zY2ExYi5jcnQwDAYDVR0TAQH/BAIwADCCAQUGCisGAQQB1nkCBAIEgfYEgfMA8QB2AKS5CZC0GFgUh7sTosxncAo8NZgE+RvfuON3zQ7IDdwQAAABZZGDMPgAAAQDAEcwRQIhAJMT8qC4MfkvcZVFy5TJkAwVEeIa6WVhXx7kIbczhKY/AiAuMRHhM8Fyb9dY9TdIb6BOGRryTQU+3VX8FKq92X7LXgB3AId1v+dZfPiMQ5lfvfNu/1aNR1Y2/0q1YMG06v9eoIMPAAABZZGDMb0AAAQDAEgwRgIhAIiRYC3UzfG92mYkaK+nuSZ801XfDTJceBHbKZNNAv7HAiEAts4qSJowajeMAG245yTh1/Y2yTkLxiR7oGkb9Bx8DBowDQYJKoZIhvcNAQELBQADggEBALarCZePQIy/J821dkTEOp1p2QurBUpU6AqADnoAzwHhF4NNBx0jXUwwgbHFVs6XFizrYfsUKulEU9xPS7xlGAIJNoo7r/7DV0nZtiqW1Re726PAjoWAUJOdXowFH9ddXNBtT7rRPK4hXauQcfU7KvIBVUV0NGWqyAhbcUFMVYV7Sj10FqvrUbcE7wviv/be1KVHkc3bJhYC3ACNy99lzfMRDOyQrG7sgYvwIKYC7W+f1kOyRZJ2FjaXn0woLT5yRe7QrRijZOrf1NIGaoPAYnC6AO+WZtbKf468BNhzu60fhjaR7giQwKoSG1lb9wzIDZQBUMYhODh8fC5MbIHlTUI=-----END CERTIFICATE-----subject=/CN=goldopen.orgissuer=/C=US/O=Amazon/OU=Server CA 1B/CN=Amazon---No client certificate CA names sentPeer signing digest: SHA512Server Temp Key: ECDH, P-256, 256 bits---SSL handshake has read 5336 bytes and written 431 bytes---New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES128-GCM-SHA256Server public key is 2048 bitSecure Renegotiation IS supportedCompression: NONEExpansion: NONENo ALPN negotiatedSSL-Session: Protocol : TLSv1.2 Cipher : ECDHE-RSA-AES128-GCM-SHA256 Session-ID: 42F89657AF05B89E836E24D639F5F84F23A3622E2D6661AD8C6C05E49CE30EC0 Session-ID-ctx: Master-Key: 2BCBB105DA476E2BF44FC37499AEE16BE237DA4BD72FBEB58BA636A7CF14B6442F424D8067C5122F1B0D6409583E7EA0 Key-Arg : None PSK identity: None PSK identity hint: None SRP username: None Start Time: 1552801551 Timeout : 300 (sec) Verify return code: 0 (ok)---HTTP/1.1 400 BAD_REQUESTContent-Length: 0Connection: Closeclosed 我又试了一下这个curl https://registry-1.docker.io/v2/ &amp;&amp; echo Works || echo Problem输出12curl: (51) SSL: certificate subject name (goldopen.org) does not match target host name 'registry-1.docker.io'Problem 这是因为 curl 访问 https 服务器时，会验证服务器证书的有效性和证书域名与访问域名一致性 解决办法： 修改 curl 选项，使其不验证服务器证书curl_setopt($curl, CURLOPT_SSL_VERIFYHOST, FALSE);curl_setopt($process, CURLOPT_SSL_VERIFYPEER, FALSE); 针对 curl 命令，-k 选项，也可以使其不验证证书 保证证书域名与访问的域名一致，因为访问的是一个 IP，在 hosts 文件添加IP域名关系映射，然后使用服务器证书的域名进行访问 重启之路重启docekr引擎，而不用关闭容器12345678修改docker 配置文件sudo vim /etc/docker/daemon.json添加&quot;live-restore&quot;: true选项，比如：&#123; &quot;live-restore&quot;: true,&#125; sudo service docker restart 重启中出错1Job for docker.service failed. See &quot;systemctl status docker.service&quot; and &quot;journalctl -xe&quot; for details. 可能跟/etc/docker/daemon.json 文件中错误的配置引起的 有关系 ,我把这个文件删掉了 在运行docker容器时可以加如下参数来保证每次docker服务重启后容器也自动重启：docker run --restart=always 如果已经启动了则可以使用如下命令：docker update --restart=always &lt;CONTAINER ID&gt; 以上都不行，那么还是换中国镜像吧！！！Docker 中国官方镜像加速可通过 registry.docker-cn.com 访问。目前该镜像库只包含流行的公有镜像，而私有镜像仍需要从美国镜像库中拉取。 方式一:使用以下命令直接从该镜像加速地址进行拉取。docker pull registry.docker-cn.com/myname/myrepo:mytag1比如 docker pull registry.docker-cn.com/grafana/grafana 注:除非您修改了Docker守护进程的–registry-mirror参数,否则您将需要完整地指定官方镜像的名称。例如，library/ubuntu、library/redis、library/nginx。 方式二：通过配置文件的方式给Docker守护进程配置加速器 通过配置文件启动Docker,修改 /etc/docker/daemon.json 文件并添加上 registry-mirrors 键值。12345678910sudo vim /etc/docker/daemon.json1&#123; "registry-mirrors": ["https://registry.docker-cn.com"]也可选用网易的镜像地址：http://hub-mirror.c.163.com&#123; “registry-mirrors”: [“http://hub-mirror.c.163.com“] &#125; 修改保存后，重启 Docker 以使配置生效。 sudo service docker restart 排除万难，下面开始正式部署该监控项目部署Influxdb服务 docker run -d -p 8086:8086 -v ~/influxdb:/var/lib/influxdb --net &quot;mybridge&quot; --name influxdb registry.docker-cn.com/tutum/influxdb 进入influxdb容器内部，并执行influx命令：docker exec -it influxdb influx 然后创建数据库test和root用户用于本次试验测试 CREATE DATABASE &quot;test&quot;CREATE USER &quot;root&quot; WITH PASSWORD &#39;root&#39; WITH ALL PRIVILEGES 部署cAdvisor服务谷歌的cadvisor可以用于收集Docker容器的时序信息，包括容器运行过程中的资源使用情况和性能数据。运行cadvisor服务 docker run -d -v /:/rootfs -v /var/run:/var/run -v /sys:/sys -v /var/lib/docker:/var/lib/docker --link=influxdb:influxdb --net &quot;mybridge&quot; --name cadvisor google/cadvisor:v0.27.3 --storage_driver=influxdb --storage_driver_host=influxdb:8086 --storage_driver_db=test --storage_driver_user=root --storage_driver_password=root 特别注意项： 在运行上述docker时，这里有可能两个其他配置项需要添加（CentOS, RHEL需要）： –privileged=true 设置为true之后，容器内的root才拥有真正的root权限，可以看到host上的设备，并且可以执行mount；否者容器内的root只是外部的一个普通用户权限。由于cadvisor需要通过socket访问docker守护进程，在CentOs和RHEL系统中需要这个这个选项。 –volume=/cgroup:/cgroup:ro 对于CentOS和RHEL系统的某些版本（比如CentOS6），cgroup的层级挂在/cgroup目录，所以运行cadvisor时需要额外添加–volume=/cgroup:/cgroup:ro选项。 部署Grafana服务docker run -d -p 5000:3000 -v ~/grafana:/var/lib/grafana --link=influxdb:influxdb --net &quot;mybridge&quot; --name grafana grafana/grafana 用3000:3000报错 12docker: Error response from daemon: driver failed programming external connectivity on endpoint grafana (4939b60d4efcd97a12f101237a2844ffd435296d988f2305cdc46a5cccf521c9): Bind for 0.0.0.0:3000 failed: port is already allocated. 换成5000:3000，然后又出现如下错误123GF_PATHS_DATA=&apos;/var/lib/grafana&apos; is not writable.You may have issues with file permissions, more information here: http://docs.grafana.org/installation/docker/#migration-from-a-previous-version-of-the-docker-container-to-5-1-or-latermkdir: cannot create directory &apos;/var/lib/grafana/plugins&apos;: Permission denied 解决办法：给~/grafana目录赋予777权限 如何监控 首先登陆grafana界面，如图 用户名 密码均为admin 在Grafana上有好几个步骤需要做，这里Install Grafana已经完成了，接下来我们需要： Add data sourceCreate dashboard……Add Data Source 点击Add data source进入，如图 我们需要根据实际情况来填写各项内容： 需要添加仪表盘（Dashboard） Add Dashboard 点击Add dashboard进入 这里有很多类型的仪表盘供选择，我们选用最常用的Graph就好进入之后，点击Panel Title下拉列表，再选择Edit进行编辑即可 在Edit里面主要的就是需要添加查询的条件，继续看下文 Add Query Editor 查询条件中我们可以选择要监控的指标： 这里选一个memory usage好了，然后要监控的容器选择grafana自身好了。当然这里不止可以监控一个指标，也不止可以监控一个容器，更多组合我们只需要在下面并列着一个一个添加query条目就好！ 最后我添加了三个监控条件，分别用于监控grafana、influxdb和cadvisor三个容器的memory usage指标，并将其同时显示于图中，怎么样是不是很直观！ 参考https://blog.roncoo.com/article/132750]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql之GTID]]></title>
    <url>%2F2019%2F03%2F23%2FMysql%E4%B9%8BGTID%2F</url>
    <content type="text"><![CDATA[万恶之源今天看一个博客的时候,发现了一个新东西GTID 这篇博客说道:CDC模块可以从mysql那里拿binlogCDC 模块解析 binlog，产生特定格式的变更消息，也就完成了一次变更抓取。但这还不够，CDC 模块本身也可能挂掉，那么恢复之后如何保证不丢数据又是一个问题。这个问题的解决方案也是要针对不同数据源进行设计的，就 MySQL 而言，通常会持久化已经消费的 binlog 位点或 Gtid(MySQL 5.6之后引入)来标记上次消费位置。其中更好的选择是 Gtid，因为该位点对于一套 MySQL 体系（主从或多主）是全局的，而 binlog 位点是单机的，无法支持主备或多主架构。 https://aleiwu.com/post/vimur/ 我开始不懂了。。。。。。。。 什么是GTIDMySQL5.6在5.5的基础上增加了GTID， GTID即全局事务ID（global transaction identifier），GTID实际上是由UUID+TID组成的。其中UUID是一个MySQL实例的唯一标识。TID代表了该实例上已经提交的事务数量，并且随着事务提交单调递增，所以GTID能够保证每个MySQL实例事务的执行（不会重复执行同一个事务，并且会补全没有执行的事务）。下面是一个GTID的具体形式： 4e659069-3cd8-11e5-9a49-001c4270714e:1-77 GTID用于在binlog中唯一标识一个事务。当事务提交时，MySQL Server在写binlog的时候，会先写一个特殊的Binlog Event，类型为GTID_Event，指定下一个事务的GTID，然后再写事务的Binlog。主从同步时GTID_Event和事务的Binlog都会传递到从库，从库在执行的时候也是用同样的GTID写binlog，这样主从同步以后，就可通过GTID确定从库同步到的位置了。也就是说，无论是级联情况，还是一主多从情况，都可以通过GTID自动找点儿，而无需像之前那样通过File_name和File_position找点儿了。 GTID有啥用？ 因为清楚了GTID的格式，所以通过UUID可以知道这个事务在哪个实例上提交的。 通过GTID可以极方便的进行复制结构上的故障转移，新主设置。很好的解决了下面这个图（图来自高性能MySQL第10章）的问题。 上面图的意思是：Server1(Master)崩溃，根据从上show slave status获得Master_log_File/Read_Master_Log_Pos的值，Server2(Slave)已经跟上了主，Server3(Slave)没有跟上主。这时要是把Server2提升为主，Server3变成Server2的从。这时在Server3上执行change的时候需要做一些计算，这里就不做说明了，具体的说明见高性能MySQL第10章，相对来说是比较麻烦的。 这个问题在5.6的GTID出现后，就显得非常的简单。由于同一事务的GTID在所有节点上的值一致，那么根据Server3当前停止点的GTID就能定位到Server2上的GTID。甚至由于MASTER_AUTO_POSITION功能的出现，我们都不需要知道GTID的具体值，直接使用CHANGE MASTER TO MASTER_HOST=’xxx’， MASTER_AUTO_POSITION命令就可以直接完成failover的工作。 原理：从服务器连接到主服务器之后，把自己执行过的GTID(Executed_Gtid_Set)&lt;SQL线程&gt; 、获取到的GTID(Retrieved_Gtid_Set）&lt;IO线程&gt;发给主服务器，主服务器把从服务器缺少的GTID及对应的transactions发过去补全即可。当主服务器挂掉的时候，找出同步最成功的那台从服务器，直接把它提升为主即可。如果硬要指定某一台不是最新的从服务器提升为主， 先change到同步最成功的那台从服务器， 等把GTID全部补全了，就可以把它提升为主了。 使用GTID注意事项 开启GTID以后，无法使用sql_slave_skip_counter跳过事务。前面介绍过了，使用GTID找点儿时，主库会把从库缺失的GTID，发送给从库，所以skip是没有用的。为了提前发现问题，MySQL在gtid模式下，直接禁止使用set global sql_slave_skip_counter ＝ x。正确的做法是，通过set grid_next= ‘zzzz’（’zzzz’为待跳过的事务），然后执行BIGIN;COMMIT产生一个空事务，占据这个GTID，再START SLAVE，会发现下一条事务的GTID已经执行过，就会跳过这个事务了 如果一个GTID已经执行过，再遇到重复的GTID，从库会直接跳过，可看作GTID执行的幂等性。 使用限制：https://dev.mysql.com/doc/refman/5.6/en/replication-gtids-restrictions.html 业界经验： https://code.facebook.com/posts/1542273532669494/lessons-from-deploying-mysql-gtid-at-scale/ https://www.percona.com/blog/2015/02/10/online-gtid-rollout-now-available-percona-server-5-6/ 参考https://www.cnblogs.com/zhoujinyi/p/4717951.htmlhttps://www.cnblogs.com/zejin2008/p/7705473.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之堆外内存]]></title>
    <url>%2F2019%2F03%2F23%2FJava%E4%B9%8B%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%2F</url>
    <content type="text"><![CDATA[堆外内存Netty的ByteBuffer采用DIRECT BUFFERS，使用堆外直接内存进行Socket读写，不需要进行字节缓冲区的二次拷贝。如果大家自己动手写过NIO和AIO的程序，就会知道我们接触的网络传输数据是直接和ByteBuffer打交道的。在JAVA的API中，一个ByteBuffer读完数据后要flip一下，将当前操作位置设置为0.里详细介绍了Netty的ByteBuf怎样将这个缓冲区分成一段一段的，还可以压缩，将读的数据滑向一侧。而堆外内存的零拷贝，如果有JVM基础也很好理解。Java内存模型里提到每个线程都有自己的高速工作内存空间，而不是直接访问主内存。想不用工作内存，直接主内存可见，就要用volatile关键字修饰。所以堆内内存，走JVM就存在这个拷贝开销。https://www.cnblogs.com/xiexj/p/6874654.html 有个小问题：堆内内存包括永久代码？ 和堆内内存相对应，堆外内存就是把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机），这样做的结果就是能够在一定程度上减少垃圾回收对应用程序造成的影响。 作为JAVA开发者我们经常用java.nio.DirectByteBuffer对象进行堆外内存的管理和使用，它会在对象创建的时候就分配堆外内存。 DirectByteBuffer类是在Java Heap外分配内存，对堆外内存的申请主要是通过成员变量unsafe来操作，下面介绍构造方法 12345678910111213141516171819202122232425262728293031DirectByteBuffer(int cap) &#123; super(-1, 0, cap, cap); //内存是否按页分配对齐 boolean pa = VM.isDirectMemoryPageAligned(); //获取每页内存大小 int ps = Bits.pageSize(); //分配内存的大小，如果是按页对齐方式，需要再加一页内存的容量 long size = Math.max(1L, (long)cap + (pa ? ps : 0)); //用Bits类保存总分配内存(按页分配)的大小和实际内存的大小 Bits.reserveMemory(size, cap); long base = 0; try &#123; //在堆外内存的基地址，指定内存大小 base = unsafe.allocateMemory(size); &#125; catch (OutOfMemoryError x) &#123; Bits.unreserveMemory(size, cap); throw x; &#125; unsafe.setMemory(base, size, (byte) 0); //计算堆外内存的基地址 if (pa &amp;&amp; (base % ps != 0)) &#123; // Round up to page boundary address = base + ps - (base &amp; (ps - 1)); &#125; else &#123; address = base; &#125; cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null; &#125; 在Cleaner 内部中通过一个列表，维护了一个针对每一个 directBuffer 的一个回收堆外内存的 线程对象(Runnable)，回收操作是发生在 Cleaner 的 clean() 方法中。 12345678910111213141516171819202122private static class Deallocator implements Runnable &#123; private static Unsafe unsafe = Unsafe.getUnsafe(); private long address; private long size; private int capacity; private Deallocator(long address, long size, int capacity) &#123; assert (address != 0); this.address = address; this.size = size; this.capacity = capacity; &#125; public void run() &#123; if (address == 0) &#123; // Paranoia return; &#125; unsafe.freeMemory(address); address = 0; Bits.unreserveMemory(size, capacity); &#125;&#125; 使用堆外内存的优点 减少了垃圾回收因为垃圾回收会暂停其他的工作。 加快了复制的速度堆内在flush到远程时，会先复制到直接内存（非堆内存），然后在发送；而堆外内存相当于省略掉了这个工作。 同样任何一个事物使用起来有优点就会有缺点，堆外内存的缺点就是内存难以控制，使用了堆外内存就间接失去了JVM管理内存的可行性，改由自己来管理，当发生内存溢出时排查起来非常困难。 使用DirectByteBuffer的注意事项 java.nio.DirectByteBuffer对象在创建过程中会先通过Unsafe接口直接通过os::malloc来分配内存，然后将内存的起始地址和大小存到java.nio.DirectByteBuffer对象里，这样就可以直接操作这些内存。这些内存只有在DirectByteBuffer回收掉之后才有机会被回收，因此如果这些对象大部分都移到了old，但是一直没有触发CMS GC或者Full GC，那么悲剧将会发生，因为你的物理内存被他们耗尽了，因此为了避免这种悲剧的发生，通过-XX:MaxDirectMemorySize来指定最大的堆外内存大小，当使用达到了阈值的时候将调用System.gc来做一次full gc，以此来回收掉没有被使用的堆外内存。 使用 HeapByteBuffer 还需要经过一次 DirectByteBuffer 的拷贝，在追求极致性能的场景下是可以通过直接复用堆外内存来避免的。 多线程下使用 HeapByteBuffer 进行文件读写，要注意 ThreadLocal&lt;Util.BufferCache&gt; bufferCache 导致的堆外内存膨胀的问题。 监控堆外内存在Java VisualVM里安装插件，Buffer Pools 插件可以监控堆外内存（包含 DirectByteBuffer 和 MappedByteBuffer） 堆外内存如何回收？ 通过System.gc 123456789public class WriteByDirectByteBufferTest &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; ByteBuffer buffer = ByteBuffer.allocateDirect(1024 * 1024 * 1024); System.in.read(); buffer = null; System.gc(); //GC 时会触发堆外空闲内存的回收。 new CountDownLatch(1).await(); &#125;&#125; 手动回收可以立刻释放堆外内存，不需要等待到 GC 的发生。 12345678public class WriteByDirectByteBufferTest &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; ByteBuffer buffer = ByteBuffer.allocateDirect(1024 * 1024 * 1024); System.in.read(); ((DirectBuffer) buffer).cleaner().clean(); new CountDownLatch(1).await(); &#125;&#125; 开源堆外缓存框架关于堆外缓存的开源实现。查询了一些资料后了解到的主要有： Ehcache 3.0：3.0基于其商业公司一个非开源的堆外组件的实现。 Chronical Map：OpenHFT包括很多类库，使用这些类库很少产生垃圾，并且应用程序使用这些类库后也很少发生Minor GC。类库主要包括：Chronicle Map，Chronicle Queue等等。 OHC：来源于Cassandra 3.0， Apache v2。 Ignite: 一个规模宏大的内存计算框架，属于Apache项目。 MappedBytebufferMappedByteBuffer 映射出一片文件内容之后，不会全部加载到内存中，而是会进行一部分的预读（体现在占用的那 100M 上），MappedByteBuffer 不是文件读写的银弹，它仍然依赖于 PageCache 异步刷盘的机制。通过 Java VisualVM 可以监控到 mmap 总映射的大小，但并不是实际占用的内存量。 如何回收MappedBytebuffer1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class MmapUtil &#123; public static void clean(MappedByteBuffer mappedByteBuffer) &#123; ByteBuffer buffer = mappedByteBuffer; if (buffer == null || !buffer.isDirect() || buffer.capacity() == 0) return; invoke(invoke(viewed(buffer), "cleaner"), "clean"); &#125; private static Object invoke(final Object target, final String methodName, final Class&lt;?&gt;... args) &#123; return AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; public Object run() &#123; try &#123; Method method = method(target, methodName, args); method.setAccessible(true); return method.invoke(target); &#125; catch (Exception e) &#123; throw new IllegalStateException(e); &#125; &#125; &#125;); &#125; private static Method method(Object target, String methodName, Class&lt;?&gt;[] args) throws NoSuchMethodException &#123; try &#123; return target.getClass().getMethod(methodName, args); &#125; catch (NoSuchMethodException e) &#123; return target.getClass().getDeclaredMethod(methodName, args); &#125; &#125; private static ByteBuffer viewed(ByteBuffer buffer) &#123; String methodName = "viewedBuffer"; Method[] methods = buffer.getClass().getMethods(); for (int i = 0; i &lt; methods.length; i++) &#123; if (methods[i].getName().equals("attachment")) &#123; methodName = "attachment"; break; &#125; &#125; ByteBuffer viewedBuffer = (ByteBuffer) invoke(buffer, methodName); if (viewedBuffer == null) return buffer; else return viewed(viewedBuffer); &#125;&#125; 测试类:通过一顿复杂的反射操作，成功地手动回收了 Mmap 的内存映射。1234567891011public class WriteByMappedByteBufferTest &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; File data = new File("/tmp/data.txt"); data.createNewFile(); FileChannel fileChannel = new RandomAccessFile(data, "rw").getChannel(); MappedByteBuffer map = fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, 1024L * 1024 * 1024); System.in.read(); MmapUtil.clean(map); new CountDownLatch(1).await(); &#125;&#125; 参考https://blog.csdn.net/ZYC88888/article/details/80228531https://juejin.im/post/5c8de9f5e51d453651442c6a]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之锁]]></title>
    <url>%2F2019%2F03%2F23%2FJava%E4%B9%8B%E9%94%81%2F</url>
    <content type="text"><![CDATA[synchronized基本原理Java中每个对象都有一个内置锁(监视器,也可以理解成锁标记)，而synchronized就是使用对象的内置锁(监视器)来将代码块(方法)锁定的！ 同步代码块： monitorenter和monitorexit指令实现的 同步方法（在这看不出来需要看JVM底层实现） 方法修饰符上的ACC_SYNCHRONIZED实现。 synchronized底层是是通过monitor对象，对象有自己的对象头，存储了很多信息，其中一个信息标示是被哪个线程持有。 具体可参考： https://blog.csdn.net/chenssy/article/details/54883355 https://blog.csdn.net/u012465296/article/details/53022317 为什么说Synchronized是重量级锁？这时因为当获取monitor的线程发现此monitor已被其他线程持有时会陷入BLOCKED阻塞状态，而这一操作是通过LWP来完成的，会引起用户态到内核态的切换，这甚至会导致切换到内核态调用OS将线程阻塞的时间比同步代码块执行所需的时间还要长。 简述锁的等级 （方法锁、对象锁、类锁） 无论是修饰方法还是修饰代码块都是 对象锁,当一个线程访问一个带synchronized方法时，由于对象锁的存在，所有加synchronized的方法都不能被访问（前提是在多个线程调用的是同一个对象实例中的方法） 无论是修饰静态方法还是锁定某个对象,都是 类锁.一个class其中的静态方法和静态变量在内存中只会加载和初始化一份，所以，一旦一个静态的方法被申明为synchronized，此类的所有的实例化对象在调用该方法时，共用同一把锁，称之为类锁。 synchronized修饰静态方法获取的是类锁(类的字节码文件对象)，synchronized修饰普通方法或代码块获取的是对象锁。 它俩是不冲突的，也就是说：获取了类锁的线程和获取了对象锁的线程是不冲突的！12345678910111213141516171819202122232425262728293031323334353637383940414243public class SynchoronizedDemo &#123; //synchronized修饰非静态方法 public synchronized void function() throws InterruptedException &#123; for (int i = 0; i &lt;3; i++) &#123; Thread.sleep(1000); System.out.println("function running..."); &#125; &#125; //synchronized修饰静态方法 public static synchronized void staticFunction() throws InterruptedException &#123; for (int i = 0; i &lt; 3; i++) &#123; Thread.sleep(1000); System.out.println("Static function running..."); &#125; &#125; public static void main(String[] args) &#123; final SynchoronizedDemo demo = new SynchoronizedDemo(); // 创建线程执行静态方法 Thread t1 = new Thread(() -&gt; &#123; try &#123; staticFunction(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); // 创建线程执行实例方法 Thread t2 = new Thread(() -&gt; &#123; try &#123; demo.function(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); // 启动 t1.start(); t2.start(); &#125;&#125; Tips: synchronized关键字不能继承。也就是说子类重写了父类中用synchronized修饰的方法，子类的方法仍然不是同步的。 定义接口方法时，不能使用synchronized关键字。 构造方法不能使用synchronized关键字，但是可以使用synchronized代码块。 synchronized锁的膨胀过程是怎样的？您指的是偏向锁、轻量级锁和重量级锁吗？就是如果临界区只有一个线程访问，这时会将锁对象的Mark Word中的偏向线程ID指向该线程，此后该线程进入临界区将省去锁获取-释放，毕竟锁获取-释放是有开销的。但是如果访问临界区的线程变多了，这时撤销偏向和重置偏向会有一定的开销，因而膨胀成轻量级锁，思路是会在线程的栈中存一份锁对象的Mark Word副本，称之为Displaced Mark Word，并该内存的指针存入锁对象的Mark Word。每个线程在获取锁的时候都需要CAS替换该指针，使其指向自己栈中的Displaced Mark Word。CAS替换失败则说明并发程度较高，膨胀成重量级锁，具有排他性。 在spring事务中使用synchronized应该注意的问题@Transcational注解和synchronized一起使用了，加锁的范围没有包括到整个事务。所以我们可以这样做： 新建一个名叫SynchronizedService类，让其去调用addEmployee()方法，整个代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@RestControllerpublic class EmployeeController &#123; @Autowired private SynchronizedService synchronizedService ; @RequestMapping("/add") public void addEmployee() &#123; for (int i = 0; i &lt; 1000; i++) &#123; new Thread(() -&gt; synchronizedService.synchronizedAddEmployee()).start(); &#125; &#125;&#125;// 新建的Service类@Servicepublic class SynchronizedService &#123; @Autowired private EmployeeService employeeService ; // 同步 public synchronized void synchronizedAddEmployee() &#123; employeeService.addEmployee(); &#125;&#125;@Servicepublic class EmployeeService &#123; @Autowired private EmployeeRepository employeeRepository; @Transactional public void addEmployee() &#123; // 查出ID为8的记录，然后每次将年龄增加一 Employee employee = employeeRepository.getOne(8); System.out.println(Thread.currentThread().getName() + employee); Integer age = employee.getAge(); employee.setAge(age + 1); employeeRepository.save(employee); &#125;&#125; ReentrantLock什么情况下使用ReenTrantLock：需要实现ReenTrantLock的三个独有功能时。 ReenTrantLock独有的能力： ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。 ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。 ReenTrantLock实现的原理：在网上看到相关的源码分析，本来这块应该是本文的核心，但是感觉比较复杂就不一一详解了，简单来说，ReenTrantLock的实现是一种自旋锁，通过循环调用CAS操作来实现加锁。它的性能比较好也是因为避免了使线程进入内核态的阻塞状态。想尽办法避免线程进入内核的阻塞状态是我们去分析和理解锁设计的关键钥匙。 synchronized 和 ReentrantLock 有什么不同 能否可重入他们两个都是可重入的，（可重入指的是同一个线程每进入一次，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。）平常开发中大多数还是用的synchronized,比较高级的才用后者， 能否造成死锁Synchronized不会造成死锁，因为jvm会自动释放，别的锁有可能会造成死锁。 锁的实现Synchronized是依赖于JVM实现的，而ReenTrantLock是JDK实现的，有什么区别，说白了就类似于操作系统来控制实现和用户自己敲代码实现的区别。前者的实现是比较难见到的，后者有直接的源码可供阅读。 性能的区别在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。 功能区别便利性：很明显Synchronized的使用比较方便简洁，并且由编译器去保证锁的加锁和释放，而ReenTrantLock需要手工声明来加锁和释放锁，为了避免忘记手工释放锁造成死锁，所以最好在finally中声明释放锁。锁的细粒度和灵活度：很明显ReenTrantLock优于Synchronized 读写锁读写锁。分为读锁和写锁，多个读锁不互斥，读锁与写锁互斥，由Java虚拟机控制。如果代码允许很多线程同时读，但不能同时写，就上读锁；如果代码不允许同时读，并且只能有一个线程在写，就上写锁。 读写锁的接口是ReadWriteLock，具体实现类是 ReentrantReadWriteLock。synchronized属于互斥锁，任何时候只允许一个线程的读写操作，其他线程必须等待；而ReadWriteLock允许多个线程获得读锁，但只允许一个线程获得写锁，效率相对较高一些。 123456789101112131415161718192021使用枚举创建一个读写锁的单例public enum Locker &#123; INSTANCE; private static final ReadWriteLock lock = new ReentrantReadWriteLock(); public Lock writeLock() &#123; return lock.writeLock(); &#125;&#125;再在addCount()方法中对count++;上锁。示例如下。public static void addCount() &#123; // 上锁 Lock writeLock = Locker.INSTANCE.writeLock(); writeLock.lock(); count++; // 释放锁 writeLock.unlock();&#125; 自旋锁，偏向锁，轻量级锁，偏向锁和轻量级锁就是为了避免阻塞，避免操作系统的介入偏向锁：通常只有一个线程在临界区执行轻量级锁：可以用多个线程交替进入临界区，在竞争不激烈的时候，稍微自旋等待一会就能获得锁重量级锁：出现了激烈的竞争，只好阻塞 可重入锁，什么时候应该使用可重入锁公平锁，非公平锁，（先申请先得到）多个线程在等待同一个锁时，必须按照申请锁的时间顺序 来依次获取，而非公平锁则不保证这一点。Synchronized 不是公平锁，reetrantlock 默认下也 是非公平的，但是在构造函数中，可以设置为公平的。 乐观锁，悲观锁什么是乐观锁悲观锁悲观锁：就是很悲观，每次去拿数据的时候都认为别人会修改， 所以每次在拿数据的时候都会上锁。这样别人想拿这个数据就会 block 直到它拿到锁。传统 的关系型数据库就用到了很多这种机制，比如行锁，写锁等，都是在操作之前上锁。2）乐 观锁：就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新 的时候会判断一下在此期间别人有没有去更新这个数据。适用于多读，比如 write_condition. 两种锁各有优缺点，不能认为一种比一种好。 CAS如何解决ABA问题CAS操作需要输入两个数值，一个旧值（原值）和一个新值，在操作期间先比较旧值有没有发生变化，如果没有发生变化，才交换成新值，发生了变化则不交换。如何解决ABA问题，简单来说就是CAS只是比较值是否一样，但有可能值一样，但已经不是原来的东西了，所以通过引入版本号来解决该ABA问题，引入版本号之后，变量的修改操作就变成了1A—2B—3A。 死锁与活锁的区别，死锁与饥饿的区别虽然trylock（）方案避免了无尽的死锁，但是不能避免死锁，这样可能发生活锁现象，活锁就是如果所有死锁线程同时超时，他们极有可能再次陷入死锁，虽然死锁没有永远持续下去，但是对资源的争夺状况却没有改善有一种方法可以减少活锁的几率，比如为每个线程设置不同的超时时间 参考https://www.cnblogs.com/wpf-7/p/9639671.htmlhttp://www.zhenganwen.top/posts/5c6e8cdf/http://zhenganwen.top/posts/ca0f0d75/https://blog.csdn.net/wufaliang003/article/details/78797203]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之iptables]]></title>
    <url>%2F2019%2F03%2F23%2FLinux%E4%B9%8Biptables%2F</url>
    <content type="text"><![CDATA[昨天读了一篇文章说，服务器不能把防火墙关闭。那么我在unbuntu上通过ufw enable开启了防火墙，然后我执行了以下命令，懵逼了，这都是啥啊。 Filter表主要和主机自身有关，真正负责主机防火墙功能（过滤流入，流出，主机的数据包）。filter是主机默认使用的表。这表定义了三个链。生产场景单台，服务器的防火墙功能全靠这张表。 INPUT：负责过滤进入主机的数据包 FORWARD：负责转发流进主机的数据包，起转发的作用，和NAT关系很大。 net.ipv4.ip_forward=0 这个参数很重要，有的场景需要开启。echo 1 &gt; /proc/sys/net/ipv4/ip_forward暂时开始路由转发。 OUTPUT ： 就是处理从主机发出去的数据包 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229root@ubuntu16-desktop:~# iptables -nL //列表形式显示默认表(filter)的所有信息 L list 列表 -n number 数字Chain INPUT (policy DROP)target prot opt source destination ufw-before-logging-input all -- 0.0.0.0/0 0.0.0.0/0 （表示anywhere） ufw-before-input all -- 0.0.0.0/0 0.0.0.0/0 ufw-after-input all -- 0.0.0.0/0 0.0.0.0/0 ufw-after-logging-input all -- 0.0.0.0/0 0.0.0.0/0 ufw-reject-input all -- 0.0.0.0/0 0.0.0.0/0 ufw-track-input all -- 0.0.0.0/0 0.0.0.0/0 //默认不能进行转发，就是从容器里边 到主机，默认是不行的。如果想要实现不同主机上的容器进行通信，需要对此进行修改。但是我做了下实验，开不开起net.ipv4.ip_forward，容器和主机都可以相互ping通啊。Chain FORWARD (policy DROP)target prot opt source destination DOCKER-USER all -- 0.0.0.0/0 0.0.0.0/0 DOCKER-ISOLATION-STAGE-1 all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHEDDOCKER all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHEDDOCKER all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHEDDOCKER all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHEDDOCKER all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ufw-before-logging-forward all -- 0.0.0.0/0 0.0.0.0/0 ufw-before-forward all -- 0.0.0.0/0 0.0.0.0/0 ufw-after-forward all -- 0.0.0.0/0 0.0.0.0/0 ufw-after-logging-forward all -- 0.0.0.0/0 0.0.0.0/0 ufw-reject-forward all -- 0.0.0.0/0 0.0.0.0/0 ufw-track-forward all -- 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT)target prot opt source destination ufw-before-logging-output all -- 0.0.0.0/0 0.0.0.0/0 ufw-before-output all -- 0.0.0.0/0 0.0.0.0/0 ufw-after-output all -- 0.0.0.0/0 0.0.0.0/0 ufw-after-logging-output all -- 0.0.0.0/0 0.0.0.0/0 ufw-reject-output all -- 0.0.0.0/0 0.0.0.0/0 ufw-track-output all -- 0.0.0.0/0 0.0.0.0/0 Chain DOCKER (4 references)target prot opt source destination ACCEPT tcp -- 0.0.0.0/0 172.18.20.217 tcp dpt:9092ACCEPT tcp -- 0.0.0.0/0 172.17.0.2 tcp dpt:9000ACCEPT tcp -- 0.0.0.0/0 172.18.20.217 tcp dpt:3000ACCEPT tcp -- 0.0.0.0/0 172.18.20.216 tcp dpt:9092ACCEPT tcp -- 0.0.0.0/0 172.18.20.215 tcp dpt:9092ACCEPT tcp -- 0.0.0.0/0 172.18.20.213 tcp dpt:2181ACCEPT tcp -- 0.0.0.0/0 172.18.20.218 tcp dpt:9300ACCEPT tcp -- 0.0.0.0/0 172.18.20.218 tcp dpt:9200ACCEPT tcp -- 0.0.0.0/0 172.18.20.214 tcp dpt:2181ACCEPT tcp -- 0.0.0.0/0 172.18.20.212 tcp dpt:2181ACCEPT tcp -- 0.0.0.0/0 172.18.20.1 tcp dpt:8086ACCEPT tcp -- 0.0.0.0/0 172.18.20.3 tcp dpt:3000ACCEPT tcp -- 0.0.0.0/0 172.17.0.6 tcp dpt:3306Chain DOCKER-ISOLATION-STAGE-1 (1 references)target prot opt source destination DOCKER-ISOLATION-STAGE-2 all -- 0.0.0.0/0 0.0.0.0/0 DOCKER-ISOLATION-STAGE-2 all -- 0.0.0.0/0 0.0.0.0/0 DOCKER-ISOLATION-STAGE-2 all -- 0.0.0.0/0 0.0.0.0/0 DOCKER-ISOLATION-STAGE-2 all -- 0.0.0.0/0 0.0.0.0/0 RETURN all -- 0.0.0.0/0 0.0.0.0/0 //默认dropChain DOCKER-ISOLATION-STAGE-2 (4 references)target prot opt source destination DROP all -- 0.0.0.0/0 0.0.0.0/0 DROP all -- 0.0.0.0/0 0.0.0.0/0 DROP all -- 0.0.0.0/0 0.0.0.0/0 DROP all -- 0.0.0.0/0 0.0.0.0/0 RETURN all -- 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-USER (1 references)target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 Chain ufw-after-forward (1 references)target prot opt source destination Chain ufw-after-input (1 references)target prot opt source destination ufw-skip-to-policy-input udp -- 0.0.0.0/0 0.0.0.0/0 udp dpt:137ufw-skip-to-policy-input udp -- 0.0.0.0/0 0.0.0.0/0 udp dpt:138ufw-skip-to-policy-input tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:139ufw-skip-to-policy-input tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:445ufw-skip-to-policy-input udp -- 0.0.0.0/0 0.0.0.0/0 udp dpt:67ufw-skip-to-policy-input udp -- 0.0.0.0/0 0.0.0.0/0 udp dpt:68ufw-skip-to-policy-input all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type BROADCASTChain ufw-after-logging-forward (1 references)target prot opt source destination LOG all -- 0.0.0.0/0 0.0.0.0/0 limit: avg 3/min burst 10 LOG flags 0 level 4 prefix "[UFW BLOCK] "Chain ufw-after-logging-input (1 references)target prot opt source destination LOG all -- 0.0.0.0/0 0.0.0.0/0 limit: avg 3/min burst 10 LOG flags 0 level 4 prefix "[UFW BLOCK] "Chain ufw-after-logging-output (1 references)target prot opt source destination Chain ufw-after-output (1 references)target prot opt source destination Chain ufw-before-forward (1 references)target prot opt source destination ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHEDACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 3ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 4ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 11ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 12ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 8ufw-user-forward all -- 0.0.0.0/0 0.0.0.0/0 Chain ufw-before-input (1 references)target prot opt source destination ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHEDufw-logging-deny all -- 0.0.0.0/0 0.0.0.0/0 ctstate INVALIDDROP all -- 0.0.0.0/0 0.0.0.0/0 ctstate INVALIDACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 3ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 4ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 11ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 12ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 8ACCEPT udp -- 0.0.0.0/0 0.0.0.0/0 udp spt:67 dpt:68ufw-not-local all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT udp -- 0.0.0.0/0 224.0.0.251 udp dpt:5353ACCEPT udp -- 0.0.0.0/0 239.255.255.250 udp dpt:1900ufw-user-input all -- 0.0.0.0/0 0.0.0.0/0 Chain ufw-before-logging-forward (1 references)target prot opt source destination Chain ufw-before-logging-input (1 references)target prot opt source destination Chain ufw-before-logging-output (1 references)target prot opt source destination Chain ufw-before-output (1 references)target prot opt source destination ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHEDufw-user-output all -- 0.0.0.0/0 0.0.0.0/0 Chain ufw-logging-allow (0 references)target prot opt source destination LOG all -- 0.0.0.0/0 0.0.0.0/0 limit: avg 3/min burst 10 LOG flags 0 level 4 prefix "[UFW ALLOW] "Chain ufw-logging-deny (2 references)target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 ctstate INVALID limit: avg 3/min burst 10LOG all -- 0.0.0.0/0 0.0.0.0/0 limit: avg 3/min burst 10 LOG flags 0 level 4 prefix "[UFW BLOCK] "Chain ufw-not-local (1 references)target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCALRETURN all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type MULTICASTRETURN all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type BROADCASTufw-logging-deny all -- 0.0.0.0/0 0.0.0.0/0 limit: avg 3/min burst 10DROP all -- 0.0.0.0/0 0.0.0.0/0 Chain ufw-reject-forward (1 references)target prot opt source destination Chain ufw-reject-input (1 references)target prot opt source destination Chain ufw-reject-output (1 references)target prot opt source destination Chain ufw-skip-to-policy-forward (0 references)target prot opt source destination DROP all -- 0.0.0.0/0 0.0.0.0/0 Chain ufw-skip-to-policy-input (7 references)target prot opt source destination DROP all -- 0.0.0.0/0 0.0.0.0/0 Chain ufw-skip-to-policy-output (0 references)target prot opt source destination ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 Chain ufw-track-forward (1 references)target prot opt source destination Chain ufw-track-input (1 references)target prot opt source destination Chain ufw-track-output (1 references)target prot opt source destination ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 ctstate NEWACCEPT udp -- 0.0.0.0/0 0.0.0.0/0 ctstate NEWChain ufw-user-forward (1 references)target prot opt source destination Chain ufw-user-input (1 references)target prot opt source destination Chain ufw-user-limit (0 references)target prot opt source destination LOG all -- 0.0.0.0/0 0.0.0.0/0 limit: avg 3/min burst 5 LOG flags 0 level 4 prefix "[UFW LIMIT BLOCK] "REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachableChain ufw-user-limit-accept (0 references)target prot opt source destination ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 Chain ufw-user-logging-forward (0 references)target prot opt source destination Chain ufw-user-logging-input (0 references)target prot opt source destination Chain ufw-user-logging-output (0 references)target prot opt source destination Chain ufw-user-output (1 references)target prot opt source destination NAT表12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758root@ubuntu16-desktop:~# iptables -nL -t natChain PREROUTING (policy ACCEPT)target prot opt source destination DOCKER all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCALChain INPUT (policy ACCEPT)target prot opt source destination Chain OUTPUT (policy ACCEPT)target prot opt source destination DOCKER all -- 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL//MASQUERADE就是针对这种场景而设计的，他的作用是，从服务器的网卡上，自动获取当前ip地址来做NAT比如下边的命令：iptables -t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j MASQUERADE如此配置的话，不用指定SNAT的目标ip了不管现在eth0的出口获得了怎样的动态ip，MASQUERADE会自动读取eth0现在的ip地址然后做SNAT出去这样就实现了很好的动态SNAT地址转换Chain POSTROUTING (policy ACCEPT)target prot opt source destination MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 //MASQUERADE为地址伪装，在iptables中有着和SNAT相近的效果 MASQUERADE all -- 172.28.0.0/16 0.0.0.0/0 MASQUERADE all -- 172.18.0.0/16 0.0.0.0/0 MASQUERADE all -- 172.29.0.0/16 0.0.0.0/0 MASQUERADE tcp -- 172.18.20.217 172.18.20.217 tcp dpt:9092MASQUERADE tcp -- 172.17.0.2 172.17.0.2 tcp dpt:9000MASQUERADE tcp -- 172.18.20.217 172.18.20.217 tcp dpt:3000MASQUERADE tcp -- 172.18.20.216 172.18.20.216 tcp dpt:9092MASQUERADE tcp -- 172.18.20.215 172.18.20.215 tcp dpt:9092MASQUERADE tcp -- 172.18.20.213 172.18.20.213 tcp dpt:2181MASQUERADE tcp -- 172.18.20.218 172.18.20.218 tcp dpt:9300MASQUERADE tcp -- 172.18.20.218 172.18.20.218 tcp dpt:9200MASQUERADE tcp -- 172.18.20.214 172.18.20.214 tcp dpt:2181MASQUERADE tcp -- 172.18.20.212 172.18.20.212 tcp dpt:2181MASQUERADE tcp -- 172.18.20.1 172.18.20.1 tcp dpt:8086MASQUERADE tcp -- 172.18.20.3 172.18.20.3 tcp dpt:3000MASQUERADE tcp -- 172.17.0.6 172.17.0.6 tcp dpt:3306Chain DOCKER (2 references)target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 RETURN all -- 0.0.0.0/0 0.0.0.0/0 RETURN all -- 0.0.0.0/0 0.0.0.0/0 RETURN all -- 0.0.0.0/0 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9094 to:172.18.20.217:9092DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9000 to:172.17.0.2:9000DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:3000 to:172.18.20.217:3000DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9093 to:172.18.20.216:9092DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9092 to:172.18.20.215:9092DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:2185 to:172.18.20.213:2181DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9300 to:172.18.20.218:9300DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:9200 to:172.18.20.218:9200DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:2186 to:172.18.20.214:2181DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:2184 to:172.18.20.212:2181DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8086 to:172.18.20.1:8086DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:5000 to:172.18.20.3:3000DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:3306 to:172.17.0.6:3306 ubuntu中查看规则iptables-save 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213# Generated by iptables-save v1.6.0 on Fri Mar 22 03:55:53 2019*nat:PREROUTING ACCEPT [4895:635241]:INPUT ACCEPT [3858:547896]:OUTPUT ACCEPT [2016:126826]:POSTROUTING ACCEPT [2018:126930]:DOCKER - [0:0]//这条规则把目标地址类型属于主机系统的本地网络地址的数据包，在数据包进入NAT表PREROUTING链时，都让它们直接jump到一个名为DOCKER的链。//iptables提供了众多的扩展模块，以支持更多的功能。addrtype就是这样的一个扩展模块，提供的是Address type match的功能。引用的方式就是 -m 模块名。比如还有conntrack模块。iptables -m addrtype --help查看帮助信息。-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER//这条规则是处理docker的SNAT地址伪装用的。具体含义是将容器网络网段发送到外部的数据包(!-o docker0)伪装成宿主机的ip，就是讲数据包的原来的容器ip换成了宿主机ip，做了一次snat。-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE-A POSTROUTING -s 172.28.0.0/16 ! -o br-c333d38dcc9f -j MASQUERADE//我自己建的网桥-A POSTROUTING -s 172.18.0.0/16 ! -o br-fd7621436686 -j MASQUERADE-A POSTROUTING -s 172.29.0.0/16 ! -o br-f8eb065b6c3a -j MASQUERADE-A POSTROUTING -s 172.18.20.217/32 -d 172.18.20.217/32 -p tcp -m tcp --dport 9092 -j MASQUERADE-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 9000 -j MASQUERADE-A POSTROUTING -s 172.18.20.217/32 -d 172.18.20.217/32 -p tcp -m tcp --dport 3000 -j MASQUERADE-A POSTROUTING -s 172.18.20.216/32 -d 172.18.20.216/32 -p tcp -m tcp --dport 9092 -j MASQUERADE-A POSTROUTING -s 172.18.20.215/32 -d 172.18.20.215/32 -p tcp -m tcp --dport 9092 -j MASQUERADE-A POSTROUTING -s 172.18.20.213/32 -d 172.18.20.213/32 -p tcp -m tcp --dport 2181 -j MASQUERADE-A POSTROUTING -s 172.18.20.218/32 -d 172.18.20.218/32 -p tcp -m tcp --dport 9300 -j MASQUERADE-A POSTROUTING -s 172.18.20.218/32 -d 172.18.20.218/32 -p tcp -m tcp --dport 9200 -j MASQUERADE-A POSTROUTING -s 172.18.20.214/32 -d 172.18.20.214/32 -p tcp -m tcp --dport 2181 -j MASQUERADE-A POSTROUTING -s 172.18.20.212/32 -d 172.18.20.212/32 -p tcp -m tcp --dport 2181 -j MASQUERADE-A POSTROUTING -s 172.18.20.1/32 -d 172.18.20.1/32 -p tcp -m tcp --dport 8086 -j MASQUERADE-A POSTROUTING -s 172.18.20.3/32 -d 172.18.20.3/32 -p tcp -m tcp --dport 3000 -j MASQUERADE-A POSTROUTING -s 172.17.0.6/32 -d 172.17.0.6/32 -p tcp -m tcp --dport 3306 -j MASQUERADE//docker启动，分别在filter和nat建立了名为DOCKER的chain，在forward转发链增加了一些ACCEPT规则，在nat增加了postrouting和prerouting以及output的规则。docker创建了一个名为DOKCER的自定义的链条Chain,iptables自定义链条的好处就是可以让防火墙的策略更加的层次化… …-A DOCKER -i docker0 -j RETURN-A DOCKER -i br-c333d38dcc9f -j RETURN-A DOCKER -i br-fd7621436686 -j RETURN-A DOCKER -i br-f8eb065b6c3a -j RETURN//如下命令表示把所有10.8.0.0网段的数据包SNAT成192.168.5.3的ip然后发出去(这里没有这个规则)//iptables -t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT –to-source 192.168.5.3-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 9094 -j DNAT --to-destination 172.18.20.217:9092-A DOCKER ! -i docker0 -p tcp -m tcp --dport 9000 -j DNAT --to-destination 172.17.0.2:9000-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 3000 -j DNAT --to-destination 172.18.20.217:3000-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 9093 -j DNAT --to-destination 172.18.20.216:9092-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 9092 -j DNAT --to-destination 172.18.20.215:9092-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 2185 -j DNAT --to-destination 172.18.20.213:2181-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 9300 -j DNAT --to-destination 172.18.20.218:9300-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 9200 -j DNAT --to-destination 172.18.20.218:9200-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 2186 -j DNAT --to-destination 172.18.20.214:2181-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 2184 -j DNAT --to-destination 172.18.20.212:2181-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 8086 -j DNAT --to-destination 172.18.20.1:8086-A DOCKER ! -i br-fd7621436686 -p tcp -m tcp --dport 5000 -j DNAT --to-destination 172.18.20.3:3000-A DOCKER ! -i docker0 -p tcp -m tcp --dport 3306 -j DNAT --to-destination 172.17.0.6:3306COMMIT# Completed on Fri Mar 22 03:55:53 2019# Generated by iptables-save v1.6.0 on Fri Mar 22 03:55:53 2019*filter:INPUT DROP [31:868]:FORWARD DROP [0:0]:OUTPUT ACCEPT [0:0]:DOCKER - [0:0]:DOCKER-ISOLATION-STAGE-1 - [0:0]:DOCKER-ISOLATION-STAGE-2 - [0:0]:DOCKER-USER - [0:0]:ufw-after-forward - [0:0]:ufw-after-input - [0:0]:ufw-after-logging-forward - [0:0]:ufw-after-logging-input - [0:0]:ufw-after-logging-output - [0:0]:ufw-after-output - [0:0]:ufw-before-forward - [0:0]:ufw-before-input - [0:0]:ufw-before-logging-forward - [0:0]:ufw-before-logging-input - [0:0]:ufw-before-logging-output - [0:0]:ufw-before-output - [0:0]:ufw-logging-allow - [0:0]:ufw-logging-deny - [0:0]:ufw-not-local - [0:0]:ufw-reject-forward - [0:0]:ufw-reject-input - [0:0]:ufw-reject-output - [0:0]:ufw-skip-to-policy-forward - [0:0]:ufw-skip-to-policy-input - [0:0]:ufw-skip-to-policy-output - [0:0]:ufw-track-forward - [0:0]:ufw-track-input - [0:0]:ufw-track-output - [0:0]:ufw-user-forward - [0:0]:ufw-user-input - [0:0]:ufw-user-limit - [0:0]:ufw-user-limit-accept - [0:0]:ufw-user-logging-forward - [0:0]:ufw-user-logging-input - [0:0]:ufw-user-logging-output - [0:0]:ufw-user-output - [0:0]-A INPUT -j ufw-before-logging-input-A INPUT -j ufw-before-input-A INPUT -j ufw-after-input-A INPUT -j ufw-after-logging-input-A INPUT -j ufw-reject-input-A INPUT -j ufw-track-input//filter表中的forward主要是对容器和宿主机以及本机容器之间数据包的放行。-A FORWARD -j DOCKER-USER-A FORWARD -j DOCKER-ISOLATION-STAGE-1-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -o docker0 -j DOCKER-A FORWARD -i docker0 ! -o docker0 -j ACCEPT-A FORWARD -i docker0 -o docker0 -j ACCEPT-A FORWARD -o br-c333d38dcc9f -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -o br-c333d38dcc9f -j DOCKER-A FORWARD -i br-c333d38dcc9f ! -o br-c333d38dcc9f -j ACCEPT-A FORWARD -i br-c333d38dcc9f -o br-c333d38dcc9f -j ACCEPT-A FORWARD -o br-fd7621436686 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -o br-fd7621436686 -j DOCKER-A FORWARD -i br-fd7621436686 ! -o br-fd7621436686 -j ACCEPT-A FORWARD -i br-fd7621436686 -o br-fd7621436686 -j ACCEPT-A FORWARD -o br-f8eb065b6c3a -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -o br-f8eb065b6c3a -j DOCKER-A FORWARD -i br-f8eb065b6c3a ! -o br-f8eb065b6c3a -j ACCEPT-A FORWARD -i br-f8eb065b6c3a -o br-f8eb065b6c3a -j ACCEPT-A FORWARD -j ufw-before-logging-forward-A FORWARD -j ufw-before-forward-A FORWARD -j ufw-after-forward-A FORWARD -j ufw-after-logging-forward-A FORWARD -j ufw-reject-forward-A FORWARD -j ufw-track-forward-A OUTPUT -j ufw-before-logging-output-A OUTPUT -j ufw-before-output-A OUTPUT -j ufw-after-output-A OUTPUT -j ufw-after-logging-output-A OUTPUT -j ufw-reject-output-A OUTPUT -j ufw-track-output-A DOCKER -d 172.18.20.217/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 9092 -j ACCEPT-A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 9000 -j ACCEPT-A DOCKER -d 172.18.20.217/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 3000 -j ACCEPT-A DOCKER -d 172.18.20.216/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 9092 -j ACCEPT-A DOCKER -d 172.18.20.215/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 9092 -j ACCEPT-A DOCKER -d 172.18.20.213/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 2181 -j ACCEPT-A DOCKER -d 172.18.20.218/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 9300 -j ACCEPT-A DOCKER -d 172.18.20.218/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 9200 -j ACCEPT-A DOCKER -d 172.18.20.214/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 2181 -j ACCEPT-A DOCKER -d 172.18.20.212/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 2181 -j ACCEPT-A DOCKER -d 172.18.20.1/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 8086 -j ACCEPT-A DOCKER -d 172.18.20.3/32 ! -i br-fd7621436686 -o br-fd7621436686 -p tcp -m tcp --dport 3000 -j ACCEPT-A DOCKER -d 172.17.0.6/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 3306 -j ACCEPT-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -i br-c333d38dcc9f ! -o br-c333d38dcc9f -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -i br-fd7621436686 ! -o br-fd7621436686 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -i br-f8eb065b6c3a ! -o br-f8eb065b6c3a -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -j RETURN-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP-A DOCKER-ISOLATION-STAGE-2 -o br-c333d38dcc9f -j DROP-A DOCKER-ISOLATION-STAGE-2 -o br-fd7621436686 -j DROP-A DOCKER-ISOLATION-STAGE-2 -o br-f8eb065b6c3a -j DROP-A DOCKER-ISOLATION-STAGE-2 -j RETURN-A DOCKER-USER -j RETURN-A ufw-after-input -p udp -m udp --dport 137 -j ufw-skip-to-policy-input-A ufw-after-input -p udp -m udp --dport 138 -j ufw-skip-to-policy-input-A ufw-after-input -p tcp -m tcp --dport 139 -j ufw-skip-to-policy-input-A ufw-after-input -p tcp -m tcp --dport 445 -j ufw-skip-to-policy-input-A ufw-after-input -p udp -m udp --dport 67 -j ufw-skip-to-policy-input-A ufw-after-input -p udp -m udp --dport 68 -j ufw-skip-to-policy-input-A ufw-after-input -m addrtype --dst-type BROADCAST -j ufw-skip-to-policy-input-A ufw-after-logging-forward -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix "[UFW BLOCK] "-A ufw-after-logging-input -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix "[UFW BLOCK] "-A ufw-before-forward -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A ufw-before-forward -p icmp -m icmp --icmp-type 3 -j ACCEPT-A ufw-before-forward -p icmp -m icmp --icmp-type 4 -j ACCEPT-A ufw-before-forward -p icmp -m icmp --icmp-type 11 -j ACCEPT-A ufw-before-forward -p icmp -m icmp --icmp-type 12 -j ACCEPT-A ufw-before-forward -p icmp -m icmp --icmp-type 8 -j ACCEPT-A ufw-before-forward -j ufw-user-forward-A ufw-before-input -i lo -j ACCEPT-A ufw-before-input -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A ufw-before-input -m conntrack --ctstate INVALID -j ufw-logging-deny-A ufw-before-input -m conntrack --ctstate INVALID -j DROP-A ufw-before-input -p icmp -m icmp --icmp-type 3 -j ACCEPT-A ufw-before-input -p icmp -m icmp --icmp-type 4 -j ACCEPT-A ufw-before-input -p icmp -m icmp --icmp-type 11 -j ACCEPT-A ufw-before-input -p icmp -m icmp --icmp-type 12 -j ACCEPT-A ufw-before-input -p icmp -m icmp --icmp-type 8 -j ACCEPT-A ufw-before-input -p udp -m udp --sport 67 --dport 68 -j ACCEPT-A ufw-before-input -j ufw-not-local-A ufw-before-input -d 224.0.0.251/32 -p udp -m udp --dport 5353 -j ACCEPT-A ufw-before-input -d 239.255.255.250/32 -p udp -m udp --dport 1900 -j ACCEPT-A ufw-before-input -j ufw-user-input-A ufw-before-output -o lo -j ACCEPT-A ufw-before-output -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A ufw-before-output -j ufw-user-output-A ufw-logging-allow -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix "[UFW ALLOW] "-A ufw-logging-deny -m conntrack --ctstate INVALID -m limit --limit 3/min --limit-burst 10 -j RETURN-A ufw-logging-deny -m limit --limit 3/min --limit-burst 10 -j LOG --log-prefix "[UFW BLOCK] "-A ufw-not-local -m addrtype --dst-type LOCAL -j RETURN-A ufw-not-local -m addrtype --dst-type MULTICAST -j RETURN-A ufw-not-local -m addrtype --dst-type BROADCAST -j RETURN-A ufw-not-local -m limit --limit 3/min --limit-burst 10 -j ufw-logging-deny-A ufw-not-local -j DROP-A ufw-skip-to-policy-forward -j DROP-A ufw-skip-to-policy-input -j DROP-A ufw-skip-to-policy-output -j ACCEPT-A ufw-track-output -p tcp -m conntrack --ctstate NEW -j ACCEPT-A ufw-track-output -p udp -m conntrack --ctstate NEW -j ACCEPT-A ufw-user-limit -m limit --limit 3/min -j LOG --log-prefix "[UFW LIMIT BLOCK] "-A ufw-user-limit -j REJECT --reject-with icmp-port-unreachable-A ufw-user-limit-accept -j ACCEPTCOMMIT# Completed on Fri Mar 22 03:55:53 2019 SNAT与DNATSNAT，DNAT，MASQUERADE都是NAT，MASQUERADE是SNAT的一个特例。SNAT是指在数据包从网卡发送出去的时候，把数据包中的源地址部分替换为指定的IP，这样，接收方就认为数据包的来源是被替换的那个IP的主机。 MASQUERADE是用发送数据的网卡上的IP来替换源IP，因此，对于那些IP不固定的场合，比如拨号网络或者通过dhcp分配IP的情况下，就得用MASQUERADE。 DNAT，就是指数据包从网卡发送出去的时候，修改数据包中的目的IP，表现为如果你想访问A，可是因为网关做了DNAT，把所有访问A的数据包的目的IP全部修改为B，那么，你实际上访问的是B。 centos中的防火墙CentOS 6使用iptables作为防火墙，CentOS 7使用firewalld作为防火墙。 CentOS 7关闭防火墙运行：systemctl stop firewalld.service CentOS 7禁止防火墙开机自动运行：systemctl disable firewalld.service 临时允许443/TCP端口,立即生效firewall-cmd –add-port=443/tcp 永久允许443/TCP端口,重启防火墙后生效firewall-cmd –permanent –add-port=443/tcp 永久打开端口需要reload一下，如果用了reload临时打开的端口就失效了firewall-cmd –reload 查看防火墙所有区域的设置，包括添加的端口和服务firewall-cmd –list-all firewalld在rule上配置非常不友好，k8s不支持firewalld，还是需要iptables来处理 在服务端禁止掉,客户端ssh的访问1234567891011121314151617iptables -t filter -A INPUT -p tcp --dport 22 -j DORP 也可以不指定表 默认是filter 表iptables -t 指定表Iptables -A 添加规则到指定链的结尾，最后一条 append 添加iptables -I 是添加规则到指定链的开头，第一条 insert 插入 插到第一行 首先执行iptables -p 指定协议 默认 不写 指的所有端口 iptables -J 指定动作 ACCEPT 接受 DROP 丢弃 REJECT 拒绝（拒绝会透漏信息一般不用） iptables --line-numbers 显示序号禁掉端口号 之后，跑到机房 VMworkstations关闭防火墙 -F清除规则 或者重启 即可iptables -t filter -A INPUT -p tcp --deport 52113 -j DROP 允许80端口iptables -A INPUT -p tcp –dport 80 -j ACCEPT 允许其他人能ping通iptables -A INPUT -p icmp -m icmp –icmp-type any -j ACCEPT 允许关联的包通过iptables -A INPUT -m state –state ESTABLISHED,RELATED -j ACCEPTiptables -A OUTPUT -m state –state ESTABLISHED,RELATED -j ACCEPT 如何永久保存我们在命令行中的操作，都是储存在内存当中，没有在磁盘，重启服务器就会丢失 我们要将其存到配置文件，做到永久存储 方法一：/etc/init.d/iptables save 方法二:重定向iptables-save&gt;/etc/sysconfig/iptables.bak 如何查看 所有端口nmap host的IP -p 1-65535 ubuntu相关命令 允许sshufw allow 22 删除已经添加过的规则：sudo ufw delete allow 22只打开使用tcp/ip协议的22端口： udo ufw allow 22/tcp打开来自192.168.0.1的tcp请求的80端口： sudo ufw allow proto tcp from 192.168.0.1 to any port 22允许某特定 IP sudo ufw allow from 192.168.254.254]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统之MMAP]]></title>
    <url>%2F2019%2F03%2F23%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8BMMAP%2F</url>
    <content type="text"><![CDATA[MMAP是什么mmap是一种内存映射文件的方法，即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用read,write等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。如下图所示： 由上图可以看出，进程的虚拟地址空间，由多个虚拟内存区域构成。虚拟内存区域是进程的虚拟地址空间中的一个同质区间，即具有同样特性的连续地址范围。上图中所示的text数据段（代码段）、初始数据段、BSS数据段、堆、栈和内存映射，都是一个独立的虚拟内存区域。而为内存映射服务的地址空间处在堆栈之间的空余部分。 mmap优点 对文件的读取操作跨过了页缓存，减少了数据的拷贝次数，用内存读写取代I/O读写，提高了文件读取效率。 实现了用户空间和内核空间的高效交互方式。两空间的各自修改操作可以直接反映在映射的区域内，从而被对方空间及时捕捉。 提供进程间共享内存及相互通信的方式。不管是父子进程还是无亲缘关系的进程，都可以将自身用户空间映射到同一个文件或匿名映射到同一片区域。从而通过各自对映射区域的改动，达到进程间通信和进程间共享的目的。同时，如果进程A和进程B都映射了区域C，当A第一次读取C时通过缺页从磁盘复制文件页到内存中；但当B再读C的相同页面时，虽然也会产生缺页异常，但是不再需要从磁盘中复制文件过来，而可直接使用已经保存在内存中的文件数据。 可用于实现高效的大规模数据传输。内存空间不足，是制约大数据操作的一个方面，解决方案往往是借助硬盘空间协助操作，补充内存的不足。但是进一步会造成大量的文件I/O操作，极大影响效率。这个问题可以通过mmap映射很好的解决。换句话说，但凡是需要用磁盘空间代替内存的时候，mmap都可以发挥其功效 MMAP和常规文件操作的不同回顾一下常规文件系统操作（调用read/fread等类函数）中，函数的调用过程： 进程发起读文件请求。 内核通过查找进程文件符表，定位到内核已打开文件集上的文件信息，从而找到此文件的inode。 inode在address_space上查找要请求的文件页是否已经缓存在页缓存中。如果存在，则直接返回这片文件页的内容。 如果不存在，则通过inode定位到文件磁盘地址，将数据从磁盘复制到页缓存。之后再次发起读页面过程，进而将页缓存中的数据发给用户进程。 总结来说，常规文件操作为了提高读写效率和保护磁盘，使用了页缓存机制。这样造成读文件时需要先将文件页从磁盘拷贝到页缓存中，由于页缓存处在内核空间，不能被用户进程直接寻址，所以还需要将页缓存中数据页再次拷贝到内存对应的用户空间中。这样，通过了两次数据拷贝过程，才能完成进程对文件内容的获取任务。写操作也是一样，待写入的buffer在内核空间不能直接访问，必须要先拷贝至内核空间对应的主存，再写回磁盘中（延迟写回），也是需要两次数据拷贝。而使用mmap操作文件中，创建新的虚拟内存区域和建立文件磁盘地址和虚拟内存区域映射这两步，没有任何文件拷贝操作。而之后访问数据时发现内存中并无数据而发起的缺页异常过程，可以通过已经建立好的映射关系，只使用一次数据拷贝，就从磁盘中将数据传入内存的用户空间中，供进程使用。 总而言之，常规文件操作需要从磁盘到页缓存再到用户主存的两次数据拷贝。而mmap操控文件，只需要从磁盘到用户主存的一次数据拷贝过程。说白了，mmap的关键点是实现了用户空间和内核空间的数据直接交互而省去了空间不同数据不通的繁琐过程。因此mmap效率更高。 注意的地方： mmap之所以快，是因为建立了页到用户进程的虚地址空间映射，以读取文件为例，避免了页从内核态拷贝到用户态。 mmap映射的页和其它的页并没有本质的不同.所以得益于主要的3种数据结构的高效，其页映射过程也很高效：(1) radix tree，用于查找某页是否已在缓存.(2) red black tree ，用于查找和更新vma结构.(3) 双向链表，用于维护active和inactive链表，支持LRU类算法进行内存回收. mmap不是银弹.(1) 对变长文件不适合.(2) 如果更新文件的操作很多，mmap避免两态拷贝的优势就被摊还，最终还是落在了大量的脏页回写及由此引发的随机IO上. 所以在随机写很多的情况下，mmap方式在效率上不一定会比带缓冲区的一般写快. 之前接触的mongodb版本无法控制对内存的使用，所以当数据比较大时，其内存使用难以控制，一切的内存 -&gt; 页 -&gt; swap 步骤都交给了操作系统，难以进行有效调优.频繁的页swap会让mongodb的优势荡然无从.]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>OS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之为何不能停止容器]]></title>
    <url>%2F2019%2F03%2F22%2FDocker%E4%B9%8B%E4%B8%BA%E4%BD%95%E4%B8%8D%E8%83%BD%E5%81%9C%E6%AD%A2%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[背景我做了个实验，在ubuntu服务器上加载了os_sec模块后，用docker run hello-world后一直卡住，关闭xshell后，docker ps后发现，此容器还在。那么当我想用docker stop命令停止此容器时，会给我正常显示，但是就是发现stop 不掉。 最后用docker rm -f 容器 可以强制删掉。但是通过查看进程它还在，无语了。是不是因为hello-world容器没有做退出容器的信号处理，所以你用stop也关不掉。 我又试图起redis容器，发现不行，抱以下错误，原因是因为我用了os_sec模块。或者一直处于created状态。1/usr/local/bin/docker-entrypoint.sh: 12: /usr/local/bin/docker-entrypoint.sh: find: Operation not permitted Docker主要执行流程Docker Stop主要流程 Docker 通过containerd向容器主进程发送SIGTERM信号后等待一段时间后，如果从containerd收到了容器退出消息那么容器退出成功。 在上一步中，如果等待超时，那么Docker将使用Docker kill 方式试图终止容器Docker Kill主要流程 Docker引擎通过containerd使用SIGKILL发向容器主进程，等待一段时间后，如果从containerd收到容器退出消息，那么容器Kill成功 在上一步中如果等待超时，Docker引擎将跳过Containerd自己亲自动手通过kill系统调用向容器主进程发送SIGKILL信号。如果此时kill系统调用返回主进程不存在，那么Docker kill成功。否则引擎将一直死等到containerd通过引擎，容器退出。Docker stop中存在的问题在上文中我们看到Docker stop首先间接向容器主进程发送sigterm信号试图通知容器主进程优雅退出。但是容器主进程如果没有显示处理sigterm信号的话，那么容器主进程对此过程会不会有任何反应，此信号被忽略了。这里和常规认识不同，在常规想法中任何进程的默认sigterm处理应该是退出。但是namespace中pid==1的进程，sigterm默认动作是忽略。也即是容器首进程如果不处理sigterm，那么此信号默认会被忽略，这就是很多时候Docker Stop不能立即优雅关闭容器的原因——因为容器主进程根本没有处理SIGTERM。特别指出linux上全局范围内pid=1的进程，不能被sigterm、sigkill、sigint终止进程组首进程退出后，子进程收到sighub Docker kill为何会阻塞容器主/子进程处于D状态进程D状态表示进程处于不可中断睡眠状态，一般都是在等待IO资源。当然有些时候如果系统IO出现问题，那么将有大量的进程处于D状态。在这种状态，信号是无法将进程唤醒；只有等待进程自己从D状态中返回。而且在常规内核中，如果某个进程一直处于D状态，那么理论上除了重启系统那么没有什么方法或手段将它从D中接回。从上面解释Docker kill第二步中可以看到一旦容器中主进程或者子进程处于D状态，那么Docker将等待，一直等到所有容器主进程和其子进程都退出后才返回，那么此时Docker kill就卡住了。 问题解释当出现问题时刻，宿主机上发现大量的stress进程（实际是容器的进程）处于D状态，而系统响应变慢。问题可以这样解释： Docker kill通过containerd间接向容器主进程发送SIGKill信号以后，由于系统响应慢，容器内部子进程（stress）处于D状态，那么在超时时间内containerd没有上报容器退出。Docker kill走到了直接发送Sigkill阶段 在此阶段前，容器内部主进程退出了，所以系统调用kill 发送SIGKILL很快就返回进程不存在了。引擎认为自己把容器杀死了，Docker kill成功返回了。 在一定时间后容器子进程从D状态中恢复，它们退出了，containerd上报容器退出，引擎清理资源，此时Docker ps看到容器才是退出状态 怎么在docker容器中捕获信号比如我们可以向容器的应用发送一个重新加载信号，容器中的应用程序在收到信号后执行相应的处理程序完成重新加载配置文件的任务。注意，只有容器中的1号进程能够收到信号 比如在dockerfile中写了ENTRYPOINT [“java”,”-jar”,]这种写法会让应用程序在容器中以1号进程的身份运行，这样执行的应用程序在容器中是1号进程，给容器发送SIGTERM信号可以关闭容器。比如通过docker container kill –signal=”SIGTERM” XXXX 这个命令。 如果应用程序不是容器中的1号进程，比如ENTRYPOINT[“./XXX.sh”],这样通过给容器发送SIGTERM信号，已经无法退出程序了。因为此场景，应用程序是由bash脚本启动，bash作为容器中的1号进程收到了SIGTERM信号，但是它没有做出任何的相应动作。可以通过docker stop来停止（发送的是SIGKILL信号）。 在脚本中捕获信号创建另外一个启动应用程序的脚本文件 XXXX.sh，内容如下：12345678910111213141516171819202122232425262728293031#!/usr/bin/env bashset -xpid=0 # SIGUSR1-handlermy_handler() &#123; echo "my_handler"&#125; # SIGTERM-handlerterm_handler() &#123; if [ $pid -ne 0 ]; then kill -SIGTERM "$pid" wait "$pid" fi exit 143; # 128 + 15 -- SIGTERM&#125;# setup handlers# on callback, kill the last background process, which is `tail -f /dev/null` and execute the specified handlertrap 'kill $&#123;!&#125;; my_handler' SIGUSR1trap 'kill $&#123;!&#125;; term_handler' SIGTERM # run applicationnode app &amp;pid="$!" # wait foreverwhile truedo tail -f /dev/null &amp; wait $&#123;!&#125;done 这个脚本文件在启动应用程序的同时可以捕获发送给它的 SIGTERM 和 SIGUSR1 信号，并为它们添加了处理程序。其中 SIGTERM 信号的处理程序就是向我们的 node 应用程序发送 SIGTERM 信号。 容器中的 1 号进程是非常重要的，如果它不能正确的处理相关的信号，那么应用程序退出的方式几乎总是被强制杀死而不是优雅的退出。究竟谁是 1 号进程则主要由 EntryPoint, CMD, RUN 等指令的写法决定，所以这些指令的使用是很有讲究的。 总结容器主进程最好需要自己处理SIGTERM信号，因为这是你优雅退出的机会。如果你不处理，那么在Docker stop里你会收到Kill，你未保存的数据就会直接丢失掉。Docker stop和Docker kill返回并不意味着容器真正退出成功了，必须通过docker ps查看。对于通过restful与docker 引擎链接的客户端，需要在docker stop和kill restful请求链接上加上超时。对于docker cli用户，需要有另外的机制监控Docker stop或Docker kill命令超时卡死处于D状态一致卡死的进程，内核无法杀死，docker系统也救不了它。只有重启系统才能清除。 参考https://www.jianshu.com/p/813d8362d497]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下的一些小工具]]></title>
    <url>%2F2019%2F03%2F17%2FLinux%E4%B8%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[扫描端口占用情况123456789101112131415161718192021222324252627282930313233343536373839#!/usr/bin/env python# -*- coding:utf-8 -*-import socket, time, threadsocket.setdefaulttimeout(3) #设置默认超时时间def socket_port(ip, port): """ 输入IP和端口号，扫描判断端口是否占用 """ try: if port &gt;=65535: print u'端口扫描结束' s=socket.socket(socket.AF_INET, socket.SOCK_STREAM) result=s.connect_ex((ip, port)) if result==0: lock.acquire() print ip,u':',port,u'端口已占用' lock.release() except: print u'端口扫描异常'def ip_scan(ip): """ 输入IP，扫描IP的0-65534端口情况 """ try: print u'开始扫描 %s' % ip start_time=time.time() for i in range(0,65534): thread.start_new_thread(socket_port,(ip, int(i))) print u'扫描端口完成，总共用时：%.2f' %(time.time()-start_time)# raw_input("Press Enter to Exit") except: print u'扫描ip出错'if __name__=='__main__': url=raw_input('Input the ip you want to scan: ') lock=thread.allocate_lock() ip_scan(url) 判断哪些进程占用的资源多1234#!/bin/bashfor proc in $(find /proc -maxdepth 1 -regex '/proc/[0-9]+'); do printf "%2d %5d %s\n" "$(cat $proc/oom_score)" "$(basename $proc)" "$(cat $proc/cmdline | tr '\0' ' ' | head -c 50)"done 2&gt;/dev/null | sort -nr | head -n 40 输出结果如下123456789101112131415161718192021root@ubuntu16-desktop:~# sh process_order.sh 53 20647 /opt/jdk-10.0.2/bin/java -Xms1g -Xmx1g -XX:+UseCon46 20593 /opt/jdk-10.0.2/bin/java -Xms1g -Xmx1g -XX:+UseCon46 20455 /opt/jdk-10.0.2/bin/java -Xms1g -Xmx1g -XX:+UseCon37 10601 /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx1G 37 10582 /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx1G 37 10497 /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx1G 32 13841 java -jar server-v1.0.jar 31 6022 /usr/java/jdk1.8.0_171/bin/java -Xms1000m -Xmx100019 19807 java -jar -Dloader.path=.,config,resources,3rd-lib 9 1362 nm-applet 8 3022 java -Duser.dir=/kafka-manager-1.3.1.8 -Dconfig.fi 6 746 /usr/sbin/NetworkManager --no-daemon 4 4511 /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Dzooke 4 3024 /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Dzooke 3 3381 /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Dzooke 2 21999 /opt/java/bin/java -Xmx20m -Dflume.root.logger=INF 2 21553 /opt/java/bin/java -Xmx20m -Dflume.root.logger=INF 1 946 /usr/bin/containerd 1 23736 grunt 1 1310 /usr/sbin/unity-greeter 隐藏命令的操作通过export HISTCONTROL=ignorespace这样只要先输入空格 后边的命令就不会被记录 禁用当前会话的所有历史记录如果你想禁用某个会话所有历史，你可以在开始命令行工作前简单地清除环境变量 HISTSIZE 的值即可。执行下面的命令来清除其值export HISTSIZE=0HISTSIZE 表示对于 bash 会话其历史列表中可以保存命令的个数（行数）。默认情况，它设置了一个非零值，例如在我的电脑上，它的值为 1000。所以上面所提到的命令将其值设置为 0，结果就是直到你关闭终端，没有东西会存储在历史记录中。记住同样你也不能通过按向上的箭头按键或运行 history 命令来看到之前执行的命令。https://www.cnblogs.com/rusking/p/5715715.html Linux性能分析的网站http://www.brendangregg.com/linuxperf.html Linux中如何检测IP地址冲突问题arping命令是用于发送arp请求到一个相邻主机的工具，arping使用arp数据包，通过ping命令检查设备上的硬件地址。能够测试一个ip地址是否是在网络上已经被使用，并能够获取更多设备信息。功能类似于ping。 arping命令是用于发送ARP请求到一个相邻主机的工具，通过ARP响应报文检查设备上的硬件地址。它能够测试一个IP地址是否是在网络上已经使用，并能够获取更多设备信息。该功能类似于ping命令。arping命令选项： 1234567-b：用于发送以太网广播帧（FFFFFFFFFFFF）。arping一开始使用广播地址，在收到响应后就使用unicast地址。-q：quiet output不显示任何信息；-f：表示在收到第一个响应报文后就退出；-timeout：设定一个超时时间，单位是秒。如果到了指定时间，arping还没到完全收到响应则退出；-c count：表示发送指定数量的ARP请求数据包后就停止。如果指定了deadline选项，则arping会等待相同数量的arp响应包，直到超时为止；-s source：设定arping发送的arp数据包中的SPA字段的值。如果为空，则按下面处理，如果是DAD模式（冲突地址探测），则设置为0.0.0.0，如果是Unsolicited ARP模式（Gratutious ARP）则设置为目标地址，否则从路由表得出；-I interface：设置ping使用的网络接口。 IP地址冲突检测在出问题的主机上，可以使用”arping -I ethN x.x.x.x”命令（其中x.x.x.x为本接口的IP地址）检测地址冲突，如果没有任何输出，则表示本IP地址无冲突。如果有冲突的话，该命令会显示冲突的IP地址使用的MAC地址。 http://man.linuxde.net/arping 内存分页监控（sar -B）例如，每10秒采样一次，连续采样3次，监控内存分页：sar -B 10 3 输出项说明：123456789pgpgin/s：表示每秒从磁盘或SWAP置换到内存的字节数(KB)pgpgout/s：表示每秒从内存置换到磁盘或SWAP的字节数(KB)fault/s：每秒钟系统产生的缺页数,即主缺页与次缺页之和(major + minor)majflt/s：每秒钟产生的主缺页数.pgfree/s：每秒被放入空闲队列中的页个数pgscank/s：每秒被kswapd扫描的页个数pgscand/s：每秒直接被扫描的页个数pgsteal/s：每秒钟从cache中被清除来满足内存需要的页个数%vmeff：每秒清除的页(pgsteal)占总扫描页(pgscank+pgscand)的百分比 SS命令ss 是 socket statistics 的缩写。顾名思义，ss 命令可以用来获取socket 统计信息，它可以显示和netstat 类似的内容。但 ss 的优势在于它能够显示更多更详细的有关TCP和连接状态的信息，而且比netstat更快速更高效。 当服务器的socket连接数量变得非常大时，无论是使用netstat命令还是 cat /proc/net/tcp，执行速度都会很慢。可能你不会有切身的感受，但请相信我，当服务器维持的连接达到上万个的时候，使用 netstat 等于浪费生命，而用 ss才是 节省时间。 天下武功唯快不破。ss快的秘诀在于，他利用了TCP协议栈中 tcp_diag. tcp_diag 是一个用于分析统计的模块，可以获得Linux 内核中第一手的信息，这就确保了ss的快捷高效。当然，如果你的系统中没有 tcp_diag,ss也可以正常运行，只是效率会变得稍慢。（但仍然比 netstat 要快。） 查找某一个时间点以后创建或者修改的文件首先创建一个对比的时间点的文件touch -t 201407201710.00 abc //创建一个文件，他的mtime是2014-07-20-17:10:00要查找2014-07-20-17:10:00 这个时间点以后创建或者修改的文件，使用命令#find -newer abc //跟刚才创建的文件对比 rz和sz 传输文件rz是上传（从本地上传到服务器），可以使用rz -y实现覆盖上传sz是下载。 实现下载可以直接使用szfilename，其中filename就是你想要下载的文件的名字，如果是目录需要打包成单个文件在实现下载。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络之TIME_WAIT与C_WAIT]]></title>
    <url>%2F2019%2F03%2F17%2F%E7%BD%91%E7%BB%9C%E4%B9%8BTIME-WAIT%E4%B8%8EC-WAIT%2F</url>
    <content type="text"><![CDATA[如何查看TIME_WAIT与CLOSE_WAIT情况通过命令查看netstat -n | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39; 发现这台服务器上 1234[root@data9 ~]# netstat -n | awk &apos;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&apos;CLOSE_WAIT 22 CLOSE_WAIT 表示被动关闭，出现这个说明在对方关闭连接之后，没有进行相应处理。ESTABLISHED 55 ESTABLISHED 表示正在通信TIME_WAIT 3 TIME_WAIT 表示主动关闭 TIME_WAIT(主动关闭)为什么TIME_WAIT状态还需要等2MSL后才能返回到CLOSED状态？这是因为：虽然双方都同意关闭连接了，而且握手的4个报文也都协调和发送完毕，按理可以直接回到CLOSED状态（就好比从SYN_SEND状态到ESTABLISH状态那样）；但是因为我们必须要假想网络是不可靠的，你无法保证你最后发送的ACK报文会一定被对方收到，因此对方处于LAST_ACK状态下的SOCKET可能会因为超时未收到ACK报文，而重发FIN报文，所以这个TIME_WAIT状态的作用就是用来重发可能丢失的ACK报文，并保证于此。 为什么会有Time_wait状态 可靠的终止TCP连接，若处于time_wait的客户端发送给服务器确认报文段丢失的话，服务器将在此重新发送FIN报文段，那么客户端必须处于一个可接收的状态就是time_wait而不是close状态。 保证迟来的TCP报文段有足够的时间被识别并丢弃，linux 中一个TCP端口不能打开两次或两次以上，当客户端处于time_wait状态时我们将无法使用此端口建立新连接，如果不存在time_wait状态，新连接可能会收到旧连接的数据。Time_wait持续的时间是2MSL，保证旧的数据可以丢弃，因为网络中的数据最大存在MSL( aximum segment lifetime) 如何解决解决思路很简单，就是让服务器能够快速回收和重用那些TIME_WAIT的资源。 /etc/sysctl.conf文件的修改1234567891011121314151617181920212223242526272829303132333435#对于一个新建连接，内核要发送多少个 SYN 连接请求才决定放弃,不应该大于255，默认值是5，对应于180秒左右时间 net.ipv4.tcp_syn_retries=2#net.ipv4.tcp_synack_retries=2#表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为300秒net.ipv4.tcp_keepalive_time=1200net.ipv4.tcp_orphan_retries=3#表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间net.ipv4.tcp_fin_timeout=30 #表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。net.ipv4.tcp_max_syn_backlog = 4096#表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭net.ipv4.tcp_syncookies = 1#表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭net.ipv4.tcp_tw_reuse = 1#表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭net.ipv4.tcp_tw_recycle = 1##减少超时前的探测次数 net.ipv4.tcp_keepalive_probes=5 ##优化网络设备接收队列 net.core.netdev_max_backlog=3000 修改完之后执行/sbin/sysctl -p让参数生效。这里头主要注意到的是net.ipv4.tcp_tw_reusenet.ipv4.tcp_tw_recycle net.ipv4.tcp_fin_timeout net.ipv4.tcp_keepalive_*这几个参数。net.ipv4.tcp_tw_reuse和net.ipv4.tcp_tw_recycle的开启都是为了回收处于TIME_WAIT状态的资源。net.ipv4.tcp_fin_timeout这个时间可以减少在异常情况下服务器从FIN-WAIT-2转到TIME_WAIT的时间。net.ipv4.tcp_keepalive_*一系列参数，是用来设置服务器检测连接存活的相关配置。 Close_Wait(被动关闭)简单来说CLOSE_WAIT数目过大是由于被动关闭连接处理不当导致的。我说一个场景，服务器A会去请求服务器B上面的apache获取文件资源，正常情况下，如果请求成功，那么在抓取完资源后服务器A会主动发出关闭连接的请求，这个时候就是主动关闭连接，连接状态我们可以看到是TIME_WAIT。如果一旦发生异常呢？假设请求的资源服务器B上并不存在，那么这个时候就会由服务器B发出关闭连接的请求，服务器A就是被动的关闭了连接，如果服务器A被动关闭连接之后自己并没有释放连接，那就会造成CLOSE_WAIT的状态了。 如何解决？待补充 最后来回答两个问题： 为啥一台机器区区几百个close_wait就导致不可继续访问?不合理啊，一台机器不是号称最大可以打开65535个端口吗？回答：由于原因#4和#3所以导致整个IoLoop慢了，进而因为#2导致很多请求堆积，也就是说很多请求在被真正处理前已经在backlog里等了一会了。导致了SLB这端的链接批量的超时，同时又由于close_wait状态不会自动消失，导式最终无法再这个端口上创建新的链接引起了停止服务。 为啥明明有多个服务器承载，却几乎同时出了close_wait？又为什么同时不能再服务？那要SLB还有啥用呢？回答：有了上一个答案，结合SLB的特性，这个也就很好解释。这就是所谓的洪水蔓延，当SLB发现下面的一个节点不可用会把请求routing到其他可用节点上，导致其他节点压力增大。也犹豫相同原因，加速了其他节点出现close_wait. 参考https://blog.csdn.net/shootyou/article/details/6622226]]></content>
      <categories>
        <category>TCP/IP</category>
      </categories>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之Semaphore]]></title>
    <url>%2F2019%2F03%2F17%2FJava%E4%B9%8BSemaphore%2F</url>
    <content type="text"><![CDATA[N个线程循环打印1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class 老司机1 implements Runnable &#123; private static final Object LOCK = new Object(); /** * 当前即将打印的数字 */ private static int current = 0; /** * 当前线程编号，从0开始 */ private int threadNo; /** * 线程数量 */ private int threadCount; /** * 打印的最大数值 */ private int maxInt; public 老司机1(int threadNo, int threadCount, int maxInt) &#123; this.threadNo = threadNo; this.threadCount = threadCount; this.maxInt = maxInt; &#125; @Override public void run() &#123; while (true) &#123; synchronized (LOCK) &#123; // 判断是否轮到当前线程执行 while (current % threadCount != threadNo) &#123; if (current &gt; maxInt) &#123; break; &#125; try &#123; // 如果不是，则当前线程进入wait LOCK.wait(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; // 最大值跳出循环 if (current &gt; maxInt) &#123; break; &#125; System.out.println("thread" + threadNo + " : " + current); current++; // 唤醒其他wait线程 LOCK.notifyAll(); &#125; &#125; &#125; public static void main(String[] args) &#123; int threadCount = 3; int max = 100; for (int i = 0; i &lt; threadCount; i++) &#123; new Thread(new 老司机1(i, threadCount, max)).start(); &#125; &#125;&#125; 上述方法存在的问题核心方法在run里面，可以看见和我们交替打印奇偶数原理差不多，这里将我们的notify改成了notifyAll，这里要注意一下很多人会将notifyAll理解成其他wait的线程全部都会执行，其实是错误的。这里只会将wait的线程解除当前wait状态，也叫作唤醒，由于我们这里用同步锁synchronized块包裹住，那么唤醒的线程会做会抢夺同步锁。这个老司机的代码的确能跑通，但是有一个问题是什么呢？当我们线程数很大的时候，由于我们不确定唤醒的线程到底是否是下一个要执行的就有可能会出现抢到了锁但不该自己执行，然后又进入wait的情况，比如现在有100个线程，现在是第一个线程在执行，他执行完之后需要第二个线程执行，但是第100个线程抢到了，发现不是自己然后又进入wait，然后第99个线程抢到了，发现不是自己然后又进入wait，然后第98,97…直到第3个线程都抢到了，最后才到第二个线程抢到同步锁，这里就会白白的多执行很多过程，虽然最后能完成目标。 通过Semaphore进行改进Mutex与Semaphore的比较 Mutex是一把钥匙，一个人拿了就可进入一个房间，出来的时候把钥匙交给队列的第一个。一般的用法是用于串行化对critical section代码的访问，保证这段代码不会被并行的运行。 Semaphore是一件可以容纳N人的房间，如果人不满就可以进去，如果人满了，就要等待有人出来。对于N=1的情况，称为binary semaphore。一般的用法是，用于限制对于某一资源的同时访问。 Binary semaphore与Mutex的差异： 在 有的系统中Binary semaphore与Mutex是没有差异的。在有的系统上，主要的差异是mutex一定要由获得锁的进程来释放。而semaphore可以由其它进程释 放（这时的semaphore实际就是个原子的变量，大家可以加或减），因此semaphore可以用于进程间同步。Semaphore的同步功能是所有 系统都支持的，而Mutex能否由其他进程释放则未定，因此建议mutex只用于保护critical section。而semaphore则用于保护某变量，或者同步。 semaphore信号量(Semaphore)，有时被称为信号灯，是在多线程环境下使用的一种设施, 它负责协调各个线程, 以保证它们能够正确、合理的使用公共资源。 Semaphore分为单值和多值两种，前者只能被一个线程获得，后者可以被若干个线程获得。 举个栗子以一个停车场是运作为例。为了简单起见，假设停车场只有三个车位，一开始三个车位都是空的。这是如果同时来了五辆车，看门人允许其中三辆不受阻碍的进入，然后放下车拦，剩下的车则必须在入口等待，此后来的车也都不得不在入口处等待。这时，有一辆车离开停车场，看门人得知后，打开车拦，放入一辆，如果又离开两辆，则又可以放入两辆，如此往复。在这个停车场系统中，车位是公共资源，每辆车好比一个线程，看门人起的就是信号量的作用。更进一步，信号量的特性如下：信号量是一个非负整数（车位数），所有通过它的线程（车辆）都会将该整数减一（通过它当然是为了使用资源），当该整数值为零时，所有试图通过它的线程都将处于等待状态。在信号量上我们定义两种操作： Wait（等待） 和 Release（释放）。 当一个线程调用Wait等待）操作时，它要么通过然后将信号量减一，要么一自等下去，直到信号量大于一或超时。Release（释放）实际上是在信号量上执行加操作，对应于车辆离开停车场，该操作之所以叫做“释放”是应为加操作实际上是释放了由信号量守护的资源。 如何改进我们上一个线程持有下一个线程的信号量，通过一个信号量数组将全部关联起来,代码如下: 通过这种方式，我们就不会有白白唤醒的线程，每一个线程都按照我们所约定的顺序去执行，让每个线程的执行都能再你手中得到控制123456789101112131415161718192021222324252627282930313233343536static int result = 0; public static void main(String[] args) throws InterruptedException &#123; int N = 3; Thread[] threads = new Thread[N]; final Semaphore[] syncObjects = new Semaphore[N]; for (int i = 0; i &lt; N; i++) &#123; syncObjects[i] = new Semaphore(1); if (i != N-1)&#123; syncObjects[i].acquire(); &#125; &#125; for (int i = 0; i &lt; N; i++) &#123; final Semaphore lastSemphore = i == 0 ? syncObjects[N - 1] : syncObjects[i - 1]; final Semaphore curSemphore = syncObjects[i]; final int index = i; threads[i] = new Thread(new Runnable() &#123; public void run() &#123; try &#123; while (true) &#123; lastSemphore.acquire(); System.out.println("thread" + index + ": " + result++); if (result &gt; 100)&#123; System.exit(0); &#125; curSemphore.release(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); threads[i].start(); &#125; &#125; 另外一个方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package chapter_1_stackandqueue;import java.util.concurrent.Semaphore;/** * * 整体思路，每个线程持有一把锁，初始化，只有第一个线程有锁 * 执行完成之后把锁传递下去 * */public class a &#123; /** * 最大线程数 */ public static final int THREAD_NUMBER = 10; /** * 最大数 */ public static final int MAX_COUNT = 100; /** * 输出值 */ private static int count = 0; public static void main(String[] args) &#123; Semaphore[] semaphores = new Semaphore[THREAD_NUMBER]; for (int i = 0; i &lt; THREAD_NUMBER; i++) &#123; if (i == 0) &#123; semaphores[i] = new Semaphore(1); &#125; else &#123; semaphores[i] = new Semaphore(0); &#125; &#125; for (int i = 0; i &lt; THREAD_NUMBER; i++) &#123; new Thread(new TestThread(semaphores, i)).start(); &#125; &#125; public static class TestThread implements Runnable &#123; private Semaphore[] semaphores; private int number = 0; public TestThread(Semaphore[] semaphores, int number) &#123; this.number = number; this.semaphores = semaphores; &#125; @Override public void run() &#123; try &#123; while (true) &#123; semaphores[number].acquire(); if (count &gt;= MAX_COUNT) &#123; break; &#125; System.out.println(Thread.currentThread().getName() + "：" + count); count++; int current = number + 1; if (current &gt;= THREAD_NUMBER) &#123; current = 0; &#125; semaphores[current].release(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 参考https://juejin.im/post/5c89b9515188257e5b2befdd]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之checkForComodification]]></title>
    <url>%2F2019%2F03%2F17%2FJava%E4%B9%8BcheckForComodification%2F</url>
    <content type="text"><![CDATA[什么时候会发生checkForComodification？使用增强for循环的话会发生什么：1234567891011121314List&lt;String&gt; userNames = new ArrayList&lt;String&gt;() &#123;&#123; add("Hollis"); add("hollis"); add("HollisChuang"); add("H");&#125;&#125;;for (String userName : userNames) &#123; if (userName.equals("Hollis")) &#123; userNames.remove(userName); &#125;&#125;System.out.println(userNames); 以上代码，使用增强for循环遍历元素，并尝试删除其中的Hollis字符串元素。运行以上代码，会抛出以下异常：1234Exception in thread "main" java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(Unknown Source) at java.util.ArrayList$Itr.next(Unknown Source) at chapter_1_stackandqueue.a.main(a.java:22) 同样的，在增强for循环中使用add方法添加元素，结果也会同样抛出该异常。之所以会出现这个异常，是因为触发了一个Java集合的错误检测机制——fail-fast 。 那么什么是fail-fast？fail-fast，即快速失败，它是Java集合的一种错误检测机制。当多个线程对集合（非fail-safe的集合类）进行结构上的改变的操作时，有可能会产生fail-fast机制，这个时候就会抛出ConcurrentModificationException（当方法检测到对象的并发修改，但不允许这种修改时就抛出该异常）。同时需要注意的是，即使不是多线程环境，如果单线程违反了规则，同样也有可能会抛出改异常。那么，在增强for循环进行元素删除，是如何违反了规则的呢？要分析这个问题，我们先将增强for循环这个语法糖进行解糖，得到以下代码： 1234567891011121314151617181920public static void main(String[] args) &#123; // 使用ImmutableList初始化一个List List&lt;String&gt; userNames = new ArrayList&lt;String&gt;() &#123;&#123; add("Hollis"); add("hollis"); add("HollisChuang"); add("H"); &#125;&#125;; Iterator iterator = userNames.iterator(); do &#123; if(!iterator.hasNext()) break; String userName = (String)iterator.next(); if(userName.equals("Hollis")) userNames.remove(userName); &#125; while(true); System.out.println(userNames);&#125; 运行以上代码，同样会抛出异常。那么我们就看下checkForComodification方法的代码，看下抛出异常的原因。 查看Iterator中的next()方法因为报错的原因是执行了：String string = (String)iterator.next();这句，那查看Iterator类的代码123456789101112131415161718192021222324//把集合修改的次数（modCount）赋值给expectedModCount记录下来 int expectedModCount = modCount; public E next() &#123; //主要用来判断集合的修改次数是否合法 checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; &#125; //主要用来判断集合的修改次数是否合法 final void checkForComodification() &#123; //modCount代表集合修改的次数（例如：每list.add()一次就加1，list.remove()一次也加1） //expectedModCount的值是等于开始遍历集合时的修改的次数 if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125; checkForComodification（）方法中的modCount与expectedModCount的值不相等就会抛出异常。那么他们两个值分别在哪里被修改了呢？其实modCount的值是在list集合执行add，remove等操作的时候赋值，而expectedModCount是在new Iterator()时，把modCount的值赋给了expectedModCount记录。 remove/add 做了什么首先，我们要搞清楚的是，到底modCount和expectedModCount这两个变量都是个什么东西。通过翻源码，我们可以发现： modCount是ArrayList中的一个成员变量。它表示该集合实际被修改的次数。 expectedModCount 是 ArrayList中的一个内部类——Itr中的成员变量。expectedModCount表示这个迭代器期望该集合被修改的次数。其值是在ArrayList.iterator方法被调用的时候初始化的。只有通过迭代器对集合进行操作，该值才会改变。 Itr是一个Iterator的实现，使用ArrayList.iterator方法可以获取到的迭代器就是Itr类的实例。 他们之间的关系如下：1234567891011class ArrayList&#123; private int modCount; public void add(); public void remove(); private class Itr implements Iterator&lt;E&gt; &#123; int expectedModCount = modCount; &#125; public Iterator&lt;E&gt; iterator() &#123; return new Itr(); &#125;&#125; 再继续看remove源码 123456789101112/* * Private remove method that skips bounds checking and does not * return the value removed. */ private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work &#125; 可以看到，它只修改了modCount，并没有对expectedModCount做任何操作。 简单总结一下简单总结一下，之所以会抛出ConcurrentModificationException异常，是因为我们的代码中使用了增强for循环，而在增强for循环中，集合遍历是通过iterator进行的，但是元素的add/remove却是直接使用的集合类自己的方法。这就导致iterator在遍历的时候，会发现有一个元素在自己不知不觉的情况下就被删除/添加了，就会抛出一个异常，用来提示用户，可能发生了并发修改！ 如何正确使用 直接使用Iterator提供的remove方法。123456789101112131415List&lt;String&gt; userNames = new ArrayList&lt;String&gt;() &#123;&#123; add("Hollis"); add("hollis"); add("HollisChuang"); add("H");&#125;&#125;;Iterator iterator = userNames.iterator();while (iterator.hasNext()) &#123; if (iterator.next().equals("Hollis")) &#123; iterator.remove(); &#125;&#125;System.out.println(userNames); 如果直接使用Iterator提供的remove方法，那么就可以修改到expectedModCount的值。那么就不会再抛出异常了 使用Java 8中提供的filter过滤 Java 8中可以把集合转换成流，对于流有一种filter操作， 可以对原始 Stream 进行某项测试，通过测试的元素被留下来生成一个新 Stream。123456789List&lt;String&gt; userNames = new ArrayList&lt;String&gt;() &#123;&#123; add("Hollis"); add("hollis"); add("HollisChuang"); add("H");&#125;&#125;;userNames = userNames.stream().filter(userName -&gt; !userName.equals("Hollis")).collect(Collectors.toList());System.out.println(userNames); 使用增强for循环其实也可以如果，我们非常确定在一个集合中，某个即将删除的元素只包含一个的话， 比如对Set进行操作，那么其实也是可以使用增强for循环的，只要在删除之后，立刻结束循环体，不要再继续进行遍历就可以了，也就是说不让代码执行到下一次的next方法。 1234567891011121314List&lt;String&gt; userNames = new ArrayList&lt;String&gt;() &#123;&#123; add("Hollis"); add("hollis"); add("HollisChuang"); add("H");&#125;&#125;;for (String userName : userNames) &#123; if (userName.equals("Hollis")) &#123; userNames.remove(userName); break; &#125;&#125;System.out.println(userNames); 直接使用fail-safe的集合类在Java中，除了一些普通的集合类以外，还有一些采用了fail-safe机制的集合类。这样的集合容器在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发ConcurrentModificationException。 123456789101112ConcurrentLinkedDeque&lt;String&gt; userNames = new ConcurrentLinkedDeque&lt;String&gt;() &#123;&#123; add("Hollis"); add("hollis"); add("HollisChuang"); add("H");&#125;&#125;;for (String userName : userNames) &#123; if (userName.equals("Hollis")) &#123; userNames.remove(); &#125;&#125; 基于拷贝内容的优点是避免了ConcurrentModificationException，但同样地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用，并发修改。 参考https://blog.csdn.net/u012987546/article/details/52190851https://juejin.im/post/5c8717ad5188257dda56c381]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring之InitializingBean接口]]></title>
    <url>%2F2019%2F03%2F17%2FSpring%E4%B9%8BInitializingBean%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[什么时候使用有时候会遇到这样的问题：在我们将一个Bean交给Spring管理的时候，有时候我们的Bean中有某个属性需要注入，但是又不能通过一般的方式注入，什么意思呢？举个栗子：首先我们有个Service,在该Service中有一个属性，但是该属性不支持Spring注入，只能通过Build或者new的方式创建（比如StringBuffer之类的），但是我们想在Spring配置Bean的时候一起将该属性注入进来，这时候该怎么办呢？这时候可以通过实现InitializingBean接口来解决！ 源代码12345678910111213package org.springframework.beans.factory;public abstract interface InitializingBean&#123; public abstract void afterPropertiesSet() throws Exception;&#125;/* Location: C:\Users\Administrator\.m2\repository\org\springframework\spring-beans\4.3.13.RELEASE\spring-beans-4.3.13.RELEASE.jar * Qualified Name: org.springframework.beans.factory.InitializingBean * Java Class Version: 6 (50.0) * JD-Core Version: 0.7.0.1 */ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127/* */ package org.springframework.cache.support;/* */ /* */ import java.util.Collection;/* */ import java.util.Collections;/* */ import java.util.LinkedHashSet;/* */ import java.util.Set;/* */ import java.util.concurrent.ConcurrentHashMap;/* */ import java.util.concurrent.ConcurrentMap;/* */ import org.springframework.beans.factory.InitializingBean;/* */ import org.springframework.cache.Cache;/* */ import org.springframework.cache.CacheManager;//* */ public abstract class AbstractCacheManager/* */ implements CacheManager, InitializingBean/* */ &#123;/* 41 */ private final ConcurrentMap&lt;String, Cache&gt; cacheMap = new ConcurrentHashMap(16);/* */ /* 43 */ private volatile Set&lt;String&gt; cacheNames = Collections.emptySet();/* */ /* */ /* */ public AbstractCacheManager() &#123;&#125;/* */ /* */ public void afterPropertiesSet()/* */ &#123;/* 50 */ initializeCaches();/* */ &#125;/* */ public void initializeCaches()/* */ &#123;/* 61 */ Collection&lt;? extends Cache&gt; caches = loadCaches();/* */ /* 63 */ synchronized (this.cacheMap) &#123;/* 64 */ this.cacheNames = Collections.emptySet();/* 65 */ this.cacheMap.clear();/* 66 */ Set&lt;String&gt; cacheNames = new LinkedHashSet(caches.size());/* 67 */ for (Cache cache : caches) &#123;/* 68 */ String name = cache.getName();/* 69 */ this.cacheMap.put(name, decorateCache(cache));/* 70 */ cacheNames.add(name);/* */ &#125;/* 72 */ this.cacheNames = Collections.unmodifiableSet(cacheNames);/* */ &#125;/* */ &#125;/* */ protected abstract Collection&lt;? extends Cache&gt; loadCaches();/* */ /* */ /* */ /* */ /* */ /* */ public Cache getCache(String name)/* */ &#123;/* 88 */ Cache cache = (Cache)this.cacheMap.get(name);/* 89 */ if (cache != null) &#123;/* 90 */ return cache;/* */ &#125;/* */ /* */ /* 94 */ synchronized (this.cacheMap) &#123;/* 95 */ cache = (Cache)this.cacheMap.get(name);/* 96 */ if (cache == null) &#123;/* 97 */ cache = getMissingCache(name);/* 98 */ if (cache != null) &#123;/* 99 */ cache = decorateCache(cache);/* 100 */ this.cacheMap.put(name, cache);/* 101 */ updateCacheNames(name);/* */ &#125;/* */ &#125;/* 104 */ return cache;/* */ &#125;/* */ &#125;/* */ /* */ /* */ public Collection&lt;String&gt; getCacheNames()/* */ &#123;/* 111 */ return this.cacheNames;/* */ &#125;/* */ /* */ /* */ protected final Cache lookupCache(String name)/* */ &#123;/* 128 */ return (Cache)this.cacheMap.get(name);/* */ &#125;/* */ /* */ /* */ /* */ /* */ /* */ @Deprecated/* */ protected final void addCache(Cache cache)/* */ &#123;/* 138 */ String name = cache.getName();/* 139 */ synchronized (this.cacheMap) &#123;/* 140 */ if (this.cacheMap.put(name, decorateCache(cache)) == null) &#123;/* 141 */ updateCacheNames(name);/* */ &#125;/* */ &#125;/* */ &#125;/* */ private void updateCacheNames(String name)/* */ &#123;/* 154 */ Set&lt;String&gt; cacheNames = new LinkedHashSet(this.cacheNames.size() + 1);/* 155 */ cacheNames.addAll(this.cacheNames);/* 156 */ cacheNames.add(name);/* 157 */ this.cacheNames = Collections.unmodifiableSet(cacheNames);/* */ &#125;/* */ /* */ protected Cache decorateCache(Cache cache)/* */ &#123;/* 170 */ return cache;/* */ &#125;/* */ /* */ protected Cache getMissingCache(String name)/* */ &#123;/* 187 */ return null;/* */ &#125;/* */ &#125;/* Location: C:\Users\Administrator\.m2\repository\org\springframework\spring-context\4.3.13.RELEASE\spring-context-4.3.13.RELEASE.jar * Qualified Name: org.springframework.cache.support.AbstractCacheManager * Java Class Version: 6 (50.0) * JD-Core Version: 0.7.0.1 */ 初始化bean的两种方式 实现InitializingBean接口，实现afterPropertiesSet方法，或者在配置文件中同过init-method指定，两种方式可以同时使用 实现InitializingBean接口是直接调用afterPropertiesSet方法，比通过反射调用init-method指定的方法效率相对来说要高点。但是init-method方式消除了对spring的依赖 如果调用afterPropertiesSet方法时出错，则不调用init-method指定的方法。 参考https://blog.csdn.net/flqljh/article/details/49834541]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之注解]]></title>
    <url>%2F2019%2F03%2F17%2FJava%E4%B9%8B%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是注解？用一个词就可以描述注解，那就是元数据，即一种描述数据的数据。所以，可以说注解就是源代码的元数据。比如，下面这段代码： 1234@Overridepublic String toString() &#123; return "This is String Representation of current object.";&#125; 上面的代码中，我重写了toString()方法并使用了@Override注解。但是，即使我不使用@Override注解标记代码，程序也能够正常执行。那么，该注解表示什么？这么写有什么好处吗？事实上，@Override告诉编译器这个方法是一个重写方法(描述方法的元数据)，如果父类中不存在该方法，编译器便会报错，提示该方法没有重写父类中的方法。如果我不小心拼写错误，例如将toString()写成了toStrring(){double r}，而且我也没有使用@Override注解，那程序依然能编译运行。但运行结果会和我期望的大不相同。现在我们了解了什么是注解，并且使用注解有助于阅读程序。 Annotation是一种应用于类、方法、参数、变量、构造器及包声明中的特殊修饰符。它是一种由JSR-175标准选择用来描述元数据的一种工具。 为什么要引入注解？使用Annotation之前(甚至在使用之后)，XML被广泛的应用于描述元数据。不知何时开始一些应用开发人员和架构师发现XML的维护越来越糟糕了。他们希望使用一些和代码紧耦合的东西，而不是像XML那样和代码是松耦合的(在某些情况下甚至是完全分离的)代码描述。如果你在Google中搜索“XML vs. annotations”，会看到许多关于这个问题的辩论。最有趣的是XML配置其实就是为了分离代码和配置而引入的。上述两种观点可能会让你很疑惑，两者观点似乎构成了一种循环，但各有利弊。下面我们通过一个例子来理解这两者的区别。 假如你想为应用设置很多的常量或参数，这种情况下，XML是一个很好的选择，因为它不会同特定的代码相连。如果你想把某个方法声明为服务，那么使用Annotation会更好一些，因为这种情况下需要注解和方法紧密耦合起来，开发人员也必须认识到这点。 另一个很重要的因素是Annotation定义了一种标准的描述元数据的方式。在这之前，开发人员通常使用他们自己的方式定义元数据。例如，使用标记interfaces，注释，transient关键字等等。每个程序员按照自己的方式定义元数据，而不像Annotation这种标准的方式。 目前，许多框架将XML和Annotation两种方式结合使用，平衡两者之间的利弊。 通过实现ConstraintValidator完成自定义校验注解 注解分类按运行机制（注解存在于程序的那个阶段）将注解分为三类： 源码注解(只在源码存在) 编译注解(在class文件中也存在) 运行时注解(在运行阶段仍然起作用) 按照来源来分的话，有如下三类： JDK自带的注解（Java目前只内置了三种标准注解：@Override、@Deprecated、@SuppressWarnings，以及四种元注解：@Target、@Retention、@Documented、@Inherited） 第三方的注解——这一类注解是我们接触最多和作用最大的一类 自定义注解——也可以看作是我们编写的注解，其他的都是他人编写注解 如何自定义一个注解？编写一个校验手机号格式是否正确的校验器。背景：用户登录页面传入手机号作为账号，后端验证手机号格式是否正确 代码123456789101112131415161718192021222324252627282930package com.sjt.miaosha.validator;import static java.lang.annotation.ElementType.ANNOTATION_TYPE;import static java.lang.annotation.ElementType.CONSTRUCTOR;import static java.lang.annotation.ElementType.FIELD;import static java.lang.annotation.ElementType.METHOD;import static java.lang.annotation.ElementType.PARAMETER;import static java.lang.annotation.RetentionPolicy.RUNTIME;import java.lang.annotation.Documented;import java.lang.annotation.Retention;import java.lang.annotation.Target;import javax.validation.Constraint;import javax.validation.Payload;@Target(&#123; METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER &#125;)@Retention(RUNTIME)@Documented@Constraint(validatedBy = &#123;IsMobileValidator.class&#125;)public @interface IsMobile &#123; boolean required() default true; String message() default "手机号码格式错误"; Class&lt;?&gt;[] groups() default &#123; &#125;; Class&lt;? extends Payload&gt;[] payload() default &#123; &#125;;&#125; 该自定义注解类中用到了四种元注解，最后一个@Constraint指定了校验类，也就是接下来的IsMobileValidator类。值得一提的是除了自定义的message、require属性外，下面的groups和payload也是必须添加的。 IsMobileValidator为自定义注解的校验类。123456789101112131415161718192021222324252627282930313233package com.sjt.miaosha.validator;import javax.validation.ConstraintValidator;import javax.validation.ConstraintValidatorContext;import org.apache.commons.lang3.StringUtils;import com.sjt.miaosha.util.ValidatorUtil;public class IsMobileValidator implements ConstraintValidator&lt;IsMobile, String&gt; &#123; private boolean required = false; public void initialize(IsMobile constraintAnnotation) &#123; required = constraintAnnotation.required(); &#125; public boolean isValid(String value, ConstraintValidatorContext context) &#123; if (required) &#123; return ValidatorUtil.isMobile(value); &#125; else &#123; if (StringUtils.isEmpty(value)) &#123; return true; &#125; else &#123; return ValidatorUtil.isMobile(value); &#125; &#125; &#125;&#125; 校验类需要实现ConstraintValidator接口。接口使用了泛型，需要指定两个参数，第一个自定义注解类，第二个为需要校验的数据类型。实现接口后要override两个方法，分别为initialize方法和isValid方法。其中initialize为初始化方法，可以在里面做一些初始化操作，isValid方法就是我们最终需要的校验方法了。可以在该方法中实现具体的校验步骤。本示例中进行了简单的手机号校验。 完成这几部分之后，一个简单的自定义校验注解就完成啦，不要忘记在使用的时候加上@Valid注解开启valid校验。 那么如何获取在注解中定义的message信息呢？在valid校验中，如果校验不通过，会产生BindException异常，捕捉到异常后可以获取到defaultMessage也就是自定义注解中定义的内容，具体实现如下：1234BindException ex = (BindException)e;List&lt;ObjectError&gt; errors = ex.getAllErrors();ObjectError error = errors.get(0);String msg = error.getDefaultMessage(); 总结 自定义注解需要去手动实现两个文件：自定义注解类 + 注解校验器类 自定义注解类：message() + groups() + payload() 必须； 注解校验器类：继承 ConstraintValidator 类&lt;注解类，注解参数类型&gt; + 两个方法（initialize：初始化操作、isValid：逻辑处理） 参考http://www.importnew.com/10294.htmlhttps://www.cnblogs.com/Qian123/p/5256084.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Epoll模型]]></title>
    <url>%2F2019%2F03%2F11%2FEpoll%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[select VS epoll相比于select，epoll最大的好处在于它不会随着监听fd数目的增长而降低效率。因为在内核中的select实现中，它是采用轮询来处理的，轮询的fd数目越多，自然耗时越多。并且，在linux/posix_types.h头文件有这样的声明：#define __FD_SETSIZE 1024表示select最多同时监听1024个fd，当然，可以通过修改头文件再重编译内核来扩大这个数目，但这似乎并不治本。 常用模型的特点Linux 下设计并发网络程序，有典型的 Apache 模型（ Process Per Connection ，简称 PPC ）， TPC （ Thread Per Connection ）模型，以及 select 模型和 poll 模型。 PPC/TPC 模型这两种模型思想类似，就是让每一个到来的连接一边自己做事去，别再来烦我 。只是 PPC 是为它开了一个进程，而 TPC 开了一个线程。可是别烦我是有代价的，它要时间和空间啊，连接多了之后，那么多的进程 / 线程切换，这开销就上来了；因此这类模型能接受的最大连接数都不会高，一般在几百个左右。 select 模型 最大并发数限制，因为一个进程所打开的 FD （文件描述符）是有限制的，由 FD_SETSIZE 设置，默认值是 1024/2048 ，因此 Select 模型的最大并发数就被相应限制了。自己改改这个 FD_SETSIZE ？想法虽好，可是先看看下面吧 … 效率问题， select 每次调用都会线性扫描全部的 FD 集合，这样效率就会呈现线性下降，把 FD_SETSIZE 改大的后果就是，大家都慢慢来，什么？都超时了？？！！ 内核 / 用户空间 内存拷贝问题，如何让内核把 FD 消息通知给用户空间呢？在这个问题上 select 采取了内存拷贝方法。 poll 模型基本上效率和 select 是相同的， select 缺点的 2 和 3 它都没有改掉。 Epoll的提升Epoll 的改进之处。 Epoll 没有最大并发连接的限制，上限是最大可以打开文件的数目，这个数字一般远大于 2048, 一般来说这个数目和系统内存关系很大 ，具体数目可以 cat /proc/sys/fs/file-max 察看。 12[root@rabbitmq _posts]# cat /proc/sys/fs/file-max94924 效率提升， Epoll 最大的优点就在于它只管你“活跃”的连接 ，而跟连接总数无关，因此在实际的网络环境中， Epoll 的效率就会远远高于 select 和 poll 。 内存拷贝， Epoll 在这点上使用了“共享内存 ”，这个内存拷贝也省略了。 Epoll为什么高效Epoll 的高效和其数据结构的设计是密不可分的。首先回忆一下 select 模型，当有 I/O 事件到来时， select 通知应用程序有事件到了快去处理，而应用程序必须轮询所有的 FD 集合，测试每个 FD 是否有事件发生，并处理事件。123456789101112int res = select(maxfd+1, &amp;readfds, NULL, NULL, 120);if (res &gt; 0)&#123; for (int i = 0; i &lt; MAX_CONNECTION; i++) &#123; if (FD_ISSET(allConnection[i], &amp;readfds)) &#123; handleEvent(allConnection[i]); &#125; &#125;&#125;// if(res == 0) handle timeout, res &lt; 0 handle error Epoll 不仅会告诉应用程序有I/0 事件到来，还会告诉应用程序相关的信息，这些信息是应用程序填充的，因此根据这些信息应用程序就能直接定位到事件，而不必遍历整个FD 集合。12345int res = epoll_wait(epfd, events, 20, 120);for (int i = 0; i &lt; res;i++)&#123; handleEvent(events[n]);&#125; epoll的接口epoll的接口非常简单，一共就三个函数： int epoll_create(int size);创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);epoll的事件注册函数，它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。控制某个 Epoll 文件描述符上的事件：注册、修改、删除。 第一个参数是epoll_create()的返回值，创建 Epoll 专用的文件描述符。相对于 select 模型中的 FD_SET 和 FD_CLR 宏。第二个参数表示动作，用三个宏来表示：EPOLL_CTL_ADD：注册新的fd到epfd中；EPOLL_CTL_MOD：修改已经注册的fd的监听事件；EPOLL_CTL_DEL：从epfd中删除一个fd；第三个参数是需要监听的fd，第四个参数是告诉内核需要监听什么事，struct epoll_event结构如下：struct epoll_event { __uint32_t events; / Epoll events / epoll_data_t data; / User data variable /}; events可以是以下几个宏的集合：EPOLLIN ： 表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；EPOLLOUT： 表示对应的文件描述符可以写；EPOLLPRI： 表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；EPOLLERR： 表示对应的文件描述符发生错误；EPOLLHUP： 表示对应的文件描述符被挂断；EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。EPOLLONESHOT： 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);等待 I/O 事件的发生；参数说明：epfd: 由 epoll_create() 生成的 Epoll 专用的文件描述符；epoll_event: 用于回传代处理事件的数组；maxevents: 每次能处理的事件数；timeout: 等待 I/O 事件发生的超时值；返回发生事件数。相对于 select 模型中的 select 函数。 参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。 生成一个 Epoll 专用的文件描述符，其实是申请一个内核空间，用来存放你想关注的 socket fd 上是否发生以及发生了什么事件。 size 就是你在这个 Epoll fd 上能关注的最大 socket fd 数，大小自定，只要内存足够。 EPOLL事件有两种模型：Edge Triggered (ET) 边缘触发 只有数据到来，才触发，不管缓存区中是否还有数据。Level Triggered (LT) 水平触发 只要有数据都会触发。 例如： 我们已经把一个用来从管道中读取数据的文件句柄(RFD)添加到epoll描述符 这个时候从管道的另一端被写入了2KB的数据 调用epoll_wait(2)，并且它会返回RFD，说明它已经准备好读取操作 然后我们读取了1KB的数据 调用epoll_wait(2)…… Edge Triggered 工作模式：如果我们在第1步将RFD添加到epoll描述符的时候使用了EPOLLET标志，那么在第5步调用epoll_wait(2)之后将有可能会挂起，因为剩余的数据还存在于文件的输入缓冲区内，而且数据发出端还在等待一个针对已经发出数据的反馈信息。只有在监视的文件句柄上发生了某个事件的时候 ET 工作模式才会汇报事件。因此在第5步的时候，调用者可能会放弃等待仍在存在于文件输入缓冲区内的剩余数据。在上面的例子中，会有一个事件产生在RFD句柄上，因为在第2步执行了一个写操作，然后，事件将会在第3步被销毁。因为第4步的读取操作没有读空文件输入缓冲区内的数据，因此我们在第5步调用 epoll_wait(2)完成后，是否挂起是不确定的。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。最好以下面的方式调用ET模式的epoll接口，在后面会介绍避免可能的缺陷。 i 基于非阻塞文件句柄 ii 只有当read(2)或者write(2)返回EAGAIN时才需要挂起，等待。但这并不是说每次read()时都需要循环读，直到读到产生一个EAGAIN才认为此次事件处理完成，当read()返回的读到的数据长度小于请求的数据长度时，就可以确定此时缓冲中已没有数据了，也就可以认为此事读事件已处理完成。 Level Triggered 工作模式相反的，以LT方式调用epoll接口的时候，它就相当于一个速度比较快的poll(2)，并且无论后面的数据是否被使用，因此他们具有同样的职能。因为即使使用ET模式的epoll，在收到多个chunk的数据的时候仍然会产生多个事件。调用者可以设定EPOLLONESHOT标志，在 epoll_wait(2)收到事件后epoll会与事件关联的文件句柄从epoll描述符中禁止掉。因此当EPOLLONESHOT设定后，使用带有 EPOLL_CTL_MOD标志的epoll_ctl(2)处理文件句柄就成为调用者必须作的事情。 详细解释ET, LT: LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的select/poll都是这种模型的代表． ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once),不过在TCP协议中，ET模式的加速效用仍需要更多的benchmark确认（这句话不理解）。 在许多测试中我们会看到如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当我们遇到大量的idle- connection(例如WAN环境中存在大量的慢速连接)，就会发现epoll的效率大大高于select/poll。（未测试） 另外，当使用epoll的ET模型来工作时，当产生了一个EPOLLIN事件后，读数据的时候需要考虑的是当recv()返回的大小如果等于请求的大小，那么很有可能是缓冲区还有数据未读完，也意味着该次事件还没有处理完，所以还需要再次读取：123456789101112131415161718192021while(rs)&#123; buflen = recv(activeevents[i].data.fd, buf, sizeof(buf), 0); if(buflen &lt; 0) &#123; // 由于是非阻塞的模式,所以当errno为EAGAIN时,表示当前缓冲区已无数据可读 // 在这里就当作是该次事件已处理处. if(errno == EAGAIN) break; else return; &#125; else if(buflen == 0) &#123; // 这里表示对端的socket已正常关闭. &#125; if(buflen == sizeof(buf) rs = 1; // 需要再次读取 else rs = 0;&#125; 还有，假如发送端流量大于接收端的流量(意思是epoll所在的程序读比转发的socket要快),由于是非阻塞的socket,那么send()函数虽然返回,但实际缓冲区的数据并未真正发给接收端,这样不断的读和发，当缓冲区满后会产生EAGAIN错误(参考man send),同时,不理会这次请求发送的数据.所以,需要封装socket_send()的函数用来处理这种情况,该函数会尽量将数据写完再返回，返回-1表示出错。在socket_send()内部,当写缓冲已满(send()返回-1,且errno为EAGAIN),那么会等待后再重试.这种方式并不很完美,在理论上可能会长时间的阻塞在socket_send()内部,但暂没有更好的办法.123456789101112131415161718192021222324252627282930ssize_t socket_send(int sockfd, const char* buffer, size_t buflen)&#123; ssize_t tmp; size_t total = buflen; const char *p = buffer; while(1) &#123; tmp = send(sockfd, p, total, 0); if(tmp &lt; 0) &#123; // 当send收到信号时,可以继续写,但这里返回-1. if(errno == EINTR) return -1; // 当socket是非阻塞时,如返回此错误,表示写缓冲队列已满, // 在这里做延时后再重试. if(errno == EAGAIN) &#123; usleep(1000); continue; &#125; return -1; &#125; if((size_t)tmp == total) return buflen; total -= tmp; p += tmp; &#125; return tmp;&#125; epoll如何解决“惊群”问题当客户端发起连接后，由于所有的worker子进程都监听着同一个端口，内核协议栈在检测到客户端连接后，会激活所有休眠的worker子进程，最终只会有一个子进程成功建立新连接，其他子进程都会accept失败。 Accept失败的子进程是不应该被内核唤醒的，因为它们被唤醒的操作是多余的，占用本不应该被占用的系统资源，引起不必要的进程上下文切换，增加了系统开销，同时也影响了客户端连接的时延。 “惊群”问题是多个子进程同时监听同一个端口引起的，因此解决的方法是同一时刻只让一个子进程监听服务器端口，这样新连接事件只会唤醒唯一正在监听端口的子进程。 因此“惊群”问题通过非阻塞的accept锁来实现进程互斥accept()，其原理是：在worker进程主循环中非阻塞trylock获取accept锁，如果trylock成功，则此进程把监听端口对应的fd通过epoll_ctl()加入到本进程自由的epoll事件集；如果trylock失败，则把监听fd从本进程对应的epoll事件集中清除。 Epoll不能提高并发 确实有很多地方说select/poll/epoll并发， 这是多么扯淡啊， 它们不过是多路复用， 而已。 很多网上程序给出的epoll代码实现的服务器， 其实是没有并发能力的， 也仅仅是迭代服务器。 select/poll/epoll的作用是IO复用， 要实现并发， 还是需要交个其他线程/进程去处理。 业界很多成熟的服务组件， 就是这么玩的, 如nginx. 参考&lt;https://blog.csdn.net/stpeace/article/details/80951371 &gt;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo如何支持异步调用？]]></title>
    <url>%2F2019%2F03%2F11%2FDubbo%E5%A6%82%E4%BD%95%E6%94%AF%E6%8C%81%E5%BC%82%E6%AD%A5%E8%B0%83%E7%94%A8%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[为什么异步？异步化的好处也是比较明显的，可以加快后台的处理效率，做到代码直接的解耦，Dubbo就是一个支持异步调用的RPC框架。 在Dubbo中使用异步 异步调用的场景 假设系统A，远程调用B系统的某个方法，这个方法与数据库的交互很多，逻辑相对复杂，正常的代码执行的时间是3秒，A系统调用完B系统之后，还需要做一些其他的逻辑操作，这个代码耗时可能需要4秒，等这个3秒的逻辑做完之后，根据B系统返回的结果再做一些其他的操作，那么同步调用的时间是3秒+4秒 = 7秒，那么一次操作的时间就是7秒 同步调用的实现： 接口实现（provider和consumer端都需要）12345678package org.bazinga.service; public interface AsyncInvokeService &#123; public Integer getResult(); &#125; 接口实现，我们默认线程sleep三秒，3秒代表代码复杂的逻辑操作和数据库的交互的时间12345678910111213141516171819package org.bazinga.service.impl; import org.bazinga.service.AsyncInvokeService; public class AsyncInvokeServiceImpl implements AsyncInvokeService &#123; public Integer getResult() &#123; try &#123; Thread.sleep(3000l); //模拟复杂的逻辑操作时间和数据库交互的时间消耗 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return 1; &#125; &#125; 在不开启异步调用的配置的时候，spring的配置文件和普通配置是一样的spring-dubbo-provider-async.xml，需要注意的是我们线程故意睡了3秒，这边我们配置timeout的时间为4秒，否则就会调用超时：1234567891011121314151617181920&lt;?xml version="1.1" encoding="UTF-8"?&gt; &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application owner="lyncc" name="bazinga-app" /&gt; &lt;!--zookeeper注册中心 --&gt; &lt;dubbo:registry protocol="zookeeper" address="127.0.0.1:2181"/&gt; &lt;dubbo:protocol name ="dubbo" port="20880" /&gt; &lt;!-- 发布这个服务 调用超时是4秒，支持异步调用--&gt; &lt;dubbo:service protocol="dubbo" timeout="4000" interface ="org.bazinga.service.AsyncInvokeService" ref="asyncInvokeService"/&gt; &lt;!-- 和本地bean一样实现服务 --&gt; &lt;bean id="asyncInvokeService" class="org.bazinga.service.impl.AsyncInvokeServiceImpl" /&gt; &lt;/beans&gt; 消费端的spring配置文件spring-dubbo-consumer-async.xml：12345678910111213141516&lt;?xml version="1.1" encoding="UTF-8"?&gt; &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application owner="lyncc" name="bazinga-consumer" /&gt; &lt;!--zookeeper注册中心 --&gt; &lt;dubbo:registry protocol="zookeeper" address="127.0.0.1:2181"/&gt; &lt;dubbo:reference id="asyncInvokeService" interface="org.bazinga.service.AsyncInvokeService"/&gt; &lt;/beans&gt; 服务提供者端的测试类：123456789101112131415package org.bazinga.service.test; import org.springframework.context.support.ClassPathXmlApplicationContext; public class DubboxProviderAsyncService &#123; public static void main(String[] args) throws InterruptedException &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext( "spring-dubbo-provider-async.xml"); context.start(); Thread.sleep(2000000l); &#125; &#125; 服务消费者端的测试类：123456789101112131415161718192021222324252627282930313233package org.bazinga.service.test; import org.bazinga.service.AsyncInvokeService; import org.springframework.context.support.ClassPathXmlApplicationContext; public class DubboConsumerAsyncService &#123; public static void main(String[] args) throws InterruptedException &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext( "spring-dubbo-consumer-async.xml"); context.start(); long beginTime = System.currentTimeMillis(); for (int count = 0; count &lt; 10; count++) &#123; // 调用10次 AsyncInvokeService asyncInvokeService = (AsyncInvokeService)context.getBean("asyncInvokeService"); Integer result = asyncInvokeService.getResult(); //wait 返回结果 等待3秒 Thread.sleep(4000l); //模拟本地复杂的逻辑操作，耗时4秒 Integer localcalcResult = 2;//本地经过4秒处理得到的计算数据是2 System.out.println(result + localcalcResult);//根据远程调用返回的结果和本地操作的值，得到结果集 &#125; System.out.println("call 10 times,cost time is " + (System.currentTimeMillis() - beginTime)); Thread.sleep(2000000l); &#125; &#125; 先启动DubboxProviderAsyncService，然后再启动DubboConsumerAsyncService的main函数： 消费端的控制台打印信息是：运行没有问题，但是调用10次，一共耗时71秒，假如改成异步调用，我们不需要等待调用返回的结果，而是在用的时候，再去获取值的话，这样会大大的提高执行的速度 异步调用实现 异步调用的实现，很简单，现在我们修改一下配置文件，使其支持异步调用，其实配置相对比较简单，只需要在调用端的spring的配置文件中加上async=”true”,注意一定是在调用端配置该关键字，异步调用，顾名思义，就是需要告之调用者，调用之后不需要等待（Note：修改的是调用的spring配置文件spring-dubbo-consumer-async.xml）12345678910111213141516&lt;?xml version="1.1" encoding="UTF-8"?&gt; &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application owner="lyncc" name="bazinga-consumer" /&gt; &lt;!--zookeeper注册中心 --&gt; &lt;dubbo:registry protocol="zookeeper" address="127.0.0.1:2181"/&gt; &lt;dubbo:reference id="asyncInvokeService" interface="org.bazinga.service.AsyncInvokeService" async="true"/&gt; &lt;/beans&gt; 其实就是加了一个标签的支持，async=”true”，这样就支持了异步的调用了 修改消费者的测试代码，配置文件的修改只是告诉Dubbo，调用者会进行异步调用，但如何异步调用，还是需要调用者自己去实现的，实现依赖于RpcContext：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package org.bazinga.service.test; import java.util.concurrent.Future; import org.bazinga.service.AsyncInvokeService; import org.springframework.context.support.ClassPathXmlApplicationContext; import com.alibaba.dubbo.rpc.RpcContext; public class DubboConsumerAsyncService &#123; public static void main(String[] args) throws InterruptedException &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext( "spring-dubbo-consumer-async.xml"); context.start(); long beginTime = System.currentTimeMillis(); for (int count = 0; count &lt; 10; count++) &#123; // 调用10次 // AsyncInvokeService asyncInvokeService = // (AsyncInvokeService)context.getBean("asyncInvokeService"); // Integer result = asyncInvokeService.getResult(); //wait 返回结果 等待3秒 // // Thread.sleep(4000l); //模拟本地复杂的逻辑操作，耗时4秒 // // Integer localcalcResult = 2;//本地经过4秒处理得到的计算数据是2 // // System.out.println(result + localcalcResult);//根据远程调用返回的结果和本地操作的值，得到结果集 AsyncInvokeService asyncInvokeService = (AsyncInvokeService) context .getBean("asyncInvokeService"); Integer remotingResult = asyncInvokeService.getResult(); // 不等待 Thread.sleep(4000l); // 模拟本地复杂的逻辑操作，耗时4秒 Future&lt;Integer&gt; future = RpcContext.getContext().getFuture(); try &#123; remotingResult = future.get(); &#125; catch (java.util.concurrent.ExecutionException e) &#123; e.printStackTrace(); &#125; Integer localcalcResult = 2;// 本地经过4秒处理得到的计算数据是2 System.out.println(remotingResult + localcalcResult);// 根据远程调用返回的结果和本地操作的值，得到结果集 &#125; System.out.println("call 10 times,cost time is " + (System.currentTimeMillis() - beginTime)); Thread.sleep(2000000l); &#125; &#125; 关键代码就是： `Integer remotingResult = asyncInvokeService.getResult(); // 不等待` 这行代码不会阻塞至服务提供者端把数据返回，而是直接返回，然后睡了4秒，这个4秒，模仿的是本地的逻辑操作，这是其实远程的逻辑也在执行，这样就可以并行操作了，最后调用： `Future&lt;Integer&gt; future = RpcContext.getContext().getFuture(); ` 获取到远程调用异步返回的结果，完成最后的操作，我们可以看啊看控制台打印的结果：可以看出结果没有变，仍旧是3，但是调用时间变成了41秒，异步调用的好处就可以显示出来了]]></content>
      <categories>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM致命错误日志hs_err_pid.log分析]]></title>
    <url>%2F2019%2F03%2F11%2FJVM%E8%87%B4%E5%91%BD%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97hs-err-pid-log%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[实验出问题了在服务器上执行nohup java -jar server-v1.0.jar -d /data/wave/wzm/out3/2018-03/ -s 172.18.20.215 -t wave201803 &gt; nohup5.out 2&gt;&amp;1 &amp;之后的nohup.out文件，如下12345678910111213141516171819202122nohup: ignoring inputERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.完成 resource 初始化！22:14:44.694 [nioEventLoopGroup-2-1] ERROR com.globigdata.server.LineParseHandler - java.io.IOException: Invalid argument5, tid=0x00007f799d0fb700## JRE version: Java(TM) SE Runtime Environment (8.0_171-b11) (build 1.8.0_171-b11)# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.171-b11 mixed mode linux-amd64 compressed oops)# Problematic frame:# J 1748 C2 sun.nio.ch.IOUtil.write(Ljava/io/FileDescriptor;[Ljava/nio/ByteBuffer;IILsun/nio/ch/NativeDispatcher;)J (509 bytes) @ 0x00007f79a52ee461 [0x00007f79a52edce0+0x781]## Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again## An error report file with more information is saved as:# /data/hs_err_pid18285.log## If you would like to submit a bug report, please visit:# http://bugreport.java.com/bugreport/crash.jsp# 什么时候会产生该文件？当JVM发生致命错误导致崩溃时，会生成一个hs_err_pid_xxx.log这样的文件，该文件包含了导致 JVM crash 的重要信息，我们可以通过分析该文件定位到导致 JVM Crash 的原因，从而修复保证系统稳定。 he_err_pid.log包含的内容日志头文件1234567891011121314## A fatal error has been detected by the Java Runtime Environment:## SIGSEGV (0xb) at pc=0x00007f79a52ee461, pid=18285, tid=0x00007f799d0fb700## JRE version: Java(TM) SE Runtime Environment (8.0_171-b11) (build 1.8.0_171-b11)# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.171-b11 mixed mode linux-amd64 compressed oops)# Problematic frame:# J 1748 C2 sun.nio.ch.IOUtil.write(Ljava/io/FileDescriptor;[Ljava/nio/ByteBuffer;IILsun/nio/ch/NativeDispatcher;)J (509 bytes) @ 0x00007f79a52ee461 [0x00007f79a52edce0+0x781]## Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again## If you would like to submit a bug report, please visit:# http://bugreport.java.com/bugreport/crash.jsp 这段内容主要简述了导致 JVM Crash 的原因。常见的原因有 JVM 自身的 bug，应用程序错误，JVM 参数，服务器资源不足，JNI 调用错误等。当然还有一些版本和配置信息，1SIGSEGV (0xb) at pc=0x00007f79a52ee461, pid=18285, tid=0x00007f799d0fb700 非预期的错误被 JRE 检测到了，其中 SIGSEGV ：信号量 0xb ：信号码 pc=0x00007f79a52ee461 ：程序计数器的值 pid=18285 ：进程号 tid=0x00007f799d0fb700：线程号 SIGSEGV(0xb) 表示 JVM Crash 时正在执行 JNI 代码，常见的描述还有EXCEPTION_ACCESS_VIOLATION，该描述表示 JVM Crash 时正在执行 JVM 自身的代码，这往往是因为 JVM 的 Bug 导致的 Crash；另一种常见的描述是EXCEPTION_STACK_OVERFLOW，该描述表示这是个栈溢出导致的错误，这往往是应用程序中存在深层递归导致的。123# Problematic frame:# J 1748 C2 sun.nio.ch.IOUtil.write(Ljava/io/FileDescriptor;[Ljava/nio/ByteBuffer;IILsun/nio/ch/NativeDispatcher;)J (509 bytes) @ 0x00007f79a52ee461 [0x00007f79a52edce0+0x781]# 这个信息比较重要，问题帧信息： C : 表示帧类型为本地帧j ：解释的Java帧V : 虚拟机帧v ：虚拟机生成的存根栈帧J ：其他帧类型，包括编译后的Java帧 导致 crash 的线程信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798--------------- T H R E A D ---------------Current thread (0x00007f79b462c000): JavaThread "nioEventLoopGroup-2-1" [_thread_in_Java, id=18354, stack(0x00007f799cffb000,0x00007f799d0fc000)]//表示导致虚拟机终止的非预期的信号信息siginfo: si_signo: 11 (SIGSEGV), si_code: 1 (SEGV_MAPERR), si_addr: 0x00007f789bc0e010Registers:RAX=0x000000075390acc8, RBX=0x0000000789d35f30, RCX=0x00007f790d624180, RDX=0x0000000000000000RSP=0x00007f799d0fa5a0, RBP=0x0000000000000008, RSI=0x00000000eaf66bf5, RDI=0x0000000003a9c856R8 =0x000000074d36a0f8, R9 =0x0000000000000067, R10=0x00007f789bc0e010, R11=0x00000000f8008092R12=0x0000000000000000, R13=0x00007f79b651b000, R14=0x0000000001000000, R15=0x00007f79b462c000RIP=0x00007f79a52ee461, EFLAGS=0x0000000000010202, CSGSFS=0x0000000000000033, ERR=0x0000000000000006 TRAPNO=0x000000000000000eTop of Stack: (sp=0x00007f799d0fa5a0)0x00007f799d0fa5a0: 000000074d36a0f8 00000000000000000x00007f799d0fa5b0: d9755167d974151e 00007f799d0fa6e80x00007f799d0fa5c0: 000000075390acc8 00007f79a536192c0x00007f799d0fa5d0: 00000000d97580ab 00007f799d0fa6200x00007f799d0fa5e0: 00007f799d0fa630 00000006cbab8a100x00007f799d0fa5f0: 00000006cbc133f0 00000002000733c00x00007f799d0fa600: 00000006cba00668 000733bf000000010x00007f799d0fa610: 00000006cba0a8e0 00007f79a5423da40x00007f799d0fa620: 00000006cba0a860 000000074d36a0f80x00007f799d0fa630: 000733c07e2166e0 00000006cba0a8700x00007f799d0fa640: 00000006cba0a8e0 0000000001801f040x00007f799d0fa650: 00000006000733c0 d974151c7e2168680x00007f799d0fa660: 00000006cbac04b0 00000007000000000x00007f799d0fa670: 0000000000000001 d97580c7000000000x00007f799d0fa680: 0000000000000000 00000006cba85ef80x00007f799d0fa690: 0000000000000031 00000000000000310x00007f799d0fa6a0: 0000000000000000 00000006cba0aa600x00007f799d0fa6b0: 00000006cba0a8f0 00007f79a552c3d40x00007f799d0fa6c0: 00000006cba0a958 0000000001801f040x00007f799d0fa6d0: 000000074d36a0f8 d974150e000733c00x00007f799d0fa6e0: 0000000000000000 000000000000000f0x00007f799d0fa6f0: 00000006cba0a870 0000000025c5eeec0x00007f799d0fa700: 00000006cba0aa60 00007f79a537d62c0x00007f799d0fa710: 00000006cba0a930 00007f79a516830c0x00007f799d0fa720: 00000006cba0a930 00000000d974151e0x00007f799d0fa730: 00000006cba0a8f0 00007f79b462c0000x00007f799d0fa740: 00007f799d0fa770 00007f79bc56196d0x00007f799d0fa750: 0000000000000004 00007f79bbd76eff0x00007f799d0fa760: 0000000000000031 0000000025c6a2c90x00007f799d0fa770: 00000000d974152b 00007f79a55495e40x00007f799d0fa780: 0000000000000400 d9750bed000000320x00007f799d0fa790: 0000608676b9d6c9 0000000000000032Instructions: (pc=0x00007f79a52ee461)0x00007f79a52ee441: f8 0f 85 58 05 00 00 48 63 c9 48 03 4b 10 83 fd0x00007f79a52ee451: 04 0f 84 34 05 00 00 44 8b 50 18 4f 8b 54 d4 180x00007f79a52ee461: 49 89 0a 4d 63 d9 4d 89 5a 08 41 ba 01 00 00 000x00007f79a52ee471: 8b 7c 24 58 e9 fd f8 ff ff 33 c0 e9 51 fe ff ffRegister to memory mapping:RAX=0x000000075390acc8 is an oopsun.nio.ch.IOVecWrapper - klass: 'sun/nio/ch/IOVecWrapper'RBX=0x0000000789d35f30 is an oopRCX=0x00007f790d624180 is an unknown valueRDI=0x0000000003a9c856 is an unknown valueR8 =0x000000074d36a0f8 is an oop[Ljava.nio.ByteBuffer; - klass: 'java/nio/ByteBuffer'[] - length: 16777216R9 =0x0000000000000067 is an unknown valueR10=0x00007f789bc0e010 is an unknown valueR11=0x00000000f8008092 is an unknown valueR12=0x0000000000000000 is an unknown valueR13=0x00007f79b651b000 is an unknown valueR14=0x0000000001000000 is an unknown valueR15=0x00007f79b462c000 is a thread//栈顶程序计数器旁的操作码，它们可以被反汇编成系统崩溃前执行的指令。Stack: [0x00007f799cffb000,0x00007f799d0fc000], sp=0x00007f799d0fa5a0, free space=1021kNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)//线程栈信息。包含了地址、栈顶、栈计数器和线程尚未使用的栈信息。到这里就基本上已经确定了问题所在原因了。Stack: [0x00007f799cffb000,0x00007f799d0fc000], sp=0x00007f799d0fa5a0, free space=1021kNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)J 2132 C2 sun.nio.ch.SocketChannelImpl.write([Ljava/nio/ByteBuffer;II)J (436 bytes) @ 0x00007f79a5423da4 [0x00007f79a5423a20+0x384]J 2159 C2 io.netty.channel.socket.nio.NioSocketChannel.doWrite(Lio/netty/channel/ChannelOutboundBuffer;)V (259 bytes) @ 0x00007f79a552c3d4 [0x00007f79a552bf00+0x4d4]J 2300 C2 io.netty.channel.nio.NioEventLoop.processSelectedKey(Ljava/nio/channels/SelectionKey;Lio/netty/channel/nio/AbstractNioChannel;)V (142 bytes) @ 0x00007f79a516830c [0x00007f79a5168180+0x18c]J 2134% C2 io.netty.channel.nio.NioEventLoop.run()V (221 bytes) @ 0x00007f79a55495e4 [0x00007f79a5548f40+0x6a4]j io.netty.util.concurrent.SingleThreadEventExecutor$5.run()V+44j io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run()V+4j java.lang.Thread.run()V+11v ~StubRoutines::call_stubV [libjvm.so+0x695b96] JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x1056V [libjvm.so+0x6960a1] JavaCalls::call_virtual(JavaValue*, KlassHandle, Symbol*, Symbol*, JavaCallArguments*, Thread*)+0x321V [libjvm.so+0x696537] JavaCalls::call_virtual(JavaValue*, Handle, KlassHandle, Symbol*, Symbol*, Thread*)+0x47V [libjvm.so+0x71596e] thread_entry(JavaThread*, Thread*)+0x7eV [libjvm.so+0xa7f243] JavaThread::thread_main_inner()+0x103V [libjvm.so+0xa7f38c] JavaThread::run()+0x11cV [libjvm.so+0x92e0f8] java_start(Thread*)+0x108C [libpthread.so.0+0x7e25] start_thread+0xc5 这部分内容包含触发 JVM 致命错误的线程详细信息和线程栈12Current thread (0x00007f79b462c000): JavaThread "nioEventLoopGroup-2-1" [_thread_in_Java, id=18354, stack(0x00007f799cffb000,0x00007f799d0fc000)] 0x00007f79b462c000：出错的线程指针 JavaThread：线程类型，可能的类型包括JavaThread：Java线程VMThread : JVM 的内部线程CompilerThread：用来调用JITing，实时编译装卸class 。 通常，jvm会启动多个线程来处理这部分工作，线程名称后面的数字也会累加，例如：CompilerThread1GCTaskThread：执行gc的线程WatcherThread：JVM 周期性任务调度的线程，是一个单例对象ConcurrentMarkSweepThread：jvm在进行CMS GC的时候，会创建一个该线程去进行GC，该线程被创建的同时会创建一个SurrogateLockerThread（简称SLT）线程并且启动它，SLT启动之后，处于等待阶段。CMST开始GC时，会发一个消息给SLT让它去获取Java层Reference对象的全局锁：Lock nioEventLoopGroup-2-1：线程名称 _thread_in_native：当前线程状态。该描述还包含有： _thread_in_native：线程当前状态，状态枚举包括：_thread_uninitialized：线程还没有创建，它只在内存原因崩溃的时候才出现_thread_new：线程已经被创建，但是还没有启动_thread_in_native：线程正在执行本地代码，一般这种情况很可能是本地代码有问题_thread_in_vm：线程正在执行虚拟机代码_thread_in_Java：线程正在执行解释或者编译后的Java代码_thread_blocked：线程处于阻塞状态…_trans：以_trans结尾，线程正处于要切换到其它状态的中间状态 id=18354：线程ID stack(0x00007f799cffb000,0x00007f799d0fc000)：栈区间 所有线程信息1234567891011121314Java Threads: ( =&gt; current thread ) 0x00007f79280ae000 JavaThread "Thread-1" [_thread_blocked, id=18363, stack(0x00007f799caf8000,0x00007f799cbf9000)] 0x00007f7928076800 JavaThread "kafka-producer-network-thread | producer-1" daemon [_thread_in_native, id=18361, stack(0x00007f799cbf9000,0x00007f799ccfa000)] 0x00007f7928011800 JavaThread "threadDeathWatcher-3-1" daemon [_thread_blocked, id=18355, stack(0x00007f799ccfa000,0x00007f799cdfb000)]=&gt;0x00007f79b462c000 JavaThread "nioEventLoopGroup-2-1" [_thread_in_Java, id=18354, stack(0x00007f799cffb000,0x00007f799d0fc000)] 0x00007f79b4268800 JavaThread "Service Thread" daemon [_thread_blocked, id=18341, stack(0x00007f799dedf000,0x00007f799dfe0000)] 0x00007f79b4253000 JavaThread "C1 CompilerThread3" daemon [_thread_blocked, id=18338, stack(0x00007f799dfe1000,0x00007f799e0e1000)] 0x00007f79b4251000 JavaThread "C2 CompilerThread2" daemon [_thread_blocked, id=18336, stack(0x00007f799e0e2000,0x00007f799e1e2000)] 0x00007f79b424f000 JavaThread "C2 CompilerThread1" daemon [_thread_blocked, id=18335, stack(0x00007f799e1e3000,0x00007f799e2e3000)] 0x00007f79b424c000 JavaThread "C2 CompilerThread0" daemon [_thread_blocked, id=18333, stack(0x00007f799e2e4000,0x00007f799e3e4000)] 0x00007f79b424a800 JavaThread "Signal Dispatcher" daemon [_thread_blocked, id=18331, stack(0x00007f799e3e4000,0x00007f799e4e5000)] 0x00007f79b4217800 JavaThread "Finalizer" daemon [_thread_blocked, id=18328, stack(0x00007f799e4e5000,0x00007f799e5e6000)] 0x00007f79b4213000 JavaThread "Reference Handler" daemon [_thread_blocked, id=18325, stack(0x00007f799e5e6000,0x00007f799e6e7000)] 0x00007f79b4009000 JavaThread "main" [_thread_blocked, id=18288, stack(0x00007f79bcf58000,0x00007f79bd058000)] _thread_blocked表示阻塞 安全点和锁信息1VM state:not at safepoint (normal execution) 虚拟机状态。not at safepoint 表示正常运行。其余状态： at safepoint：所有线程都因为虚拟机等待状态而阻塞，等待一个虚拟机操作完成；synchronizing：一个特殊的虚拟机操作，要求虚拟机内的其它线程保持等待状态。1VM Mutex/Monitor currently owned by a thread: None 虚拟机的 Mutex 和 Monitor目前没有被线程持有。Mutex 是虚拟机内部的锁，而 Monitor 则关联到了 Java 对象。 堆信息12345678910Heap: PSYoungGen total 889856K, used 459726K [0x000000076e900000, 0x00000007c0000000, 0x00000007c0000000) eden space 445440K, 64% used [0x000000076e900000,0x000000077ffbb808,0x0000000789c00000) from space 444416K, 39% used [0x0000000789c00000,0x0000000794638000,0x00000007a4e00000) to space 444416K, 0% used [0x00000007a4e00000,0x00000007a4e00000,0x00000007c0000000) ParOldGen total 2669568K, used 2452299K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 91% used [0x00000006cba00000,0x00000007614d2d60,0x000000076e900000) Metaspace used 15147K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576KCard table byte_map: [0x00007f79b9b78000,0x00007f79ba31c000] byte_map_base: 0x00007f79b651b000 新生代、老年代、元空间一目了然。 Card table表示一种卡表，是 jvm 维护的一种数据结构，用于记录更改对象时的引用，以便 gc 时遍历更少的 table 和 root。 本地代码缓存1234CodeCache: size=245760Kb used=5251Kb max_used=5700Kb free=240509Kb bounds [0x00007f79a5000000, 0x00007f79a55b0000, 0x00007f79b4000000] total_blobs=2174 nmethods=1737 adapters=350 compilation: enabled 一块用于编译和保存本地代码的内存。 编译事件1234567891011Compilation events (10 events):Event: 6618.434 Thread 0x00007f79b424f000 2366 4 io.netty.util.internal.PromiseNotificationUtil::tryFailure (66 bytes)Event: 6618.439 Thread 0x00007f79b424f000 nmethod 2366 0x00007f79a53d9110 code [0x00007f79a53d9260, 0x00007f79a53d9328]Event: 6618.486 Thread 0x00007f79b424c000 nmethod 2364 0x00007f79a52d9a50 code [0x00007f79a52d9d60, 0x00007f79a52db770]Event: 6618.722 Thread 0x00007f79b4251000 nmethod 2361% 0x00007f79a54b0550 code [0x00007f79a54b0860, 0x00007f79a54b21f0]Event: 6618.723 Thread 0x00007f79b424f000 2367 ! 4 io.netty.channel.ChannelOutboundBuffer::failFlushed (42 bytes)Event: 6618.726 Thread 0x00007f79b424f000 nmethod 2367 0x00007f79a540c810 code [0x00007f79a540c980, 0x00007f79a540ca78]Event: 6651.937 Thread 0x00007f79b4253000 2368 3 sun.misc.Unsafe::getAndAddLong (27 bytes)Event: 6651.939 Thread 0x00007f79b4253000 nmethod 2368 0x00007f79a5529c10 code [0x00007f79a5529d80, 0x00007f79a5529f90]Event: 6652.498 Thread 0x00007f79b424f000 2369 4 io.netty.util.internal.PlatformDependent0::directBufferAddress (8 bytes)Event: 6652.519 Thread 0x00007f79b424f000 nmethod 2369 0x00007f79a5147050 code [0x00007f79a5147180, 0x00007f79a51471b8] 记录10次编译事件。这里的信息也印证了上面的结论。 gc 相关记录123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103GC Heap History (10 events):Event: 6483.329 GC heap before&#123;Heap before GC invocations=1775 (full 41): PSYoungGen total 889856K, used 433403K [0x000000076e900000, 0x00000007c0000000, 0x00000007c0000000) eden space 445440K, 97% used [0x000000076e900000,0x000000078903ecd8,0x0000000789c00000) from space 444416K, 0% used [0x00000007a4e00000,0x00000007a4e00000,0x00000007c0000000) to space 444416K, 0% used [0x0000000789c00000,0x0000000789c00000,0x00000007a4e00000) ParOldGen total 2669568K, used 2669359K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 99% used [0x00000006cba00000,0x000000076e8cbc50,0x000000076e900000) Metaspace used 15146K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576KEvent: 6534.282 GC heap afterHeap after GC invocations=1775 (full 41): PSYoungGen total 889856K, used 419683K [0x000000076e900000, 0x00000007c0000000, 0x00000007c0000000) eden space 445440K, 94% used [0x000000076e900000,0x00000007882d8de0,0x0000000789c00000) from space 444416K, 0% used [0x00000007a4e00000,0x00000007a4e00000,0x00000007c0000000) to space 444416K, 0% used [0x0000000789c00000,0x0000000789c00000,0x00000007a4e00000) ParOldGen total 2669568K, used 2669358K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 99% used [0x00000006cba00000,0x000000076e8cbbe0,0x000000076e900000) Metaspace used 15146K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576K&#125;Event: 6534.282 GC heap before&#123;Heap before GC invocations=1776 (full 42): PSYoungGen total 889856K, used 419683K [0x000000076e900000, 0x00000007c0000000, 0x00000007c0000000) eden space 445440K, 94% used [0x000000076e900000,0x00000007882d8de0,0x0000000789c00000) from space 444416K, 0% used [0x00000007a4e00000,0x00000007a4e00000,0x00000007c0000000) to space 444416K, 0% used [0x0000000789c00000,0x0000000789c00000,0x00000007a4e00000) ParOldGen total 2669568K, used 2669358K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 99% used [0x00000006cba00000,0x000000076e8cbbe0,0x000000076e900000) Metaspace used 15146K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576KEvent: 6579.155 GC heap afterHeap after GC invocations=1776 (full 42): PSYoungGen total 889856K, used 419683K [0x000000076e900000, 0x00000007c0000000, 0x00000007c0000000) eden space 445440K, 94% used [0x000000076e900000,0x00000007882d8de0,0x0000000789c00000) from space 444416K, 0% used [0x00000007a4e00000,0x00000007a4e00000,0x00000007c0000000) to space 444416K, 0% used [0x0000000789c00000,0x0000000789c00000,0x00000007a4e00000) ParOldGen total 2669568K, used 2669358K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 99% used [0x00000006cba00000,0x000000076e8cbb28,0x000000076e900000) Metaspace used 15146K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576K&#125;Event: 6579.728 GC heap before&#123;Heap before GC invocations=1777 (full 43): PSYoungGen total 889856K, used 445440K [0x000000076e900000, 0x00000007c0000000, 0x00000007c0000000) eden space 445440K, 100% used [0x000000076e900000,0x0000000789c00000,0x0000000789c00000) from space 444416K, 0% used [0x00000007a4e00000,0x00000007a4e00000,0x00000007c0000000) to space 444416K, 0% used [0x0000000789c00000,0x0000000789c00000,0x00000007a4e00000) ParOldGen total 2669568K, used 2669362K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 99% used [0x00000006cba00000,0x000000076e8cc8c8,0x000000076e900000) Metaspace used 15146K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576KEvent: 6618.338 GC heap afterHeap after GC invocations=1777 (full 43): PSYoungGen total 889856K, used 318952K [0x000000076e900000, 0x00000007c0000000, 0x00000007c0000000) eden space 445440K, 71% used [0x000000076e900000,0x000000078207a230,0x0000000789c00000) from space 444416K, 0% used [0x00000007a4e00000,0x00000007a4e00000,0x00000007c0000000) to space 444416K, 0% used [0x0000000789c00000,0x0000000789c00000,0x00000007a4e00000) ParOldGen total 2669568K, used 2669283K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 99% used [0x00000006cba00000,0x000000076e8b8cb0,0x000000076e900000) Metaspace used 15146K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576K&#125;Event: 6620.112 GC heap before&#123;Heap before GC invocations=1778 (full 44): PSYoungGen total 889856K, used 445440K [0x000000076e900000, 0x00000007c0000000, 0x00000007c0000000) eden space 445440K, 100% used [0x000000076e900000,0x0000000789c00000,0x0000000789c00000) from space 444416K, 0% used [0x00000007a4e00000,0x00000007a4e00000,0x00000007c0000000) to space 444416K, 0% used [0x0000000789c00000,0x0000000789c00000,0x00000007a4e00000) ParOldGen total 2669568K, used 2669283K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 99% used [0x00000006cba00000,0x000000076e8b8cb0,0x000000076e900000) Metaspace used 15146K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576KEvent: 6649.833 GC heap afterHeap after GC invocations=1778 (full 44): PSYoungGen total 889856K, used 0K [0x000000076e900000, 0x00000007c0000000, 0x00000007c0000000) eden space 445440K, 0% used [0x000000076e900000,0x000000076e900000,0x0000000789c00000) from space 444416K, 0% used [0x00000007a4e00000,0x00000007a4e00000,0x00000007c0000000) to space 444416K, 0% used [0x0000000789c00000,0x0000000789c00000,0x00000007a4e00000) ParOldGen total 2669568K, used 2452299K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 91% used [0x00000006cba00000,0x00000007614d2d60,0x000000076e900000) Metaspace used 15146K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576K&#125;&#125;Deoptimization events (10 events): ParOldGen total 2669568K, used 2452299K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 91% used [0x00000006cba00000,0x00000007614d2d60,0x000000076e900000) Metaspace used 15147K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576KEvent: 6657.512 GC heap afterHeap after GC invocations=1779 (full 44): PSYoungGen total 889856K, used 174304K [0x000000076e900000, 0x00000007c0000000, 0x00000007c0000000) eden space 445440K, 0% used [0x000000076e900000,0x000000076e900000,0x0000000789c00000) from space 444416K, 39% used [0x0000000789c00000,0x0000000794638000,0x00000007a4e00000) to space 444416K, 0% used [0x00000007a4e00000,0x00000007a4e00000,0x00000007c0000000) ParOldGen total 2669568K, used 2452299K [0x00000006cba00000, 0x000000076e900000, 0x000000076e900000) object space 2669568K, 91% used [0x00000006cba00000,0x00000007614d2d60,0x000000076e900000) Metaspace used 15147K, capacity 15314K, committed 15616K, reserved 1062912K class space used 1858K, capacity 1927K, committed 2048K, reserved 1048576K&#125; jvm 内存映射12345678910111213141516Dynamic libraries:00400000-00401000 r-xp 00000000 fd:00 2149917265 /usr/java/jdk1.8.0_171/bin/java00600000-00601000 rw-p 00000000 fd:00 2149917265 /usr/java/jdk1.8.0_171/bin/java01b84000-01ba5000 rw-p 00000000 00:00 0 [heap]6cba00000-76e900000 rw-p 00000000 00:00 076e900000-7c0000000 rw-p 00000000 00:00 07c0000000-7c0200000 rw-p 00000000 00:00 07c0200000-800000000 ---p 00000000 00:00 07f78a4000000-7f78a7001000 rw-p 00000000 00:00 07f78a7001000-7f78a8000000 ---p 00000000 00:00 07f78a8000000-7f78ab001000 rw-p 00000000 00:00 07f78ab001000-7f78ac000000 ---p 00000000 00:00 07f78ac000000-7f78af001000 rw-p 00000000 00:00 07f78af001000-7f78b0000000 ---p 00000000 00:00 07f78b0000000-7f78b3001000 rw-p 00000000 00:00 0......... 这些信息是虚拟机崩溃时的虚拟内存列表区域。它可以告诉你崩溃原因时哪些类库正在被使用，位置在哪里，还有堆栈和守护页信息。 00400000-00401000：内存区域 r-xp：权限，r/w/x/p/s分别表示读/写/执行/私有/共享 00000000：文件内的偏移量 jvm 启动参数1234567891011VM Arguments:java_command: server-v1.0.jar -d /data/wave/wzm/out3/2018-02/ -s 172.18.20.215 -t wave201802java_class_path (initial): server-v1.0.jarLauncher Type: SUN_STANDARDEnvironment Variables:JAVA_HOME=/usr/java/jdk1.8.0_171JRE_HOME=/usr/java/jdk1.8.0_171/jreCLASSPATH=.:/usr/java/jdk1.8.0_171/lib/dt.jar:/usr/java/jdk1.8.0_171/lib/tools.jar:/usr/java/jdk1.8.0_171/jre/libPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/java/jdk1.8.0_171/bin:/usr/java/jdk1.8.0_171/jre/bin:/root/binSHELL=/bin/bash jvm 虚拟机参数和环境变量 服务器信息12345678--------------- S Y S T E M ---------------OS:CentOS Linux release 7.4.1708 (Core)uname:Linux 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64libc:glibc 2.17 NPTL 2.17rlimit: STACK 8192k, CORE 0k, NPROC 62424, NOFILE 102400, AS infinityload average:19.77 19.23 14.25 参考https://www.jianshu.com/p/7652f931cafd]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之内部类]]></title>
    <url>%2F2019%2F03%2F11%2FJava%E4%B9%8B%E5%86%85%E9%83%A8%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[为什么需要内部类？首先举一个简单的例子，如果你想实现一个接口，但是这个接口中的一个方法和你构想的这个类中的一个方法的名称，参数相同，你应该怎么办？这时候，你可以创建一个内部类实现这个接口。由于内部类对于外部类的所有内容都是可以访问的，所以这样做可以完成你直接实现这个接口的功能。 不过你可能要质疑，更改一下方法不就可以了吗？ 的确，以此作为设计内部类的理由，没有说服力。 真正的原因是这样的，java中的内部类和接口加在一起，可以解决常被C++程序员抱怨java中存在的一个问题 没有多继承。实际上，c++的多继承设计起来很复杂，而java通过内部类+接口，可以很好的实现多继承的效果。 内部类分类 成员内部类：外部类的成员。 局部内部类：定义在一个方法里面甚至一个代码块之内 静态内部类：类似于静态成员。 匿名内部类：实现一个接口 or 继承一个抽象类。 内部类作用 更好的封装性，隐藏你不想让别人知道的操作 内部类成员可以直接访问外部类的私有数据，因为内部类被当成其外部类成员，但外部类不能访问内部类的实现细节，例如内部类的成员变量 匿名内部类适合用于创建那些仅需要一次使用的类 Demo:通过内部类来实例化pojo对象https://blog.csdn.net/cd18333612683/article/details/79129503 局部内部类123456789101112131415161718192021222324252627复制代码public class TestInnerClass&#123; public static void main(String args[])&#123; Goods good = new Goods(); Destination destination = good.dest("beijing"); System.out.println(destination.readLabel()); &#125;&#125;interface Destination&#123; String readLabel();&#125;class Goods&#123; public Destination dest(String s)&#123; class GDestination implements Destination&#123; private String label; public GDestination(String whereTo)&#123; label = whereTo; &#125; public String readLabel()&#123; return label; &#125; &#125; return new GDestination(s); &#125;&#125; 上面就是这样一个例子。在方法dest（）中我们定义了一个内部类，最后由这个方法返回这个内部类。如果我们在创建一个对象的时候仅需要创建一个对象并且创给外部，就可以这样做。当然，定义在方法中的内部类可以使设计多样化，用途绝不仅仅在这一点。 静态内部类静态内部类的使用目的。在定义内部类的时候，可以在其前面加上一个权限修饰符static。此时这个内部类就变为了静态内部类。不过由于种种的原因，如使用上的限制等等因素(具体的使用限制，笔者在下面的内容中会详细阐述)，在实际工作中用的并不是很多。但是并不是说其没有价值。在某些特殊的情况下，少了这个静态内部类还真是不行。如在进行代码程序测试的时候，如果在每一个Java源文件中都设置一个主方法(主方法是某个应用程序的入口，必须具有)，那么会出现很多额外的代码。而且最主要的时这段主程序的代码对于Java文件来说，只是一个形式，其本身并不需要这种主方法。但是少了这个主方法又是万万不行的。在这种情况下，就可以将主方法写入到静态内部类中，从而不用为每个Java源文件都设置一个类似的主方法。这对于代码测试是非常有用的。在一些中大型的应用程序开发中，则是一个常用的技术手段。为此，这个静态内部类虽然不怎么常用，但是程序开发人员还必须要掌握它。也许在某个关键的时刻，其还可以发挥巨大的作用也说不定。 静态内部类的使用限制。 静态成员(包括静态变量与静态成员)的定义。一般情况下，如果一个内部类不是被定义成静态内部类，那么在定义成员变量或者成员方法的时候，是不能够被定义成静态成员变量与静态成员方法的。也就是说，在非静态内部类中不可以声明静态成员。如现在在一个student类中定义了一个内部类age，如果没有将这个类利用static关键字修饰，即没有定义为静态类，那么在这个内部类中如果要利用static关键字来修饰某个成员方法或者成员变量是不允许的。在编译的时候就通不过。故程序开发人员需要注意，只有将某个内部类修饰为静态类，然后才能够在这个类中定义静态的成员变量与成员方法。这是静态内部类都有的一个特性。也正是因为这个原因，有时候少了这个静态的内部类，很多工作就无法完成。或者说要绕一个大圈才能够实现某个用户的需求。这也是静态的内部类之所以要存在的一个重要原因。 在成员的引用上，有比较大的限制。一般的非静态内部类，可以随意的访问外部类中的成员变量与成员方法。即使这些成员方法被修饰为private(私有的成员变量或者方法)，其非静态内部类都可以随意的访问。则是非静态内部类的特权。因为在其他类中是无法访问被定义为私有的成员变量或则方法。但是如果一个内部类被定义为静态的，那么在银用外部类的成员方法或则成员变量的时候，就会有诸多的限制。如不能够从静态内部类的对象中访问外部类的非静态成员(包括成员变量与成员方法)。这是什么意思呢?如果在外部类中定义了两个变量，一个是非静态的变量，一个是静态的变量。那么在静态内部类中，无论在成员方法内部还是在其他地方，都只能够引用外部类中的静态的变量，而不能够访问非静态的变量。在静态内部类中，可以定义静态的方法(也只有在静态的内部类中可以定义静态的方法)，在静态方法中引用外部类的成员。但是无论在内部类的什么地方引用，有一个共同点，即都只能够引用外部类中的静态成员方法或者成员变量。对于那些非静态的成员变量与成员方法，在静态内部类中是无法访问的。这就是静态内部类的最大使用限制。在普通的非静态内部类中是没有这个限制的。也正是这个原因，决定了静态内部类只应用在一些特定的场合。其应用范围远远没有像非静态的内部类那样广泛。 在创建静态内部类时不需要将静态内部类的实例绑定在外部类的实例上。 通常情况下，在一个类中创建成员内部类的时候，有一个强制性的规定，即内部类的实例一定要绑定在外部类的实例中。也就是说，在创建内部类之前要先在外部类中要利用new关键字来创建这个内部类的对象。如此的话如果从外部类中初始化一个内部类对象，那么内部类对象就会绑定在外部类对象上。也就是说，普通非静态内部类的对象是依附在外部类对象之中的。但是，如果成员开发人员创建的时静态内部类，那么这就又另当别论了。通常情况下，程序员在定义静态内部类的时候，是不需要定义绑定在外部类的实例上的。也就是说，要在一个外部类中定义一个静态的内部类，不需要利用关键字new来创建内部类的实例。即在创建静态类内部对象时，不需要其外部类的对象。具体为什么会这样，一般程序开发人员不需要了解这么深入，只需要记住有这个规则即可。在定义静态内部类的时候，千万不要犯画蛇添足的错误。 从以上的分析中可以看出，静态内部类与非静态的内部类还是有很大的不同的。一般程序开发人员可以这么理解，非晶态的内部类对象隐式地在外部类中保存了一个引用，指向创建它的外部类对象。不管怎么理解，程序开发人员都需要牢记静态内部类与非静态内部类的差异。如是否可以创建静态的成员方法与成员变量(静态内部类可以创建静态的成员而非静态的内部类不可以)、对于访问外部类的成员的限制(静态内部类只可以访问外部类中的静态成员变量与成员方法而非静态的内部类即可以访问静态的也可以访问非静态的外部类成员方法与成员变量)。这两个差异是静态内部类与非静态外部类最大的差异，也是静态内部类之所以存在的原因。了解了这个差异之后，程序开发人员还需要知道，在什么情况下该使用静态内部类。如在程序测试的时候，为了避免在各个Java源文件中书写主方法的代码，可以将主方法写入到静态内部类中，以减少代码的书写量，让代码更加的简洁。 参考https://www.cnblogs.com/gaodong/p/3608665.htmlhttps://www.cnblogs.com/Gaojiecai/p/4041663.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之ThreadLocal]]></title>
    <url>%2F2019%2F03%2F11%2FJava%E4%B9%8BThreadLocal%2F</url>
    <content type="text"><![CDATA[什么是ThreadLocalThreadLocal是线程执行时的上下文,用于存放线程局部变量。ThreadLocal 类为每一个线程都维护了自己独有的变量拷贝。每个线程都拥有自己独立的变量，其作用在于数据独立。 ThreadLocal 采用 hash 表的方式来为每个线程提供一个变量的副本 ThreadLocal是本地变量，无法跨jvm传递 ThreadLocalMap 是存放局部变量的容器 Thread中通过变量ThreadLocal.ThreadLocalMap threadLocals来持有ThreadLocalMap的实例 ThreadLocal则是ThreadLocalMap的manager，控制着ThreadLocalMap的创建、存取、删除等工作。 ThreadLocalMapThreadLocalMap和Map接口没有关系，它是使用数组来存储变量的：private Entry[] table,table的初始容量是16，当table的实际长度大于容量时进行成倍扩容，所以table的容量始终是2的幂。 EntryEntry使用ThreadLocal对象作为键，注意不是使用线程(Thread)对象作为键。 WeakReference表示一个对象的弱引用，java将对象的引用按照强弱等级分为四种： 强引用：”Person p = new Person()”代表一个强引用，只要p存在，GC不会回收Person对象。 软引用：SoftReference代表一个软引用，在内存不足将要发生内存溢出时，GC会回收软引用对象。 弱引用：WeakReference代表一个弱引用，其生命周期在下次垃圾回收之前，不管内存足不足，都会被GC回收。 虚引用：PhantomReference代表一个虚引用，无法通过虚引用获取引用对象的值，也被称为”幽灵引用”，它的意义就是检查对象是否被回收。 关于弱引用的一个小栗子:123456789101112import java.lang.ref.WeakReference;public class WeakReferenceTest &#123; public static void main(String[] args) &#123; Object o = new Object(); WeakReference&lt;Object&gt; wof = new WeakReference&lt;Object&gt;(new Object()); System.out.println(wof.get()==null);//false System.out.println(o==null);//false System.gc();// 通知系统GC System.out.println(wof.get()==null);//true System.out.println(o==null);//false &#125;&#125; Entry定义成弱引用的目的是确保没有了ThreadLocal对象的强引用时，能释放ThreadLocalMap中的变量内存。因为ThreadLocalMap是隐藏在内部的，程序员不可见，所以必须要有一个机制能释放ThreadLocalMap对象中的变量内存。12345// 定义ThreadLocalpublic static final ThreadLocal&lt;Session&gt; sessions = new ThreadLocal&lt;Session&gt;();在某个时刻：sessions = null;说明已不使用sessions了，应该释放ThreadLocalMap中的变量内存。 存入逻辑： s1:从当前线程中拿出ThreadLocalMap,如果为空进行创建create，不为空进行值存入转入s2s2:使用ThreadLocal实例作为key和value调用ThreadLocalMap的set方法s3:使用ThreadLocal实例的threadLocalHashCode与table的容量取模，计算值要放入table中的坐标s4:使用这个坐标线性向后探测table，如果发现相同的key则更新值，返回。如果发现实现的key(ThreadLocal实例被GC回收,因为它是WeakReference，所以key为空)，转入s5,未发现相同的key和实现的key，转入s6s5:调用replaceStaleEntry方法清理talbe中key失效的entry，在清理过程中发现相同的key进行值更新，否则新建Entry插入table(坐标为staleSlot)，返回。s6:新建一个Entry插入talbe(坐标为i)，使用i和自增后的长度sz调用cleanSomeSlots做table的连续段清理，转入s7s7:清理之后发现table长度大于等于扩容阈值threshold，进行table扩容 源码 123456789101112131415161718192021222324252627282930313233public void set(T value) &#123; Thread t = Thread.currentThread(); // getMap(t):t.threadLocals,s1 从当前线程中拿出ThreadLocalMap ThreadLocalMap map = getMap(t); if (map != null)// map 不为空set map.set(this, value); // s2 ThreadLocal的实例(this)作为key else // map为空create createMap(t, value);&#125;private void set(ThreadLocal key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1);// s3 hash计算出table坐标 // 线性探测 for (Entry e = tab[i];e != null;e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal k = e.get(); // 找到对应的entry if (k == key) &#123; e.value = value;// 更新值 return; &#125; // 替换失效的entry if (k == null) &#123; replaceStaleEntry(key, value, i);//清理 return; &#125; &#125; tab[i] = new Entry(key, value);// 插入新值 int sz = ++size; // 长度增加 // table连续清理，并判断是否扩容 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();// 扩容table，并重新hash&#125; 扩容逻辑： s1:进行一次全量的清理(清理对应ThreadLocal已经被GC回收的entry)s2:因为进行了一次清理，所以talbe的长度会变小，改变扩容的阈值，由原来的2/3改为1/2，如果table长度大于等于阈值，扩容转入s3s3:新建一个数组，容量是table容量的2倍，数组拷贝,首先使用hash算法生成entry在新数组中的坐标，如果发生碰撞，使用线性探测重新确定坐标 代码：1234567891011121314151617181920212223242526272829303132private void rehash() &#123; expungeStaleEntries(); // s1 做一次全量清理 // s2 size很可能会变小调低阈值来判断是否需要扩容 if (size &gt;= threshold - threshold / 4) resize();// 扩容&#125;private void resize() &#123; // s3 Entry[] oldTab = table; int oldLen = oldTab.length;// 原来的容量 int newLen = oldLen * 2; // 扩容两倍 Entry[] newTab = new Entry[newLen];// 新数组 int count = 0; for (int j = 0; j &lt; oldLen; ++j) &#123;// 拷贝 Entry e = oldTab[j]; if (e != null) &#123; ThreadLocal k = e.get(); if (k == null) &#123; // key失效，被回收 e.value = null; // 帮助GC &#125; else &#123; // Hash获取元素的坐标 int h = k.threadLocalHashCode &amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); // 线性探测 获取坐标 newTab[h] = e; count++; &#125; &#125; &#125; setThreshold(newLen);// 设置新的扩容阈值 size = count; table = newTab;&#125; 魔数HASH_INCREMENT = 0x61c88647这个数字和斐波那契散列有关(数学问题感兴趣可以深入研究)，通过这个数字可以得到均匀的散列码。 一个小栗子:12345678910111213public class Hash &#123; private static AtomicInteger nextHashCode = new AtomicInteger(); private static final int HASH_INCREMENT = 0x61c88647; private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT); &#125; public static void main(String[] args) &#123; int length = 32; for(int i=0;i&lt;n;i++) &#123; System.out.println(nextHashCode()&amp;(length-1)); &#125; &#125;&#125; 会发现生成的散列码非常均匀，如果把length改为31就会发现得到的散列码不那么均匀了。 length-1的二进制表示就是低位连续的N个1,nextHashCode()&amp;(length-1)的值就是nextHashCode()的低N位, 这样就能均匀的产生均匀的分布,这是为什么ThreadLocalMap中talbe的容量必须为2的幂的原因。 取值逻辑： s1:从当前线程中拿出ThreadLocalMap,如果为空进行初始化设置setInitialValue，不为空，使用ThreadLocal实例作为key从ThreadLocalMap中取值，转入s2s2:使用hash算法生成key对应entry在table中的坐标i,如果table[i]对应的entry不为空且key未失效，说明命中直接返回，否则转入s3s3:线性探测table,如果发现相同的key，返回。如果发现失效的key，调用expungeStaleEntry清理talbe,探测完毕返回null源码：123456789101112131415161718192021222324252627282930313233343536public T get() &#123; Thread t = Thread.currentThread();// 当前线程 ThreadLocalMap map = getMap(t);// 拿出ThreadLocalMap if (map != null) &#123; // s1 ThreadLocalMap不为空 ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) return (T)e.value; &#125; return setInitialValue();// ThreadLocalMap为空&#125;private Entry getEntry(ThreadLocal key) &#123; int i = key.threadLocalHashCode &amp; (table.length - 1); // hash坐标 Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) // s2 key有效，命中返回 return e; else return getEntryAfterMiss(key, i, e); // 线性探测，继续查找&#125;private Entry getEntryAfterMiss(ThreadLocal key, int i, Entry e) &#123; // s3 Entry[] tab = table; int len = tab.length; while (e != null) &#123; ThreadLocal k = e.get(); if (k == key) // 找到目标 return e; if (k == null) // entry对应的ThreadLocal已经被回收，清理无效entry expungeStaleEntry(i); else i = nextIndex(i, len); // 往后走 e = tab[i]; &#125; return null;&#125; 删除源码：12345678910111213141516171819public void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread());// 从当前线程拿出ThreadLocalMap if (m != null) m.remove(this);// 删除，key为ThreadLocal实例&#125;private void remove(ThreadLocal key) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1);// hash定位 for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; if (e.get() == key) &#123; e.clear();// 断开弱引用 expungeStaleEntry(i);// 从i开始，进行段清理 return; &#125; &#125;&#125; 怎么防止内存泄露通过上文可以看到ThreadLocal为应对内存泄露做的工作： 将Entry定义成弱引用，如果ThreadLocal实例不存在强引用了那么Entry的key就会失效 get()、set()方法都进行失效key的清理即便是这样也不能保证万无一失： 通常情况下为了使线程可以共用ThreadLocal，会这样定义：static final ThreadLocal threadLocal,这样static变量的生命周期是随class一起的，所以它永远不会被GC回收，这个强引用在key就不会失效。 不使用static定义threadLocal，由于有一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value的存在，在长生命周期的线程中(比如线程池)也有内存泄露的风险。短生命周期的线程则无所谓，因为随着线程生命周期的结束，一切都烟消云散。当然这并不可怕，只要在使用完threadLocal后调用下remove()方法，清除数据，就可以了。 秒杀中ThreadLocal的应用12345678910111213141516package com.sjt.miaosha.access;import com.sjt.miaosha.entity.User;public class UserContext &#123; //每一个线程都对应自己的threadlocal private static ThreadLocal&lt;User&gt; userHolder = new ThreadLocal&lt;User&gt;(); public static void setUser(User user) &#123; userHolder.set(user); &#125; public static User getUser() &#123; return userHolder.get(); &#125;&#125; 实例中用thradlocal保存每个用户上下文。1234567891011121314151617181920212223242526272829303132333435363738public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; if(handler instanceof HandlerMethod) &#123; User user = getuser(request, response); //得到用户信息 UserContext.setUser(user); //放入Usercontext中，里边是Threadlocal HandlerMethod hm = (HandlerMethod) handler; AccessLimit accessLimit = hm.getMethodAnnotation(AccessLimit.class); if(accessLimit == null) &#123; return true; &#125; int seconds = accessLimit.seconds(); int maxCount = accessLimit.maxCount(); boolean needLogin = accessLimit.needLogin(); String key = request.getRequestURI(); if(needLogin) &#123; if(user == null) &#123; render(response, CodeMsg.SESSION_ERROR); return false; &#125; key += "_" + user.getId(); &#125;else &#123; // do nothing &#125; AccessKey ak = AccessKey.withExpire(seconds); Integer count = redisService.get(ak, key, Integer.class); if(count == null) &#123; redisService.set(ak, key, 1); &#125;else if(count &lt; maxCount)&#123; redisService.incr(ak, key); &#125;else &#123; render(response, CodeMsg.ACCESS_LIMIT_REACHED); return false; &#125; &#125; return true; &#125; Hibernate中的OpenSessionInViewThreadLocal的出现可以减少通过参数来传递（使代码更加简洁，降低耦合性），Hibernate中的OpenSessionInView，就始终保证当前线程只有一个在使用中的Connection（或Hibernate Session），代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041public class ConnectionManager &#123; /** 线程内共享Connection，ThreadLocal通常是全局的，支持泛型 */ private static ThreadLocal&lt;Connection&gt; threadLocal = new ThreadLocal&lt;Connection&gt;(); public static Connection getCurrConnection() &#123; // 获取当前线程内共享的Connection Connection conn = threadLocal.get(); try &#123; // 判断连接是否可用 if(conn == null || conn.isClosed()) &#123; // 创建新的Connection赋值给conn(略) // 保存Connection threadLocal.set(conn); &#125; &#125; catch (SQLException e) &#123; // 异常处理 &#125; return conn; &#125; /** * 关闭当前数据库连接 */ public static void close() &#123; // 获取当前线程内共享的Connection Connection conn = threadLocal.get(); try &#123; // 判断是否已经关闭 if(conn != null &amp;&amp; !conn.isClosed()) &#123; // 关闭资源 conn.close(); // 移除Connection threadLocal.remove(); conn = null; &#125; &#125; catch (SQLException e) &#123; // 异常处理 &#125; &#125; &#125; ThreadLocal和同步机制synchonzied相比 synchonzied同步机制是为了实现同步多线程对相同资源的并发访问控制。同步的主要目的是保证多线程间的数据共享。同步会带来巨大的性能开销，所以同步操作应该是细粒度的（对象中的不同元素使用不同的锁，而不是整个对象一个锁）。如果同步使用得当，带来的性能开销是微不足道的。使用同步真正的风险是复杂性和可能破坏资源安全,而不是性能。 ThreadLocal以空间换取时间，提供了一种非常简便的多线程实现方式。因为多个线程并发访问无需进行等待，所以使用ThreadLocal会获得更大的性能。 ThreadLocal中的对象，通常都是比较小的对象。另外使用ThreadLocal不能使用原子类型，只能使用Object类型。ThreadLocal的使用比synchronized要简单得多。 synchronized是利用锁的机制，使变量或代码块在某一时该只能被一个线程访问。而ThreadLocal为每一个线程都提供了变量的副本，使得每个线程在某一时间访问到的并不是同一个对象，这样就隔离了多个线程对数据的数据共享。而Synchronized却正好相反，它用于在多个线程间通信时能够获得数据共享。 Synchronized用于线程间的数据共享，而ThreadLocal则用于线程间的数据隔离。 在线程池中使用ThreadLocal需要注意 线程池由于未创建新的线程，导致线程变量也是之前的内容。 解决办法:一、直接在进入线程时remove操作。 二、使用后清空。https://blog.csdn.net/qq_33522040/article/details/85322094 ThreadLocal可以为当前线程保存局部变量，而InheritableThreadLocal则可以在创建子线程的时候将父线程的局部变量传递到子线程中。 如果使用了线程池(如Executor)，那么即使即使父线程已经结束，子线程依然存在并被池化。这样，线程池中的线程在下一次请求被执行的时候，ThreadLocal对象的get()方法返回的将不是当前线程中设定的变量，因为池中的“子线程”根本不是当前线程创建的，当前线程设定的ThreadLocal变量也就无法传递给线程池中的线程。因此，必须将外部线程中的ThreadLocal变量显式地传递给线程池中的线程。https://blog.csdn.net/comliu/article/details/3186778?utm_source=blogxgwz2 Netty的FastThreadLocal参考我的博客 并发包中ThreadLocalRandom类Random在多线程下存在竞争种子原子变量更新操作失败后自旋等待的缺点，从而引出ThreadLocalRandom类，ThreadLocalRandom使用ThreadLocal的原理，让每个线程内持有一个本地的种子变量，该种子变量只有在使用随机数时候才会被初始化，多线程下计算新种子时候是根据自己线程内维护的种子变量进行更新，从而避免了竞争。https://blog.csdn.net/wangyunpeng0319/article/details/78903541 小结 ThreadLocal是线程执行时的上下文,用于存放线程局部变量。它不能解决并发情况下数据共享的问题 ThreadLocal是以ThreadLocal对象本身作为key的，不是线程(Thread)对象 ThreadLocal存在内存泄露的风险，要养成用完即删的习惯 ThreadLocal使用散列定位数据存储坐标，如果发生碰撞，使用线性探测重新定位，这在高并发场景下会影响一点性能。改善方法如netty的FastThreadLocal，使用固定坐标，以空间换时间，后面会分析FastThreadLocal实现。 参考https://www.cnblogs.com/handsomeye/p/5390720.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之seccomp]]></title>
    <url>%2F2019%2F03%2F11%2FDocker%E4%B9%8Bseccomp%2F</url>
    <content type="text"><![CDATA[什么是seccompseccomp（全称securecomputing mode）是linux kernel从2.6.23版本开始所支持的一种安全机制。 在Linux系统里，大量的系统调用（systemcall）直接暴露给用户态程序。但是，并不是所有的系统调用都被需要，而且不安全的代码滥用系统调用会对系统造成安全威胁。通过seccomp，我们限制程序使用某些系统调用，这样可以减少系统的暴露面，同时是程序进入一种“安全”的状态。 如何使用seccompseccomp可以通过系统调用ptrctl(2)或者通过系统调用seccomp(2)开启，前提是内核配置中开启了CONFIG_SECCOMP和CONFIG_SECCOMP_FILTER。 seccomp支持两种模式：SECCOMP_MODE_STRICT和SECCOMP_MODE_FILTER。 在SECCOMP_MODE_STRICT模式下，进程不能使用read(2)，write(2)，_exit(2)和sigreturn(2)以外的其他系统调用。 在SECCOMP_MODE_FILTER模式下，可以利用BerkeleyPacket Filter配置哪些系统调用及它们的参数可以被进程使用。 如何查看是否使用了seccomp通常有两种方法： 利用prctl(2)的PR_GET_SECCOMP的参数获取当前进程的seccomp状态。返回值0表示没有使用seccomp;返回值2表示使用了seccomp并处于SECCOMP_MODE_FILTER模式；其他情况进程会被SIGKILL信号杀死。 从Linux3.8开始，可以利用/proc/[pid]/status中的Seccomp字段查看。如果没有seccomp字段，说明内核不支持seccomp。 下面这个是docker守护进程的status123456789101112131415161718192021222324252627282930313233343536373839404142434445464748Name: dockerdState: S (sleeping)Tgid: 1345Ngid: 0Pid: 1345PPid: 1TracerPid: 0Uid: 0 0 0 0Gid: 0 0 0 0FDSize: 256Groups: NStgid: 1345NSpid: 1345NSpgid: 1345NSsid: 1345VmPeak: 1150756 kBVmSize: 1150628 kBVmLck: 0 kBVmPin: 0 kBVmHWM: 102020 kBVmRSS: 87212 kBVmData: 1049204 kBVmStk: 132 kBVmExe: 46768 kBVmLib: 4612 kBVmPTE: 484 kBVmPMD: 24 kBVmSwap: 0 kBHugetlbPages: 0 kBThreads: 36SigQ: 0/62554SigPnd: 0000000000000000ShdPnd: 0000000000000000SigBlk: fffffffe3bfa2800SigIgn: 0000000000000000SigCgt: ffffffffffc1feffCapInh: 0000000000000000CapPrm: 0000003fffffffffCapEff: 0000003fffffffffCapBnd: 0000003fffffffffCapAmb: 0000000000000000Seccomp: 0 //这是0，表示没有使用seccompCpus_allowed: ffCpus_allowed_list: 0-7Mems_allowed: 00000000,00000001Mems_allowed_list: 0voluntary_ctxt_switches: 2334nonvoluntary_ctxt_switches: 4 在docker中使用只有在使用 seccomp 构建 Docker 并且内核配置了 CONFIG_SECCOMP 的情况下，此功能才可用。要检查你的内核是否支持 seccomp：12$ cat /boot/config-`uname -r` | grep CONFIG_SECCOMP=CONFIG_SECCOMP=y 为容器传递配置文件默认的 seccomp 配置文件为使用 seccomp 运行容器提供了一个合理的设置，并禁用了大约 44 个超过 300+ 的系统调用。它具有适度的保护性，同时提供广泛的应用兼容性。默认的 Docker 配置文件可以在 这里 找到。https://github.com/moby/moby/blob/master/profiles/seccomp/default.json 实际上，该配置文件是白名单，默认情况下阻止访问所有的系统调用，然后将特定的系统调用列入白名单。该配置文件工作时需要定义 SCMP_ACT_ERRNO 的 defaultAction 并仅针对特定的系统调用覆盖该 action。SCMP_ACT_ERRNO 的影响是触发 Permission Denied 错误。接下来，配置文件中通过将 action 被覆盖为 SCMP_ACT_ALLOW，定义一个完全允许的系统调用的特定列表。最后，一些特定规则适用于个别的系统调用，如 personality，socket，socketcall 等，以允许具有特定参数的那些系统调用的变体（to allow variants of those system calls with specific arguments）。 seccomp 有助于以最小权限运行 Docker 容器。不建议更改默认的 seccomp 配置文件。 运行容器时，如果没有通过 –security-opt 选项覆盖容器，则会使用默认配置。例如，以下显式指定了一个策略：1234docker run --rm \ -it \ --security-opt seccomp=/path/to/seccomp/profile.json \ hello-world 不使用默认的 seccomp 配置文件可以传递 unconfined 以运行没有默认 seccomp 配置文件的容器。 12docker run --rm -it --security-opt seccomp=unconfined debian:jessie \ unshare --map-root-user --user sh -c whoami 参考https://blog.csdn.net/mashimiao/article/details/73607485https://blog.csdn.net/kikajack/article/details/79596843]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户态与内核态]]></title>
    <url>%2F2019%2F03%2F11%2F%E7%94%A8%E6%88%B7%E6%80%81%E4%B8%8E%E5%86%85%E6%A0%B8%E6%80%81%2F</url>
    <content type="text"><![CDATA[内核态与用户态当一个任务（进程）执行系统调用而陷入内核代码中执行时，我们就称进程处于内核运行态（或简称为内核态）。此时处理器处于特权级最高的（0级）内核代码中执行。当进程处于内核态时，执行的内核代码会使用当前进程的内核栈。每个进程都有自己的内核栈。当进程在执行用户自己的代码时，则称其处于用户运行态（用户态）。即此时处理器在特权级最低的（3级）用户代码中运行。当正在执行用户程序而突然被中断程序中断时，此时用户程序也可以象征性地称为处于进程的内核态。因为中断处理程序将使用当前进程的内核栈。这与处于内核态的进程的状态有些类似。 用系统调用时进入核心态。Linux对硬件的操作只能在核心态，这可以通过写驱动程序来控制。在用户态操作硬件会造成core dump。 要注意区分系统调用和一般的函数。系统调用由内核提供，如read()、write()、open()等。而一般的函数由软件包中的函数库提供，如sin()、cos()等。在语法上两者没有区别。 一般情况：系统调用运行在核心态，函数运行在用户态。但也有一些函数在内部使用了系统调用（如fopen），这样的函数在调用系统调用是进入核心态，其他时候运行在用户态。 大概是当用户程序调用系统的API时，就产生中断，进入内核态的API,处理完成后，用中断再退出，返回用户态的调用函数。 user api --&gt; interrupt --&gt; kernel api --&gt; interrupt 简单来讲一个进程由于执行系统调用而开始执行内核代码,我们称该进程处于内核态中. 一个进程执行应用程序自身代码则称该进程处于用户态. CPU运行级别intel x86 架构的 CPU 分为好几个运行级别,从 0–3 , 0 为最高级别, 3 为最低级别针对不同的级别,有很多的限制,比如说传统的 in ,out 指令,就是端口的输入输出指令,在 0 级下是可以用的,但在 3 级下就不能用,你用就产生陷阱,告诉你出错了,当然限制还有很多了,不只是这一点。操作系统下是利用这个特点,当操作系统自己的代码运行时, CPU 就切成 0 级,当用户的程序运行是就只让它在 3 级运行,这样如果用户的程序想做什么破坏系统的事情的话,也没办法做到当然,低级别的程序是没法把自己升到高级别的,也就是说 用户程序运行在 3 级,他想把自己变成 0 级自己是做不到的,除非是操作系统帮忙,利用这个特性,操作系统就可以控制所有的程序的运行,确保系统的安全了. 平时把操作系统运行时的级别就叫内核态(因为是操作系统内核运行时的状态),而且普通用户程序运行时的那个级别叫用户态…当操作系统刚引导时, CPU 处于实模式,这时就相当于是 0 级,于是操作系统就自动得到最高权限,然后切到保护模式时就是0级,这时操作系统就占了先机,成为了最高级别的运行者,由于你的程序都是由操作系统来加载的,所以当它把你加载上来后,就把你的运行状态设为 3 级,即最低级,然后才让你运行,所以没办法,你只能在最低级运行了,因为没办法把自己从低级上升到高级, 这就是操作系统在内核态可以管理用户程序,杀死用户程序的原因。 用户态和内核态的概念区别 究竟什么是用户态，什么是内核态，这两个基本概念以前一直理解得不是很清楚，根本原因个人觉得是在于因为大部分时候我们在写程序时关注的重点和着眼的角度放在了实现的功能和代码的逻辑性上，先看一个例子： 1）例子12345678910111213 void testfork()&#123; if(0 = = fork())&#123; printf(“create new process success!\n”); &#125; printf(“testfork ok\n”); &#125;这段代码很简单，从功能的角度来看，就是实际执行了一个fork()，生成一个新的进程，从逻辑的角度看，就是判断了如果fork()返回的是0则打印相关语句，然后函数最后再打印一句表示执行完整个testfork()函数。代码的执行逻辑和功能上看就是如此简单，一共四行代码，从上到下一句一句执行而已，完全看不出来哪里有体现出用户态和进程态的概念。 如果说前面两种是静态观察的角度看的话，我们还可以从动态的角度来看这段代码，即它被转换成CPU执行的指令后加载执行的过程，这时这段程序就是一个动态执行的指令序列。而究竟加载了哪些代码，如何加载就是和操作系统密切相关了。 2）特权级 熟悉Unix/Linux系统的人都知道，fork的工作实际上是以系统调用的方式完成相应功能的，具体的工作是由sys_fork负责实施。其实无论是不是Unix或者Linux，对于任何操作系统来说，创建一个新的进程都是属于核心功能，因为它要做很多底层细致地工作，消耗系统的物理资源，比如分配物理内存，从父进程拷贝相关信息，拷贝设置页目录页表等等，这些显然不能随便让哪个程序就能去做，于是就自然引出特权级别的概念，显然，最关键性的权力必须由高特权级的程序来执行，这样才可以做到集中管理，减少有限资源的访问和使用冲突。 特权级显然是非常有效的管理和控制程序执行的手段，因此在硬件上对特权级做了很多支持，就Intel x86架构的CPU来说一共有0~3四个特权级，0级最高，3级最低，硬件上在执行每条指令时都会对指令所具有的特权级做相应的检查，相关的概念有CPL、DPL和RPL，这里不再过多阐述。硬件已经提供了一套特权级使用的相关机制，软件自然就是好好利用的问题，这属于操作系统要做的事情，对于Unix/Linux来说，只使用了0级特权级和3级特权级。也就是说在Unix/Linux系统中，一条工作在0级特权级的指令具有了CPU能提供的最高权力，而一条工作在3级特权级的指令具有CPU提供的最低或者说最基本权力。 3）用户态和内核态 现在我们从特权级的调度来理解用户态和内核态就比较好理解了，当程序运行在3级特权级上时，就可以称之为运行在用户态，因为这是最低特权级，是普通的用户进程运行的特权级，大部分用户直接面对的程序都是运行在用户态；反之，当程序运行在0级特权级上时，就可以称之为运行在内核态。 虽然用户态下和内核态下工作的程序有很多差别，但最重要的差别就在于特权级的不同，即权力的不同。运行在用户态下的程序不能直接访问操作系统内核数据结构和程序，比如上面例子中的testfork()就不能直接调用sys_fork()，因为前者是工作在用户态，属于用户态程序，而sys_fork()是工作在内核态，属于内核态程序。 当我们在系统中执行一个程序时，大部分时间是运行在用户态下的，在其需要操作系统帮助完成某些它没有权力和能力完成的工作时就会切换到内核态，比如testfork()最初运行在用户态进程下，当它调用fork()最终触发sys_fork()的执行时，就切换到了内核态。 用户态和内核态的转换 1）用户态切换到内核态的3种方式 a. 系统调用 这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，比如前例中fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。 b. 异常 当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。 c. 外围设备的中断 当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。 这3种方式是系统在运行时由用户态转到内核态的最主要方式，其中系统调用可以认为是用户进程主动发起的，异常和外围设备中断则是被动的。 参考https://www.cnblogs.com/cyjaysun/p/4422508.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式锁]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[什么场景下会使用分布式锁？单机应用架构中，秒杀案例使用ReentrantLcok或者synchronized来达到秒杀商品互斥的目的。然而在分布式系统中，会存在多台机器并行去实现同一个功能。也就是说，在多进程中，如果还使用以上JDK提供的进程锁，来并发访问数据库资源就可能会出现商品超卖的情况。因此，需要我们来实现自己的分布式锁。 分布式锁的缺点比如你用zookeeper实现了分布式锁，他有一个缺点就是并发量上不去，因为他是串行的嘛，如果说可以优化的话(优化就会带来复杂性)，可以借鉴ConcurrentHashMap的分段锁的思想，比如1000个库存，分成20个段，每段50个库存。其实最好的是通过redis原子操作加异步队列 实现一个分布式锁应该具备的特性： 高可用、高性能的获取锁与释放锁 在分布式系统环境下，一个方法或者变量同一时间只能被一个线程操作 具备锁失效机制，网络中断或宕机无法释放锁时，锁必须被删除，防止死锁 具备阻塞锁特性，即没有获取到锁，则继续等待获取锁 具备非阻塞锁特性，即没有获取到锁，则直接返回获取锁失败 具备可重入特性，一个线程中可以多次获取同一把锁，比如一个线程在执行一个带锁的方法，该方法中又调用了另一个需要相同锁的方法，则该线程可以直接执行调用的方法，而无需重新获得锁 关于分布式锁几种实现方式： 基于数据库实现分布式锁 基于 Redis 实现分布式锁 基于 Zookeeper 实现分布式锁 Zookeeper实现分布式锁前两种对于分布式生产环境来说并不是特别推荐，高并发下数据库锁性能太差，Redis在锁时间限制和缓存一致性存在一定问题。这里我们重点介绍一下 Zookeeper 如何实现分布式锁。 实现原理ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，它内部是一个分层的文件系统目录树结构，规定同一个目录下只能存在唯一文件名。 ZooKeeper数据模型与文件系统目录树(源自网络) 数据模型PERSISTENT 持久化节点，节点创建后，不会因为会话失效而消失EPHEMERAL 临时节点， 客户端session超时此类节点就会被自动删除EPHEMERAL_SEQUENTIAL 临时自动编号节点PERSISTENT_SEQUENTIAL 顺序自动编号持久化节点，这种节点会根据当前已存在的节点数自动加 1 监视器（watcher）当创建一个节点时，可以注册一个该节点的监视器，当节点状态发生改变时，watch被触发时，ZooKeeper将会向客户端发送且仅发送一条通知，因为watch只能被触发一次。根据zookeeper的这些特性，我们来看看如何利用这些特性来实现分布式锁： 创建一个锁目录lock 线程A获取锁会在lock目录下，创建临时顺序节点 获取锁目录下所有的子节点，然后获取比自己小的兄弟节点，如果不存在，则说明当前线程顺序号最小，获得锁 线程B创建临时节点并获取所有兄弟节点，判断自己不是最小节点，设置监听(watcher)比自己次小的节点 线程A处理完，删除自己的节点，线程B监听到变更事件，判断自己是最小的节点，获得锁 代码分析尽管ZooKeeper已经封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。但是如果让一个普通开发者去手撸一个分布式锁还是比较困难的，在秒杀案例中我们直接使用 Apache 开源的curator 开实现 Zookeeper 分布式锁。这里我们使用以下版本，截止目前最新版4.0.1：123456&lt;!-- zookeeper 分布式锁、注意zookeeper版本 这里对应的是3.4.6--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.10.0&lt;/version&gt;&lt;/dependency&gt; 首先，我们看下InterProcessLock接口中的几个方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217/*** 获取锁、阻塞等待、可重入*/public void acquire() throws Exception;/*** 获取锁、阻塞等待、可重入、超时则获取失败*/public boolean acquire(long time, TimeUnit unit) throws Exception;/*** 释放锁*/public void release() throws Exception;/*** Returns true if the mutex is acquired by a thread in this JVM*/boolean isAcquiredInThisProcess();获取锁：//获取锁public void acquire() throws Exception &#123; if ( !internalLock(-1, null) ) &#123; throw new IOException("Lost connection while trying to acquire lock: " + basePath); &#125; &#125;具体实现：private boolean internalLock(long time, TimeUnit unit) throws Exception &#123; /* 实现同一个线程可重入性，如果当前线程已经获得锁， 则增加锁数据中lockCount的数量(重入次数)，直接返回成功 */ //获取当前线程 Thread currentThread = Thread.currentThread(); //获取当前线程重入锁相关数据 LockData lockData = threadData.get(currentThread); if ( lockData != null ) &#123; //原子递增一个当前值，记录重入次数，后面锁释放会用到 lockData.lockCount.incrementAndGet(); return true; &#125; //尝试连接zookeeper获取锁 String lockPath = internals.attemptLock(time, unit, getLockNodeBytes()); if ( lockPath != null ) &#123; //创建可重入锁数据，用于记录当前线程重入次数 LockData newLockData = new LockData(currentThread, lockPath); threadData.put(currentThread, newLockData); return true; &#125; //获取锁超时或者zk通信异常返回失败 return false; &#125;Zookeeper获取锁实现： String attemptLock(long time, TimeUnit unit, byte[] lockNodeBytes) throws Exception &#123; //获取当前时间戳 final long startMillis = System.currentTimeMillis(); //如果unit不为空(非阻塞锁)，把当前传入time转为毫秒 final Long millisToWait = (unit != null) ? unit.toMillis(time) : null; //子节点标识 final byte[] localLockNodeBytes = (revocable.get() != null) ? new byte[0] : lockNodeBytes; //尝试次数 int retryCount = 0; String ourPath = null; boolean hasTheLock = false; boolean isDone = false; //自旋锁，循环获取锁 while ( !isDone ) &#123; isDone = true; try &#123; //在锁节点下创建临时且有序的子节点，例如:_c_008c1b07-d577-4e5f-8699-8f0f98a013b4-lock-000000001 ourPath = driver.createsTheLock(client, path, localLockNodeBytes); //如果当前子节点序号最小，获得锁则直接返回，否则阻塞等待前一个子节点删除通知(release释放锁) hasTheLock = internalLockLoop(startMillis, millisToWait, ourPath); &#125; catch ( KeeperException.NoNodeException e ) &#123; //异常处理，如果找不到节点，这可能发生在session过期等时，因此，如果重试允许，只需重试一次即可 if ( client.getZookeeperClient().getRetryPolicy().allowRetry(retryCount++, System.currentTimeMillis() - startMillis, RetryLoop.getDefaultRetrySleeper()) ) &#123; isDone = false; &#125; else &#123; throw e; &#125; &#125; &#125; //如果获取锁则返回当前锁子节点路径 if ( hasTheLock ) &#123; return ourPath; &#125; return null; &#125;判断是否为最小节点： private boolean internalLockLoop(long startMillis, Long millisToWait, String ourPath) throws Exception &#123; boolean haveTheLock = false; boolean doDelete = false; try &#123; if ( revocable.get() != null ) &#123; client.getData().usingWatcher(revocableWatcher).forPath(ourPath); &#125; //自旋获取锁 while ( (client.getState() == CuratorFrameworkState.STARTED) &amp;&amp; !haveTheLock ) &#123; //获取所有子节点集合 List&lt;String&gt; children = getSortedChildren(); //判断当前子节点是否为最小子节点 String sequenceNodeName = ourPath.substring(basePath.length() + 1); // +1 to include the slash PredicateResults predicateResults = driver.getsTheLock(client, children, sequenceNodeName, maxLeases); //如果是最小节点则获取锁 if ( predicateResults.getsTheLock() ) &#123; haveTheLock = true; &#125; else &#123; //获取前一个节点，用于监听 String previousSequencePath = basePath + "/" + predicateResults.getPathToWatch(); synchronized(this) &#123; try &#123; //这里使用getData()接口而不是checkExists()是因为，如果前一个子节点已经被删除了那么会抛出异常而且不会设置事件监听器，而checkExists虽然也可以获取到节点是否存在的信息但是同时设置了监听器，这个监听器其实永远不会触发，对于Zookeeper来说属于资源泄露 client.getData().usingWatcher(watcher).forPath(previousSequencePath); if ( millisToWait != null ) &#123; millisToWait -= (System.currentTimeMillis() - startMillis); startMillis = System.currentTimeMillis(); //如果设置了获取锁等待时间 if ( millisToWait &lt;= 0 ) &#123; doDelete = true; // 超时则删除子节点 break; &#125; //等待超时时间 wait(millisToWait); &#125; else &#123; wait();//一直等待 &#125; &#125; catch ( KeeperException.NoNodeException e ) &#123; // it has been deleted (i.e. lock released). Try to acquire again //如果前一个子节点已经被删除则deException，只需要自旋获取一次即可 &#125; &#125; &#125; &#125; &#125; catch ( Exception e ) &#123; ThreadUtils.checkInterrupted(e); doDelete = true; throw e; &#125; finally &#123; if ( doDelete ) &#123; deleteOurPath(ourPath);//获取锁超时则删除节点 &#125; &#125; return haveTheLock; &#125;释放锁： public void release() throws Exception &#123; Thread currentThread = Thread.currentThread(); LockData lockData = threadData.get(currentThread); //没有获取锁，你释放个球球，如果为空抛出异常 if ( lockData == null ) &#123; throw new IllegalMonitorStateException("You do not own the lock: " + basePath); &#125; //获取重入数量 int newLockCount = lockData.lockCount.decrementAndGet(); //如果重入锁次数大于0，直接返回 if ( newLockCount &gt; 0 ) &#123; return; &#125; //如果重入锁次数小于0，抛出异常 if ( newLockCount &lt; 0 ) &#123; throw new IllegalMonitorStateException("Lock count has gone negative for lock: " + basePath); &#125; try &#123; //释放锁 internals.releaseLock(lockData.lockPath); &#125; finally &#123; //移除当前线程锁数据 threadData.remove(currentThread); &#125; &#125; 测试案例为了更好的理解其原理和代码分析中获取锁的过程，这里我们实现一个简单的Demo：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 基于curator的zookeeper分布式锁 */public class CuratorUtil &#123; private static String address = "192.168.1.180:2181"; public static void main(String[] args) &#123; //1、重试策略：初试时间为1s 重试3次 RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); //2、通过工厂创建连接 CuratorFramework client = CuratorFrameworkFactory.newClient(address, retryPolicy); //3、开启连接 client.start(); //4 分布式锁 final InterProcessMutex mutex = new InterProcessMutex(client, "/curator/lock"); //读写锁 //InterProcessReadWriteLock readWriteLock = new InterProcessReadWriteLock(client, "/readwriter"); ExecutorService fixedThreadPool = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 5; i++) &#123; fixedThreadPool.submit(new Runnable() &#123; @Override public void run() &#123; boolean flag = false; try &#123; //尝试获取锁，最多等待5秒 flag = mutex.acquire(5, TimeUnit.SECONDS); Thread currentThread = Thread.currentThread(); if(flag)&#123; System.out.println("线程"+currentThread.getId()+"获取锁成功"); &#125;else&#123; System.out.println("线程"+currentThread.getId()+"获取锁失败"); &#125; //模拟业务逻辑，延时4秒 Thread.sleep(4000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally&#123; if(flag)&#123; try &#123; mutex.release(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;); &#125; &#125;&#125; 这里我们开启5个线程，每个线程获取锁的最大等待时间为5秒，为了模拟具体业务场景，方法中设置4秒等待时间。开始执行main方法，通过ZooInspector监控/curator/lock下的节点如下图： 对，没错，设置4秒的业务处理时长就是为了观察生成了几个顺序节点。果然如案例中所述，每个线程都会生成一个节点并且还是有序的。观察控制台，我们会发现只有两个线程获取锁成功，另外三个线程超时获取锁失败会自动删除节点。线程执行完毕我们刷新一下/curator/lock节点，发现刚才创建的五个子节点已经不存在了。 基于数据库的分布式锁实现方式数据库的乐观锁(通过版本号)或者悲观锁(通过for update)还可以通过mysql的unique key来实现分布式锁，在mysql中插入一条记录，表明获取锁。删除一条记录，表明释放锁。 且在mysql表中设置一个unique key字段， 当有一台机器获得锁后， 其他机器无法获取。 这个有几个问题： 1. 如果一台机器获得锁，在释放锁之前进程挂了， 那么其他机器无法获取到锁。 可以引入锁有效时间的概念，超时后，删除记录，释放锁。 2. 万一获取锁的操作失败了，就直接做错误处理， 也不太好。 可以引入循环重试的方式来解决，控制重试次数。 参考https://blog.csdn.net/stpeace/article/details/84679324 基于数据库分布式锁注意的地方 基于数据库表的方式需要集群中多个节点的服务器时钟同步。 基于mysql数据库时需要在JDBC连接地址中增加时区配置，serverTimezone=GMT%2b8。关于脑裂问题： 由于网络、进程假死、中间件可能导致的脑裂问题在一定程度上不可避免，所以我们需要做一种权衡，即脑裂发生时是否可以避免问题进一步扩大。基于JDBC的分布式锁住要避免多个节点同时对事务表进行扫描，两者都是进行数据库操作，因为它们都基于数据库所以能够在一定程度上避免脑裂导致的集群处理问题，但是基于zookeeper,redis的方式可能会导致真实的脑裂发生 基于 Redis 的分布式锁（可以单机也可以集群）首先说明一下setnx()命令，setnx的含义就是SET if Not Exists，其主要有两个参数 setnx(key, value)。该方法是原子的，如果key不存在，则设置当前key成功，返回1；如果当前key已经存在，则设置当前key失败，返回0。value是System.currentTimeMillis() (获取锁的时间)+锁持有的时间。但是要注意的是setnx命令不能设置key的超时时间，只能通过expire()来对key设置。 具体的使用步骤如下: setnx(lockkey, 1) 如果返回0，则说明占位失败；如果返回1，则说明占位成功 expire()命令对lockkey设置超时时间，为的是避免死锁问题。 执行完业务代码后，可以通过delete命令删除key。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间(timeOut)，它要远小于锁的有效时间（几十毫秒量级）上边的解决方案可能存在的问题这个方案其实是可以解决日常工作中的需求的，但从技术方案的探讨上来说，可能还有一些可以完善的地方。比如，如果在第一步setnx执行成功后，在expire()命令执行成功前，发生了宕机的现象，那么就依然会出现死锁的问题，所以如果要对其进行完善的话，可以使用redis的setnx()、get()和getset()方法来实现分布式锁。 getSet(key,value)的命令会返回key对应的value，然后再把key原来的值更新为value。也就是说getSet()返回的是已过期的时间戳。如果这个已过期的时间戳等于currentValue，说明获取锁成功。 假设客户端A一开始持有锁，保存在redis中的value(时间戳)等于T1。 这时候客户端A的锁已经过期，那么C，D客户端就可以开始争抢锁了。currentValue是T1，C客户端的value是T2，D客户端的value是T3。首先C客户端进入到String oldValue = jedis.getSet(realKey, value);这行代码，获得的oldValue是T1，同时也会把realKey对应的value更新为T2。再执行后续的代码，oldValue等于currentValue，那么客户端C获取锁成功。接着D客户端也执行到了String oldValue = jedis.getSet(realKey, value);这行代码，获取的oldValue是T2，同时也会把realKey对应的value更新为T3。由于oldValue不等于currentValue，那么客户端D获取锁失败。12345678910111213141516171819202122232425262728293031323334353637public boolean lock(KeyPrefix prefix, String key, String value) &#123; Jedis jedis = null; Long lockWaitTimeOut = 200L; Long deadTimeLine = System.currentTimeMillis() + lockWaitTimeOut; try &#123; jedis = jedisPool.getResource(); String realKey = prefix.getPrefix() + key; for (;;) &#123; if (jedis.setnx(realKey, value) == 1) &#123; return true; &#125; String currentValue = jedis.get(realKey); // if lock is expired if (!StringUtils.isEmpty(currentValue) &amp;&amp; Long.valueOf(currentValue) &lt; System.currentTimeMillis()) &#123; // gets last lock time String oldValue = jedis.getSet(realKey, value); if (!StringUtils.isEmpty(oldValue) &amp;&amp; oldValue.equals(currentValue)) &#123; return true; &#125; &#125; lockWaitTimeOut = deadTimeLine - System.currentTimeMillis(); if (lockWaitTimeOut &lt;= 0L) &#123; return false; &#125; &#125; &#125; finally &#123; returnToPool(jedis); &#125;&#125; 单机Redis分布式锁不同的客户端通过生成随机字符串对制定的Key进行set if not exists + ttl 操作来进行抢占,通过key的TTL时间来决定持有锁的时间. 然后通过LUA脚本执行事务操作进行Compare and delete进行释放锁.其中的TTL和本机时钟有关. 释放锁示例lua脚本内容如下12345if redis.call("get",KEYS[1]) == ARGV[1] then return redis.call("del",KEYS[1])else return 0end 这段Lua脚本在执行的时候要把的lockValue作为ARGV[1]的值传进去，把lockKey作为KEYS[1]的值传进去。现在来看看解锁的java代码123456789public void unlock() &#123; // 使用lua脚本进行原子删除操作 String checkAndDelScript = "if redis.call('get', KEYS[1]) == ARGV[1] then " + "return redis.call('del', KEYS[1]) " + "else " + "return 0 " + "end"; jedis.eval(checkAndDelScript, 1, lockKey, lockValue);&#125; 。 集群Redis分布式锁Redis 的作者提供了RedLock 的算法来实现一个分布式锁。 加锁RedLock算法加锁步骤如下: 获取当前Unix时间，以毫秒为单位。 依次尝试从N个实例，使用相同的key和随机值获取锁。在步骤2，当向Redis设置锁时,客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端Redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试另外一个Redis实例。 客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（这里是3个节点）的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。 如果因为某些原因，获取锁失败（没有在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功）。 解锁向所有的Redis实例发送释放锁命令即可，不用关心之前有没有从Redis实例成功获取到锁. 实际秒杀场景中的应用实际上抢购，好像不用分布式锁，？？？？而是直接将库存放入到redis，是否结束标记放入到内存中，通过内存标记和redis中的decr()预减库存，然后将秒杀消息入队到消息队列中，最后消费消息并落地到DB中 分布式锁的重入该如何实现？待补充 如何确保过期时间大于业务执行时间可以加一个定时线程增加过期时间 codis待补充 参考https://juejin.im/post/5b737b9b518825613d3894f4]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你知道CoreDump吗？]]></title>
    <url>%2F2019%2F03%2F11%2F%E4%BD%A0%E7%9F%A5%E9%81%93CoreDump%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[什么是core dump文件Core dump文件是当一个进程在收到某些信号后终止时产生的文件，其中包含进程终止时刻进程内存的镜像。我们可以使用gdb从该镜像中观察进程终止时处于什么状态，用于追踪排查定位问题。 产生core dump的可能原因 内存访问越界 a) 由于使用错误的下标，导致数组访问越界 b) 搜索字符串时，依靠字符串结束符来判断字符串是否结束，但是字符串没有正常的使用结束符 c) 使用strcpy, strcat, sprintf, strcmp, strcasecmp等字符串操作函数，将目标字符串读/写爆。应该使用strncpy, strlcpy, strncat, strlcat, snprintf, strncmp, strncasecmp等函数防止读写越界。 多线程程序使用了线程不安全的函数。 多线程读写的数据未加锁保护。对于会被多个线程同时访问的全局数据，应该注意加锁保护，否则很容易造成core dump 非法指针 a) 使用空指针 b) 随意使用指针转换。一个指向一段内存的指针，除非确定这段内存原先就分配为某种结构或类型，或者这种结构或类型的数组，否则不要将它转换为这种结构或类型的指针，而应该将这段内存拷贝到一个这种结构或类型中，再访问这个结构或类型。这是因为如果这段内存的开始地址不是按照这种结构或类型对齐的，那么访问它时就很容易因为bus error而core dump. 堆栈溢出.不要使用大的局部变量（因为局部变量都分配在栈上），这样容易造成堆栈溢出，破坏系统的栈和堆结构，导致出现莫名其妙的错误。如何判断一个文件是coredump文件？在类unix系统下，coredump文件本身主要的格式也是ELF格式，因此，我们可以通过readelf命令进行判断。123456789101112131415161718192021root@ubuntu16-desktop:~# readelf -h core ELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2&apos;s complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: CORE (Core file) //这里可以看出来 Machine: Advanced Micro Devices X86-64 Version: 0x1 Entry point address: 0x0 Start of program headers: 64 (bytes into file) Start of section headers: 0 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 56 (bytes) Number of program headers: 19 Size of section headers: 0 (bytes) Number of section headers: 0 Section header string table index: 0 或者通过file命令12root@ubuntu16-desktop:~# file corecore: ELF 64-bit LSB core file x86-64, version 1 (SYSV), SVR4-style, from &apos;./test&apos; 如何分析core dump文件使用调试器，如gdbhttps://www.gnu.org/software/gdb/举个栗子：写了个空指针栗子12345#include &lt;stdio.h&gt;int main()&#123; int *p = NULL; printf("hello world! \n"); printf("this will cause core dump p %d", *p);&#125; 执行该可执行文件会产生一个core文件（必须先通过参数打开该功能，要不即使有core dump，也不会产生core文件），如何分析呢？123456789101112131415161718192021222324gdb ./test core GNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word"...Reading symbols from ./test...done.[New LWP 18173]Core was generated by `./test'.Program terminated with signal SIGSEGV, Segmentation fault.#0 0x0000000000400584 in main () at main.c:44 printf("this will cause core dump p %d", *p);(gdb) bt //一般就可以看到出错的代码是哪一句了，还可以打印出相应变量的数值，进行进一步分析。#0 0x0000000000400584 in main () at main.c:4(gdb) 对于结构复杂的程序，如涉及模板类及复杂的调用，gdb得出了出错位置，似乎这还不够，这时候要使用更为专业的工具——valgrind。下载地址：http://valgrind.org/downloads/current.html 进入下载文件夹，分别执行(需要root权限，且必须按默认路径安装，否则有加载错误)：12345./configuremakemake install 安装成功后，使用类似如下命令启动程序： valgrind --tool=memcheck --leak-check=full --track-origins=yes --leak-resolution=high --show-reachable=yes --log-file=memchecklog ./controller_test 其中，–log-file=memchecklog指记录日志文件，名字为memchecklog；–tool=memcheck和–leak-check=full用于内存检测。 可以得到类似的记录：123456789==23735====23735== Thread 1:==23735== Invalid read of size 4==23735== at 0x804F327: ResourceHandler&lt;HBMessage&gt;::~ResourceHandler() (ResourceHandler.cpp:48)==23735== by 0x804FDBE: ConnectionManager&lt;HBMessage&gt;::~ConnectionManager() (ConnectionManager.cpp:74)==23735== by 0×8057288: MainThread::~MainThread() (MainThread.cpp:73)==23735== by 0x8077B2F: main (Main.cpp:177)==23735== Address 0×0 is not stack’d, malloc’d or (recently) free’d==23735== 可以看到说明为无法访问Address 0x0，明显为一处错误。 这样valgrind直接给出了出错原因以及程序中所有的内存调用、释放记录，非常智能，在得知错误原因的情况下，找出错误就效率高多了。再说一句，valgrind同时给出了程序的Memory Leak情况的报告，给出了new-delete对应情况，所有泄漏点位置给出，这一点在其他工具很难做到，十分好用。 与core dump有关的内核参数 kernel.core_pattern设置core文件保存位置或文件名,只有文件名时，则保存在应用程序运行的目录下 /proc/sys/kernel/core_pipe_limit定义了可以有多少个并发的崩溃程序可以通过管道模式传递给指定的core信息收集程序。如果超过了指定数，则后续的程序将不会处理，只在内核日志中做记录。0是个特殊的值，当设置为0时，不限制并行捕捉崩溃的进程，但不会等待用户程序搜集完毕方才回收/proc/pid目录（就是说，崩溃程序的相关信息可能随时被回收，搜集的信息可能不全）。 kernel.core_uses_pidCore文件的文件名是否添加应用程序pid做为扩展。0：不添加，1：添加 设置Core文件大小上限 123456 # ulimit -c 0 // 0表示不产生core文件# ulimit -c 100 // 100表示设置core文件最大为100k，当然可以根据自己需要设置，注意设置单位是KB# ulimit -c unlimited // 不限制core文件大小使用上述命令设置core文件大小只对当前shell环境有效，系统重启或者退出当前shell设置就会失效，恢复默认设置。若想永久生效可以把该shell放到/etc/profile文件中，系统重新启动初始化环境时会执行其中的命令，该设置就会在全局范围内生效，达到永久生效的效果。也可以使用 source /etc/profile命令立即全局生效。# echo "unlimit -c unlimited" &gt;&gt; /etc/profile // 配置添加到/etc/profile中# source /etc/profile // 立即生效 设置Core文件存储目录 1234567891011121314# echo "/var/core-dir/core-%e-%p-%t" &gt; /proc/sys/kernel/core_pattern该命令可以控制core文件保存位置和文件名格式。注意需要使用root权限执行，并且存储路径必须是绝对路径，不能是相对路径其中%e表示添加用户程序名称，%p表示添加程序进程PID，%t表示添加产生core文件时的时间戳，还有其他一些非常有用的格式，可以参阅CORE(5)文档。这样修改后系统重启后也会消失，同样也有永久生效的办法修改/etc/sysctl.conf文件在其中修改或者添加/etc/sysctl.confkernel.core_pattern = /var/core-dir/core-%e-%p-%t然后执行下面命令配置立即生效，这样系统重启后配置依然有效# sysctl –p /etc/sysctl.conf core dump文件产生流程大概说一下从进程出现异常到生成core文件的流程，不会涉及太多Linux系统实现细节，具体细节可以参考相关文档和linux内核源码。 进程运行时发生一个异常，比如非法内存地址访问（即段错误），相应硬件会上报该异常，CPU检测到该异常时会进入异常处理流程,包括保存当前上下文，跳转到对应的中断向量执行入口等 在异常处理流程中如果判断该异常发生时是处于用户态，则该异常只会影响当前进程，此时向用户态进程发送相应的信号，如段错误就会发送SIGSEGV信号 当用户态进程被调度时会检查pending的信号，如果发现pending的信号是SIG_KERNEL_COREDUMP_MASK中的一个，就会进入core文件内容收集存储流程，然后根据配置(core_pattern等)生成core文件。 处理docker容器进程Core Dump的方法 使用ulimit -c和/proc/sys/kernel/core_pattern设置Core Dump文件大小限制和存储位置 使用管道程序增强Core Dump文件处理能力 使用管道程序和容器内进程结合的方式完成内核态转到用户态，在容器内处理Core文件存储 简单来说就是，因为有关core dump的设置容器并没有隔离，所以在主机上所有的设置，容器也是一样的。但是我们想针对容器进行特殊处理，那么我们可以使用linux的piping技术转储core文件。但是管道程序是作为内核线程在运行的，运行在内核态，并且在宿主机Initial Namespace中以root用户身份运行，不在任何容器内。我们想在容器内运行这么一个程序，可以使用socket activation技术，简单来说，就是由系统init进程（对于目前大多数linux系统来说是systemd）来为普通应用进程监听特定socket，此时应用进程并未启动，当有连接到达该socket后，由init进程接管该连接并跟进配置文件启动相应的应用进程，然后把连接传递给应用进程来处理，主要好处是当没有连接到达时，应用进程无需常驻后台空跑耗费系统资源。非常适合像Core Dump这种低频服务。我们可以设置一个unix socket来把管道程序的文件描述符传递到容器内进程，完成传递后， 管道程序就可以退出，由容器内进程处理core文件的存储。 参考小米运维https://blog.csdn.net/pengzhouzhou/article/details/88313891https://www.cnblogs.com/bodhitree/p/5850212.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-compose的问题]]></title>
    <url>%2F2019%2F02%2F22%2Fdocker-compose%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[今天我启动docker-compose的时候，出现了如下问题 经过查询得知https://blog.csdn.net/tianshuhao521/article/details/84782309，原因是关闭防火墙之后docker需要重启，执行以下命令重启docker即可：service docker restart 但是我重启不行，journalctl -xe查看日志，发现 原来有访问控制了，可能是因为我们实验室对该服务器的内核进行了白名单。lsmod命令一输，结果如下，发现确实有一个操作系统安全module。 通过rmmod卸载掉之后就可以 了。 通过dmesg查看信息发现， 进一步思考：他的这个原理是什么？有待补充]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跳表]]></title>
    <url>%2F2019%2F02%2F22%2F%E8%B7%B3%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[这是一个基本的跳表数据结构,简单的代码实现:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;//跳表的最大层级为32级#define ZSKIPLIST_MAXLEVEL 32#define ZSKIPLIST_P 0.25#include&lt;math.h&gt;//跳表节点typedef struct zskiplistNode &#123; int key; int value; struct zskiplistLevel &#123; struct zskiplistNode *forward; &#125; level[1];&#125; zskiplistNode;//跳表typedef struct zskiplist &#123; struct zskiplistNode *header; int level;&#125; zskiplist;//创建跳表的节点zskiplistNode *zslCreateNode(int level, int key, int value) &#123; zskiplistNode *zn = (zskiplistNode *)malloc(sizeof(*zn)+level*sizeof(zn-&gt;level)); zn-&gt;key = key; zn-&gt;value = value; return zn;&#125;//初始化跳表zskiplist *zslCreate(void) &#123; int j; zskiplist *zsl; zsl = (zskiplist *) malloc(sizeof(*zsl)); zsl-&gt;level = 1;//将层级设置为1 zsl-&gt;header = zslCreateNode(ZSKIPLIST_MAXLEVEL,NULL,NULL); for (j = 0; j &lt; ZSKIPLIST_MAXLEVEL; j++) &#123; zsl-&gt;header-&gt;level[j].forward = NULL; &#125; return zsl;&#125;//向跳表中插入元素时，通过随机函数获取一个层级，表示插入在哪一层,ZSKIPLIST_P为何设置为0.25？int zslRandomLevel(void) &#123; int level = 1; while ((rand()&amp;0xFFFF) &lt; (ZSKIPLIST_P * 0xFFFF)) level += 1; return (level&lt;ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;&#125;//向跳表中插入元素zskiplistNode *zslInsert(zskiplist *zsl, int key, int value) &#123; zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; int i, level; x = zsl-&gt;header; //在跳表中寻找合适的位置并插入元素 for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123; while (x-&gt;level[i].forward &amp;&amp; (x-&gt;level[i].forward-&gt;key &lt; key || (x-&gt;level[i].forward-&gt;key == key &amp;&amp; x-&gt;level[i].forward-&gt;value &lt; value))) &#123; x = x-&gt;level[i].forward; &#125; update[i] = x; &#125; //获取元素所在的随机层数 level = zslRandomLevel(); if (level &gt; zsl-&gt;level) &#123; for (i = zsl-&gt;level; i &lt; level; i++) &#123; update[i] = zsl-&gt;header; &#125; zsl-&gt;level = level; &#125; //为新创建的元素创建数据节点 x = zslCreateNode(level,key,value); for (i = 0; i &lt; level; i++) &#123; x-&gt;level[i].forward = update[i]-&gt;level[i].forward; update[i]-&gt;level[i].forward = x; &#125; return x;&#125;//跳表中删除节点的操作void zslDeleteNode(zskiplist *zsl, zskiplistNode *x, zskiplistNode **update) &#123; int i; for (i = 0; i &lt; zsl-&gt;level; i++) &#123; if (update[i]-&gt;level[i].forward == x) &#123; update[i]-&gt;level[i].forward = x-&gt;level[i].forward; &#125; &#125; //如果层数变了，相应的将层数进行减1操作 while(zsl-&gt;level &gt; 1 &amp;&amp; zsl-&gt;header-&gt;level[zsl-&gt;level-1].forward == NULL) zsl-&gt;level--;&#125;//从跳表中删除元素int zslDelete(zskiplist *zsl, int key, int value) &#123; zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; int i; x = zsl-&gt;header; //寻找待删除元素 for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123; while (x-&gt;level[i].forward &amp;&amp; (x-&gt;level[i].forward-&gt;key &lt; key || (x-&gt;level[i].forward-&gt;key == key &amp;&amp; x-&gt;level[i].forward-&gt;value &lt; value))) &#123; x = x-&gt;level[i].forward; &#125; update[i] = x; &#125; x = x-&gt;level[0].forward; if (x &amp;&amp; key == x-&gt;key &amp;&amp; x-&gt;value == value) &#123; zslDeleteNode(zsl, x, update); //别忘了释放节点所占用的存储空间 free(x); return 1; &#125; else &#123; //未找到相应的元素 return 0; &#125; return 0;&#125;//将链表中的元素打印出来void printZslList(zskiplist *zsl) &#123; zskiplistNode *x; x = zsl-&gt;header; for (int i = zsl-&gt;level-1; i &gt;= 0; i--) &#123; zskiplistNode *p = x-&gt;level[i].forward; while (p) &#123; printf(" %d|%d ",p-&gt;key,p-&gt;value); p = p-&gt;level[i].forward; &#125; printf("\n"); &#125;&#125;int main() &#123; zskiplist *list = zslCreate(); zslInsert(list,1,2); zslInsert(list,4,5); zslInsert(list,2,2); zslInsert(list,7,2); zslInsert(list,7,3); zslInsert(list,7,3); printZslList(list); zslDelete(list,7,2); printZslList(list);&#125;/* 4|5 1|2 2|2 4|5 7|2 7|3 7|3 4|5 1|2 2|2 4|5 7|3 7|3 */ 我们都知道redis中zset有序集，内部实现用了跳表，那么又有哪些优化呢？ Redis中跳表的基本数据结构定义如下，与基本跳表数据结构相比，在Redis中实现的跳表其特点是不仅有前向指针，也存在后向指针，而且在前向指针的结构中存在span跨度字段，这个跨度字段的出现有助于快速计算元素在整个集合中的排名。 12345678910111213141516171819202122//定义跳表的基本数据节点typedef struct zskiplistNode &#123; robj *obj; // zset value double score;// zset score struct zskiplistNode *backward;//后向指针 struct zskiplistLevel &#123;//前向指针 struct zskiplistNode *forward; unsigned int span; &#125; level[];&#125; zskiplistNode;typedef struct zskiplist &#123; struct zskiplistNode *header, *tail; unsigned long length; int level;&#125; zskiplist;//有序集数据结构typedef struct zset &#123; dict *dict;//字典存放value,以value为key zskiplist *zsl;&#125; zset; 将如上数据结构转化成更形式化的图形表示，如下图所示 可以看到header指针指向的是一个具有固定层级(32层)的表头节点,为什么定义成32,是因为定义成32层理论上对于2^32-1个元素的查询最优，而2^32=4294967296个元素，对于绝大多数的应用来说，已经足够了，所以就定义成了32层,到于为什么查询最优，你可以将其想像成一个32层的完全二叉排序树，算算这个树中节点的数量。 Redis中有序集另一个值得注意的地方就是当Score相同的时候，是如何存储的，当集合中两个值的Score相同，这时在跳表中存储会比较这两个值，对这两个值按字典排序存储在跳表结构中 下面来看看Redis对zskiplist/zskiplistNode的相关操作,源码如下所示(源码均出自t_zset.c) 创建跳表结构的源码123456789101112131415161718192021//#define ZSKIPLIST_MAXLEVEL 32 /* Should be enough for 2^32 elements */zskiplist *zslCreate(void) &#123; int j; zskiplist *zsl; //分配内存 zsl = zmalloc(sizeof(*zsl)); zsl-&gt;level = 1;//默认层级为1 zsl-&gt;length = 0;//跳表长度设置为0 zsl-&gt;header = zslCreateNode(ZSKIPLIST_MAXLEVEL,0,NULL); for (j = 0; j &lt; ZSKIPLIST_MAXLEVEL; j++) &#123; //因为没有任何元素，将表头节点的前向指针均设置为0 zsl-&gt;header-&gt;level[j].forward = NULL; //将表头节点前向指针结构中的跨度字段均设为0 zsl-&gt;header-&gt;level[j].span = 0; &#125; //表头后向指针设置成0 zsl-&gt;header-&gt;backward = NULL; //表尾节点设置成NULL zsl-&gt;tail = NULL; return zsl;&#125; 在上述代码中调用了zslCreateNode这个函数,函数的源码如下所示。 123456zskiplistNode *zslCreateNode(int level, double score, robj *obj) &#123; zskiplistNode *zn = zmalloc(sizeof(*zn)+level*sizeof(struct zskiplistLevel)); zn-&gt;score = score; zn-&gt;obj = obj; return zn;&#125; 执行完上述代码之后会创建如下图所示的跳表结构 创建了跳表的基本结构，下面就是插入操作了，Redis中源码如下所示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950zskiplistNode *zslInsert(zskiplist *zsl, double score, robj *obj) &#123; zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; //update[32] unsigned int rank[ZSKIPLIST_MAXLEVEL];//rank[32] int i, level; redisAssert(!isnan(score)); x = zsl-&gt;header; //寻找元素插入的位置 for (i = zsl-&gt;level-1; i &gt;= 0; i--) &#123; /* store rank that is crossed to reach the insert position */ rank[i] = i == (zsl-&gt;level-1) ? 0 : rank[i+1]; while (x-&gt;level[i].forward &amp;&amp; (x-&gt;level[i].forward-&gt;score &lt; score || //以下是得分相同的情况下，比较value的字典排序 (x-&gt;level[i].forward-&gt;score == score &amp;&amp;compareStringObjects(x-&gt;level[i].forward-&gt;obj,obj) &lt; 0))) &#123; rank[i] += x-&gt;level[i].span; x = x-&gt;level[i].forward; &#125; update[i] = x; &#125; //产生随机层数 level = zslRandomLevel(); if (level &gt; zsl-&gt;level) &#123; for (i = zsl-&gt;level; i &lt; level; i++) &#123; rank[i] = 0; update[i] = zsl-&gt;header; update[i]-&gt;level[i].span = zsl-&gt;length; &#125; //记录最大层数 zsl-&gt;level = level; &#125; //产生跳表节点 x = zslCreateNode(level,score,obj); for (i = 0; i &lt; level; i++) &#123; x-&gt;level[i].forward = update[i]-&gt;level[i].forward; update[i]-&gt;level[i].forward = x; //更新跨度 x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]); update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1; &#125; //此种情况只会出现在随机出来的层数小于最大层数时 for (i = level; i &lt; zsl-&gt;level; i++) &#123; update[i]-&gt;level[i].span++; &#125; x-&gt;backward = (update[0] == zsl-&gt;header) ? NULL : update[0]; if (x-&gt;level[0].forward) x-&gt;level[0].forward-&gt;backward = x; else zsl-&gt;tail = x; zsl-&gt;length++; return x;&#125; 上述源码中，有一个产生随机层数的函数，源代码如下所示: 12345678int zslRandomLevel(void) &#123; int level = 1; //#define ZSKIPLIST_P 0.25 while ((random()&amp;0xFFFF) &lt; (ZSKIPLIST_P * 0xFFFF)) level += 1; //#ZSKIPLIST_MAXLEVEL 32 return (level&lt;ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;&#125; 图形化的形式描述如下图所示: 参考https://www.cnblogs.com/WJ5888/p/4595306.html]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>DataStructure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[当你在浏览器输入google.com之后]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%BD%93%E4%BD%A0%E5%9C%A8%E6%B5%8F%E8%A7%88%E5%99%A8%E8%BE%93%E5%85%A5google-com%E4%B9%8B%E5%90%8E%2F</url>
    <content type="text"><![CDATA[按下”g”键当你按下“g”键，浏览器接收到这个消息之后，会触发自动完成机制。浏览器根据自己的算法，以及你是否处于隐私浏览模式，会在浏览器的地址框下方给出输入建议。大部分算法会优先考虑根据你的搜索历史和书签等内容给出建议。你打算输入 “google.com”，因此给出的建议并不匹配。但是输入过程中仍然有大量的代码在后台运行，你的每一次按键都会使得给出的建议更加准确。甚至有可能在你输入之前，浏览器就将 “google.com” 建议给你。 回车键按下为了从零开始，我们选择键盘上的回车键被按到最低处作为起点。在这个时刻，一个专用于回车键的电流回路被直接地或者通过电容器间接地闭合了，使得少量的电流进入了键盘的逻辑电路系统。这个系统会扫描每个键的状态，对于按键开关的电位弹跳变化进行噪音消除(debounce)，并将其转化为键盘码值。在这里，回车的码值是13。键盘控制器在得到码值之后，将其编码，用于之后的传输。现在这个传输过程几乎都是通过通用串行总线(USB)或者蓝牙(Bluetooth)来进行的，以前是通过PS/2或者ADB连接进行。 USB键盘：键盘的USB元件通过计算机上的USB接口与USB控制器相连接，USB接口中的第一号针为它提供了5V的电压键码值存储在键盘内部电路一个叫做”endpoint”的寄存器内USB控制器大概每隔10ms便查询一次”endpoint”以得到存储的键码值数据，这个最短时间间隔由键盘提供键值码值通过USB串行接口引擎被转换成一个或者多个遵循低层USB协议的USB数据包这些数据包通过D+针或者D-针(中间的两个针)，以最高1.5Mb/s的速度从键盘传输至计算机。速度限制是因为人机交互设备总是被声明成”低速设备”（USB 2.0 compliance）这个串行信号在计算机的USB控制器处被解码，然后被人机交互设备通用键盘驱动进行进一步解释。之后按键的码值被传输到操作系统的硬件抽象层 虚拟键盘（触屏设备）：在现代电容屏上，当用户把手指放在屏幕上时，一小部分电流从传导层的静电域经过手指传导，形成了一个回路，使得屏幕上触控的那一点电压下降，屏幕控制器产生一个中断，报告这次“点击”的坐标然后移动操作系统通知当前活跃的应用，有一个点击事件发生在它的某个GUI部件上了，现在这个部件是虚拟键盘的按钮虚拟键盘引发一个软中断，返回给OS一个“按键按下”消息这个消息又返回来向当前活跃的应用通知一个“按键按下”事件 解析URL浏览器通过 URL 能够知道下面的信息：Protocol “http”使用HTTP协议Resource “/“请求的资源是主页(index) 输入的是 URL 还是搜索的关键字？当协议或主机名不合法时，浏览器会将地址栏中输入的文字传给默认的搜索引擎。大部分情况下，在把文字传递给搜索引擎的时候，URL会带有特定的一串字符，用来告诉搜索引擎这次搜索来自这个特定浏览器。 转换非 ASCII 的 Unicode 字符浏览器检查输入是否含有不是 a-z， A-Z，0-9， - 或者 . 的字符这里主机名是 google.com ，所以没有非ASCII的字符；如果有的话，浏览器会对主机名部分使用 Punycode 编码 检查 HSTS 列表 浏览器检查自带的“预加载 HSTS（HTTP严格传输安全）”列表，这个列表里包含了那些请求浏览器只使用HTTPS进行连接的网站 如果网站在这个列表里，浏览器会使用 HTTPS 而不是 HTTP 协议，否则，最初的请求会使用HTTP协议发送 注意，一个网站哪怕不在 HSTS 列表里，也可以要求浏览器对自己使用 HSTS 政策进行访问。浏览器向网站发出第一个 HTTP 请求之后，网站会返回浏览器一个响应，请求浏览器只使用 HTTPS 发送请求。然而，就是这第一个 HTTP 请求，却可能会使用户受到 downgrade attack 的威胁，这也是为什么现代浏览器都预置了 HSTS 列表。 DNS查询浏览器检查域名是否在缓存当中（要查看 Chrome 当中的缓存， 打开 chrome://net-internals/#dns）。如图如果缓存中没有，就去调用 gethostbyname 库函数（操作系统不同函数也不同）进行查询。gethostbyname 函数在试图进行DNS解析之前首先检查域名是否在本地 Hosts 里，Hosts 的位置 不同的操作系统有所不同如果 gethostbyname 没有这个域名的缓存记录，也没有在 hosts 里找到，它将会向 DNS 服务器发送一条 DNS 查询请求。DNS 服务器是由网络通信栈提供的，通常是本地路由器或者 ISP 的缓存 DNS 服务器。查询本地 DNS 服务器如果 DNS 服务器和我们的主机在同一个子网内，系统会按照下面的 ARP 过程对 DNS 服务器进行 ARP查询如果 DNS 服务器和我们的主机在不同的子网，系统会按照下面的 ARP 过程对默认网关进行查询 ARP 过程要想发送 ARP（地址解析协议）广播，我们需要有一个目标 IP 地址，同时还需要知道用于发送 ARP 广播的接口的 MAC 地址。 首先查询 ARP 缓存，如果缓存命中，我们返回结果：目标 IP = MAC 如果缓存没有命中： 查看路由表，看看目标 IP 地址是不是在本地路由表中的某个子网内。是的话，使用跟那个子网相连的接口，否则使用与默认网关相连的接口。 查询选择的网络接口的 MAC 地址 我们发送一个二层（ OSI 模型 中的数据链路层）ARP 请求：ARP Request:1234Sender MAC: interface:mac:address:hereSender IP: interface.ip.goes.hereTarget MAC: FF:FF:FF:FF:FF:FF (Broadcast)Target IP: target.ip.goes.here 根据连接主机和路由器的硬件类型不同，可以分为以下几种情况： 直连： 如果我们和路由器是直接连接的，路由器会返回一个 ARP Reply （见下面）。 集线器： 如果我们连接到一个集线器，集线器会把 ARP 请求向所有其它端口广播，如果路由器也“连接”在其中，它会返回一个 ARP Reply 。 交换机： 如果我们连接到了一个交换机，交换机会检查本地 CAM/MAC 表，看看哪个端口有我们要找的那个 MAC 地址，如果没有找到，交换机会向所有其它端口广播这个 ARP 请求。如果交换机的 MAC/CAM 表中有对应的条目，交换机会向有我们想要查询的 MAC 地址的那个端口发送 ARP 请求如果路由器也“连接”在其中，它会返回一个 ARP ReplyARP Reply:1234Sender MAC: target:mac:address:hereSender IP: target.ip.goes.hereTarget MAC: interface:mac:address:hereTarget IP: interface.ip.goes.here 现在我们有了 DNS 服务器或者默认网关的 IP 地址172.21.0.21，如下图，我们可以继续 DNS 请求了： 使用 53 端口向 DNS 服务器发送 UDP 请求包，如果响应包太大，会使用 TCP 协议 如果本地/ISP DNS 服务器没有找到结果，它会发送一个递归查询请求，一层一层向高层 DNS 服务器做查询，直到查询到起始授权机构，如果找到会把结果返回。 使用套接字当浏览器得到了目标服务器的 IP 地址，以及 URL 中给出来端口号（http 协议默认端口号是 80， https 默认端口号是 443），它会调用系统库函数 socket ，请求一个 TCP流套接字，对应的参数是 AF_INET/AF_INET6 和 SOCK_STREAM 。 这个请求首先被交给传输层，在传输层请求被封装成 TCP segment。目标端口会被加入头部，源端口会在系统内核的动态端口范围内选取（Linux下是ip_local_port_range) TCP segment 被送往网络层，网络层会在其中再加入一个 IP 头部，里面包含了目标服务器的IP地址以及本机的IP地址，把它封装成一个IP packet。 这个 TCP packet 接下来会进入链路层，链路层会在封包中加入 frame 头部，里面包含了本地内置网卡的MAC地址以及网关（本地路由器）的 MAC 地址。像前面说的一样，如果内核不知道网关的 MAC 地址，它必须进行 ARP 广播来查询其地址。 到了现在，TCP 封包已经准备好了，可以使用下面的方式进行传输： 以太网 WiFi 蜂窝数据网络 TCP三次握手TLS握手 客户端发送一个 ClientHello 消息到服务器端，消息中同时包含了它的 Transport Layer Security (TLS) 版本，可用的加密算法和压缩算法。 服务器端向客户端返回一个 ServerHello 消息，消息中包含了服务器端的TLS版本，服务器所选择的加密和压缩算法，以及数字证书认证机构（Certificate Authority，缩写 CA）签发的服务器公开证书，证书中包含了公钥。客户端会使用这个公钥加密接下来的握手过程，直到协商生成一个新的对称密钥 客户端根据自己的信任CA列表，验证服务器端的证书是否可信。如果认为可信，客户端会生成一串伪随机数，使用服务器的公钥加密它。这串随机数会被用于生成新的对称密钥 服务器端使用自己的私钥解密上面提到的随机数，然后使用这串随机数生成自己的对称主密钥客户端发送一个 Finished 消息给服务器端，使用对称密钥加密这次通讯的一个散列值 服务器端生成自己的 hash 值，然后解密客户端发送来的信息，检查这两个值是否对应。如果对应，就向客户端发送一个 Finished 消息，也使用协商好的对称密钥加密从现在开始，接下来整个 TLS 会话都使用对称秘钥进行加密，传输应用层（HTTP）内容 HTTP 协议如果浏览器是 Google 出品的，它不会使用 HTTP 协议来获取页面信息，而是会与服务器端发送请求，商讨使用 SPDY 协议。 如果浏览器使用 HTTP 协议而不支持 SPDY 协议，它会向服务器发送这样的一个请求:1234GET / HTTP/1.1Host: google.comConnection: close[其他头部] 如下图 浏览器背后的故事当服务器提供了资源之后（HTML，CSS，JS，图片等），浏览器会执行下面的操作： 解析 —— HTML，CSS，JS渲染 —— 构建 DOM 树 -&gt; 渲染 -&gt; 布局 -&gt; 绘制。 参考https://github.com/skyline75489/what-happens-when-zh_CN]]></content>
      <categories>
        <category>TCP/IP</category>
      </categories>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql查询缓存]]></title>
    <url>%2F2019%2F02%2F22%2FMysql%E6%9F%A5%E8%AF%A2%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[开启查询缓存后，查询语句的解析过程： 在解析一个查询语句之前，如果查询缓存是打开的，那么MySQL会优先检查这个查询是否命中查询缓存中的数据。如果当前的查询恰好命中了查询缓存，那么在返回查询结果之前MySQL会检查一次用户权限。若权限没有问题，MySQL会跳过所有其他阶段（解析、优化、执行等），直接从缓存中拿到结果并返回给客户端。这种情况下，查询不会被解析，不用生成执行计划，不会被执行。 开启查询缓存设置使用查询缓存的方式使用 query_cache_type 变量来开启查询缓存，开启方式有三种： ON : 正常缓存。表示在使用 SELECT 语句查询时，若没指定 SQL_NO_CACHE 或其他非确定性函数，则一般都会将查询结果缓存下来。 DEMAND ：指定SQL_CACHE才缓存。表示在使用 SELECT 语句查询时，必须在该 SELECT 语句中指定 SQL_CACHE 才会将该SELECT语句的查询结果缓存下来。 例如：select SQL_CACHE name from user where id = 15; #只有明确指定 SQL_CACHE 的SELECT语句，才会将查询结果缓存。 OFF： 关闭查询缓存。 立刻生效，重启服务失效 mysql&gt; set global query_cache_type=1; 当my.cnf 中，query_cache_type = OFF ，启动mysql服务后，在mysql命令行中使用上面语句开启查询缓存，会报错：ERROR 1651 (HY000): Query cache is disabled; restart the server with query_cache_type=1 to enable it。遇到这种情况，是无法在mysql命令行中开启查询缓存的，必须修改my.cnf的query_cache_type = ON，然后重启mysql服务。 设置查询缓存的大小 query_cache_size ：查询缓存的总体可用空间。 注意：如果 query_cache_size=0 ，那即便你设置了 query_cache_type = ON，查询缓存仍然是无法工作的。 查询缓存相关参数123456uery_cache_limit : MySQL能够缓存的最大查询结果；如果某查询的结果大小大于此值，则不会被缓存；query_cache_min_res_unit : 查询缓存中分配内存的最小单位；(注意：此值通常是需要调整的，此值被调整为接近所有查询结果的平均值是最好的) 计算单个查询的平均缓存大小：（query_cache_size-Qcache_free_memory）/Qcache_queries_in_cachequery_cache_size : 查询缓存的总体可用空间，单位为字节；其必须为1024的倍数；query_cache_type: 查询缓存类型；是否开启缓存功能，开启方式有三种&#123;ON|OFF|DEMAND&#125;；query_cache_wlock_invalidate : 当其它会话锁定此次查询用到的资源时，是否不能再从缓存中返回数据；（OFF表示可以从缓存中返回数据） 查询缓存状态12345678910111213mysql&gt; SHOW GLOBAL STATUS LIKE &apos;Qcache%&apos;;+-------------------------+----------+| Variable_name | Value |+-------------------------+----------+| Qcache_free_blocks | 1 | #查询缓存中的空闲块| Qcache_free_memory | 16759656| #查询缓存中尚未使用的空闲内存空间| Qcache_hits | 16 | #缓存命中次数| Qcache_inserts | 71 | #向查询缓存中添加缓存记录的条数| Qcache_lowmem_prunes | 0 | #表示因缓存满了而不得不清理部分缓存以存储新的缓存，这样操作的次数。若此数值过大，则表示缓存空间太小了。| Qcache_not_cached | 57 | #没能被缓存的次数| Qcache_queries_in_cache | 0 | #此时仍留在查询缓存的缓存个数| Qcache_total_blocks | 1 | #共分配出去的块数+-------------------------+----------+ 衡量缓存是否有效方式一：1234567mysql&gt; SHOW GLOBAL STATUS WHERE Variable_name=&apos;Qcache_hits&apos; OR Variable_name=&apos;Com_select&apos;;+---------------+-----------+| Variable_name | Value |+---------------+-----------+| Com_select | 279292490 | #非缓存查询次数| Qcache_hits | 307366973 | # 缓存命中次数+---------------+----------- 缓存命中率：Qcache_hits/(Qcache_hits+Com_select) 方式二：“命中和写入”的比率这是另外一种衡量缓存是否有效的指标。 123456mysql&gt; SHOW GLOBAL STATUS WHERE Variable_name=&apos;Qcache_hits&apos; OR Variable_name=&apos;Qcache_inserts&apos;;+----------------+-----------+| Variable_name | Value |+----------------+-----------+| Qcache_hits | 307416113 | #缓存命中次数| Qcache_inserts | 108873957 | #向查询缓存中添加缓存记录的条数 “命中和写入”的比率: Qcache_hits/Qcache_inserts # 如果此比值大于3:1, 说明缓存也是有效的；如果高于10:1，相当理想； 缓存失效问题当数据表改动时，基于该数据表的任何缓存都会被删除。（表层面的管理，不是记录层面的管理，因此失效率较高） 注意事项： 应用程序，不应该关心query cache的使用情况。可以尝试使用，但不能由query cache决定业务逻辑，因为query cache由DBA来管理。 缓存是以SQL语句为key存储的，因此即使SQL语句功能相同，但如果多了一个空格或者大小写有差异都会导致匹配不到缓存。 参考：https://www.jianshu.com/p/5c3ddf9a454chttps://juejin.im/post/5c6b9c09f265da2d8a55a855]]></content>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络之raw_socket]]></title>
    <url>%2F2019%2F02%2F22%2F%E7%BD%91%E7%BB%9C%E4%B9%8Braw-socket%2F</url>
    <content type="text"><![CDATA[什么是Raw_Socket?raw socket，即原始套接字，可以接收本机网卡上的数据帧或者数据包,对与监听网络的流量和分析是很有作用的.一共可以有3种方式创建这种socket 1.socket(AF_INET, SOCK_RAW, IPPROTO_TCP|IPPROTO_UDP|IPPROTO_ICMP)发送接收ip数据包 2.socket(PF_PACKET, SOCK_RAW, htons(ETH_P_IP|ETH_P_ARP|ETH_P_ALL))发送接收以太网数据帧 3.socket(AF_INET, SOCK_PACKET, htons(ETH_P_IP|ETH_P_ARP|ETH_P_ALL))用于抓包程序，过时了,不要用啊 (raw socket只有在Linux平台上才能发挥它本该有的功能。因为windows进行了限制，比如只能发送UDP包，只能发送正确的UDP包，不能冒充源地址，即源地址只能填本机地址) 通过一个例子来理解Raw_scoket的原理比如网卡收到了一个 14+20+8+100+4 的udp的以太网数据帧. 首先,网卡对该数据帧进行硬过滤(根据网卡的模式不同会有不同的动作,如果设置了promisc混杂模式的话,则不做任何过滤直接交给下一层输入例程,否则非本机mac或者广播mac会被直接丢弃).按照上面的例子,如果成功的话,会进入ip输入例程.但是在进入ip输入例程之前,系统会检查系统中是否有通过socket(AF_PACKET, SOCK_RAW, ..)创建的套接字.如果有的话并且协议相符,在这个例子中就是需要ETH_P_IP或者ETH_P_ALL类型.系统就给每个这样的socket接收缓冲区发送一个数据帧拷贝.然后进入下一步. 其次,进入了ip输入例程(ip层会对该数据包进行软过滤,就是检查校验或者丢弃非本机ip或者广播ip的数据包等,具体要参考源代码),例子中就是如果成功的话会进入udp输入例程.但是在交给udp输入例程之前,系统会检查系统中是否有通过socket(AF_INET, SOCK_RAW, ..)创建的套接字.如果有的话并且协议相符,在这个例子中就是需要IPPROTO_UDP类型.系统就给每个这样的socket接收缓冲区发送一个数据帧拷贝.然后进入下一步。 最后,进入udp输入例程 … ps:如果校验和出错的话,内核会直接丢弃该数据包的.而不会拷贝给sock_raw的套接字,因为校验和都出错了,数据肯定有问题的包括所有信息都没有意义了. socket的分类根据网路的7层模型，socket可以分为3种。 传输层socket-最常用的socket，非raw socket 网络层socket - 网络层 raw socket MAC层socket -收发数据链路层数据 raw socket的用途 通过raw socket来接收发向本机的ICMP,IGMP协议包,或者用来发送这些协议包. 接收发向本机但TCP/IP栈不能够处理的IP包：现在许多操作系统在实现网络部分的时候,通常只实现了常用的几种协议,如tcp,udp,icmp等,但象其它的如ospf,ggp等协议,操作系统往往没有实现,如果自己有必要编写位于其上的应用,就必须借助raw socket来实现,这是因为操作系统遇到自己不能够处理的数据包(ip头中的protocol所指定的上层协议不能处理)就将这个包交给协议对应的raw socket. 用来发送一些自己制定源地址等特殊作用的IP包(自己写IP头,TCP头等等)，因为内核不能识别的协议、格式等将传给原始套接字，因此，可以使用原始套接字定义用户自己的协议格式 内核接收网络数据后在rawsocket上处理原则 (1):对于UDP/TCP产生的IP数据包,内核不将它传递给任何网络层的原始套接字,而只是将这些数据直接交给对应的传输层UDP/TCP数据socket处理句柄。所以,只能通过MAC层原始套接字将第3个参数指定为htons(ETH_P_IP)来访问TCP/UDP数据。 (2):对于ICMP和EGP等使用IP数据包承载数据但又在传输层之下的协议类型的IP数据包,内核不管是否已经有注册了句柄来处理这些数据,都会将这些IP数据包复制一份传递给协议类型匹配的原始套接字.（网络层套接字能截获除TCP/UDP以外传输层协议号protocol相同的ip数据） (3):对于不能识别协议类型的数据包,内核进行必要的校验,然后会查看是否有类型匹配的原始套接字负责处理这些数据,如果有的话,就会将这些IP数据包复制一份传递给匹配的原始套接字,否则,内核将会丢弃这个IP数据包,并返回一个ICMP主机不可达的消息给源主机. (4):如果原始套接字bind绑定了一个地址,核心只将目的地址为本机IP地址的数包传递给原始套接字,如果某个原始套接字没有bind地址,核心就会把收到的所有IP数据包发给这个原始套接字. (5):如果原始套接字调用了connect函数,则核心只将源地址为connect连接的IP地址的IP数据包传递给这个原始套接字. (6):如果原始套接字没有调用bind和connect函数,则核心会将所有协议匹配的IP数据包传递给这个原始套接字. 啥是Packet套接字？？在linux环境中要从链路层（MAC）直接收发数据帧，可以通过libpcap与libnet两个动态库来分别完成收与发的工作。虽然它已被广泛使用，但在要求进行跨平台移植的软件中使用仍然有很多弊端。 这里介绍一种更为直接地、无须安装其它库的从MAC层收发数据帧的方式，即通过定义链路层的套接字来完成。 Packet套接字用于在MAC层上收发原始数据帧，这样就允许用户在用户空间完成MAC之上各个层次的实现。给无论是进行开发还是测试的人们带来了极大的便利性。 Packet套接字的定义方式与传送层的套接字定义类似，如下： packet_socket=socket(PF_PACKET,int socket_type,int protocol);（这个套接字的打开需要用户有root权限） RAW套接字和Packet套接字的比较 Packet套接字需要关联到一个特定的网卡直接发送，无需经过路由查找和地址解析。这是显然的，路由查找的目的无非也就是定位到一个网卡，现在网卡已经有了，直接发送即可，至于发到了哪里，能不能到达目的地，听天由命了。 RAW套接字这种RAW套接字发送的报文是需要经过路由查找的，只是说IP头以及IP上层的协议以及数据可以自己构造。 代码：用rawSocket来进行自定义IP报文的源地址。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netinet/ip.h&gt;#include &lt;netinet/udp.h&gt;#include&lt;memory.h&gt;#include&lt;stdlib.h&gt;#include &lt;linux/if_ether.h&gt;#include &lt;linux/if_packet.h&gt; // sockaddr_ll#include&lt;arpa/inet.h&gt;#include&lt;netinet/if_ether.h&gt;#include&lt;iomanip&gt;#include&lt;iostream&gt; // The packet length#define PCKT_LEN 100 //UDP的伪头部struct UDP_PSD_Header&#123; u_int32_t src; u_int32_t des; u_int8_t mbz; u_int8_t ptcl; u_int16_t len;&#125;;//计算校验和unsigned short csum(unsigned short *buf, int nwords)&#123; unsigned long sum; for (sum = 0; nwords &gt; 0; nwords--) &#123; sum += *buf++; &#125; sum = (sum &gt;&gt; 16) + (sum &amp; 0xffff); sum += (sum &gt;&gt; 16); return (unsigned short)(~sum);&#125; // Source IP, source port, target IP, target port from the command line argumentsint main(int argc, char *argv[])&#123; int sd; char buffer[PCKT_LEN] ; //查询www.chongfer.cn的DNS报文 unsigned char DNS[] = &#123; 0xd8, 0xcb , 0x01, 0x00, 0x00, 0x01, 0x00 ,0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x77, 0x77, 0x77, 0x08, 0x63, 0x68, 0x6f, 0x6e, 0x67, 0x66, 0x65, 0x72, 0x02, 0x63, 0x6e, 0x00, 0x00, 0x01, 0x00, 0x01 &#125;; struct iphdr *ip = (struct iphdr *) buffer; struct udphdr *udp = (struct udphdr *) (buffer + sizeof(struct iphdr)); // Source and destination addresses: IP and port struct sockaddr_in sin, din; int one = 1; const int *val = &amp;one; //缓存清零 memset(buffer, 0, PCKT_LEN); if (argc != 5) &#123; printf("- Invalid parameters!!!\n"); printf("- Usage %s &lt;source hostname/IP&gt; &lt;source port&gt; &lt;target hostname/IP&gt; &lt;target port&gt;\n", argv[0]); exit(-1); &#125; // Create a raw socket with UDP protocol sd = socket(AF_INET, SOCK_RAW, IPPROTO_UDP); if (sd &lt; 0) &#123; perror("socket() error"); // If something wrong just exit exit(-1); &#125; else printf("socket() - Using SOCK_RAW socket and UDP protocol is OK.\n"); //IPPROTO_TP说明用户自己填写IP报文 //IP_HDRINCL表示由内核来计算IP报文的头部校验和，和填充那个IP的id if (setsockopt(sd, IPPROTO_IP, IP_HDRINCL, val, sizeof(int))) &#123; perror("setsockopt() error"); exit(-1); &#125; else printf("setsockopt() is OK.\n"); // The source is redundant, may be used later if needed // The address family sin.sin_family = AF_INET; din.sin_family = AF_INET; // Port numbers sin.sin_port = htons(atoi(argv[2])); din.sin_port = htons(atoi(argv[4])); // IP addresses sin.sin_addr.s_addr = inet_addr(argv[1]); din.sin_addr.s_addr = inet_addr(argv[3]); // Fabricate the IP header or we can use the // standard header structures but assign our own values. ip-&gt;ihl = 5; ip-&gt;version = 4;//报头长度，4*32=128bit=16B ip-&gt;tos = 0; // 服务类型 ip-&gt;tot_len = ((sizeof(struct iphdr) + sizeof(struct udphdr)+sizeof(DNS))); //ip-&gt;id = htons(54321);//可以不写 ip-&gt;ttl = 64; // hops生存周期 ip-&gt;protocol = 17; // UDP ip-&gt;check = 0; // Source IP address, can use spoofed address here!!! ip-&gt;saddr = inet_addr(argv[1]); // The destination IP address ip-&gt;daddr = inet_addr(argv[3]); // Fabricate the UDP header. Source port number, redundant udp-&gt;source = htons(atoi(argv[2]));//源端口 // Destination port number udp-&gt;dest = htons(atoi(argv[4]));//目的端口 udp-&gt;len = htons(sizeof(struct udphdr)+sizeof(DNS));//长度 //forUDPCheckSum用来计算UDP报文的校验和用 //UDP校验和需要计算 伪头部、UDP头部和数据部分 char * forUDPCheckSum = new char[sizeof(UDP_PSD_Header) + sizeof(udphdr)+sizeof(DNS)+1]; memset(forUDPCheckSum, 0, sizeof(UDP_PSD_Header) + sizeof(udphdr) + sizeof(DNS) + 1); UDP_PSD_Header * udp_psd_Header = (UDP_PSD_Header *)forUDPCheckSum; udp_psd_Header-&gt;src = inet_addr(argv[1]); udp_psd_Header-&gt;des = inet_addr(argv[3]); udp_psd_Header-&gt;mbz = 0; udp_psd_Header-&gt;ptcl = 17; udp_psd_Header-&gt;len = htons(sizeof(udphdr)+sizeof(DNS)); memcpy(forUDPCheckSum + sizeof(UDP_PSD_Header), udp, sizeof(udphdr)); memcpy(forUDPCheckSum + sizeof(UDP_PSD_Header) + sizeof(udphdr), DNS, sizeof(DNS)); //ip-&gt;check = csum((unsigned short *)ip, sizeof(iphdr)/2);//可以不用算 //计算UDP的校验和，因为报文长度可能为单数，所以计算的时候要补0 udp-&gt;check = csum((unsigned short *)forUDPCheckSum,(sizeof(udphdr)+sizeof(UDP_PSD_Header)+sizeof(DNS)+1)/2); setuid(getpid());//如果不是root用户，需要获取权限 // Send loop, send for every 2 second for 2000000 count printf("Trying...\n"); printf("Using raw socket and UDP protocol\n"); printf("Using Source IP: %s port: %u, Target IP: %s port: %u.\n", argv[1], atoi(argv[2]), argv[3], atoi(argv[4])); std::cout &lt;&lt; "Ip length:" &lt;&lt; ip-&gt;tot_len &lt;&lt; std::endl; int count; //将DNS报文拷贝进缓存区 memcpy(buffer + sizeof(iphdr) + sizeof(udphdr), DNS, sizeof(DNS)); for (count = 1; count &lt;= 2000000; count++) &#123; if (sendto(sd, buffer, ip-&gt;tot_len, 0, (struct sockaddr *)&amp;din, sizeof(din)) &lt; 0) // Verify &#123; perror("sendto() error"); exit(-1); &#125; else &#123; printf("Count #%u - sendto() is OK.\n", count); sleep(2); &#125; &#125; close(sd); return 0;&#125; 结果：在IP为192.168.1.50的机器上冒充那个IP为192.168.1.71的机器向阿里的DNS服务器114.114.114.114上发送DNS查询报文 在IP为192.168.1.71的机器上收到阿里DNS服务器传回来的DNS报文 为啥是 unreachable - admin prohibited, length 413，经过查询是因为防火墙的原因。 关闭centos防火墙，service iptables stop关闭ubuntu防火墙，ufw stop。一般用户，只需如下设置：sudo apt-get install ufwsudo ufw enablesudo ufw default deny以上三条命令已经足够安全了，如果你需要开放某些服务，再使用sudo ufw allow开启。 关闭防火墙之后，再次查看，如下结果： 参考：https://blog.csdn.net/firefoxbug/article/details/7561159https://blog.51cto.com/a1liujin/1697465https://blog.csdn.net/yong61/article/details/8549654https://blog.csdn.net/dog250/article/details/83830872https://www.cnblogs.com/sammyliu/p/4981194.htmlhttps://blog.csdn.net/luchengtao11/article/details/73878760]]></content>
      <categories>
        <category>TCP/IP</category>
      </categories>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用travis-ci自动部署github上的项目]]></title>
    <url>%2F2019%2F02%2F03%2F%E4%BD%BF%E7%94%A8travis-ci%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2github%E4%B8%8A%E7%9A%84%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[travis-ci账户用github账户登录，找到对应项目进行勾选，如下图。 生成token，如下图。 配置token，如下图。（配置私密的环境变量时一定要加密，因为会显示在日志中且能够被他人看到） 还需要添加一些环境变量使起更方便(地址别填错了)，如下图。 ###有个问题：hexo g 之后并没有在travis中看到相关信息，难道非要用git推送吗？还有.travis.yml应该放在哪，我现在放在了blogs目录下。 答案：项目应该有两个分支，一个源码分支，一个发布之后的分支，.travis.yml应该放在源码分支的根目录下。 参考https://www.cnblogs.com/morang/p/7228488.htmlhttps://blog.csdn.net/woblog/article/details/51319364http://lujiahao.tk/2018/06/27/Travis%20CI%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/ 为博客添加一个新分支hexo-source，并设置为默认分支，这样就是用两个分支进行管理了。执行git add .、git commit -m “”、git push origin hexo来提交hexo网站源文件依次执行hexo g和hexo d生成静态网页部署至Github上 hexo备份参考: https://www.jianshu.com/p/57b5a384f234 https://www.jianshu.com/p/a27e9761ecf3]]></content>
      <categories>
        <category>TravisCI</category>
      </categories>
      <tags>
        <tag>TravisCI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker化秒杀疑云]]></title>
    <url>%2F2019%2F01%2F25%2FDocker%E5%8C%96%E7%A7%92%E6%9D%80%E7%96%91%E4%BA%91%2F</url>
    <content type="text"><![CDATA[为什么用docker-compose就有一堆错误，而一个一个起容器就不会有错误呢？所有的操作都一样啊。。莫非跟网关有关系吗？？一个用br-xxxxx，一个用docker0。docker-compose用的网关是br-10e925ab0d49这种形式的。而且每次br还不一样。没有docker0作为网关 下面是报的一些错误： 1234567Caused by: com.rabbitmq.client.AuthenticationFailureException: ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile. at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:342) ~[amqp-client-4.0.3.jar!/:4.0.3] at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:909) ~[amqp-client-4.0.3.jar!/:4.0.3] at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:859) ~[amqp-client-4.0.3.jar!/:4.0.3] at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:799) ~[amqp-client-4.0.3.jar!/:4.0.3] at org.springframework.amqp.rabbit.connection.AbstractConnectionFactory.createBareConnection(AbstractConnectionFactory.java:352) ~[spring-rabbit-1.7.4.RELEASE.jar!/:na] 原因：默认安装好rabbitmq就有服务（windows服务），但没有web监控（localhost:15672）;默认的端口就是5672，用户guest密码guest，但这个用户名只能在本机访问，如果要网络访问，需要做用户管理；参考：https://blog.csdn.net/lwkcn/article/details/25086467得知，就是写错用户名和密码了。 将rabbitmq的172.21.6.57改为localhost之后 报错信息变成了 12345678910111213141516171819202122232425262019-01-21 02:32:22.362 ERROR 1 --- [cTaskExecutor-2] o.s.a.r.l.SimpleMessageListenerContainer : Failed to check/redeclare auto-delete queue(s).org.springframework.amqp.AmqpConnectException: java.net.ConnectException: Connection refused (Connection refused) at org.springframework.amqp.rabbit.support.RabbitExceptionTranslator.convertRabbitAccessException(RabbitExceptionTranslator.java:62) ~[spring-rabbit-1.7.4.RELEASE.jar!/:na] at org.springframework.amqp.rabbit.connection.AbstractConnectionFactory.createBareConnection(AbstractConnectionFactory.java:368) ~[spring-rabbit-1.7.4.RELEASE.jar!/:na] at org.springframework.amqp.rabbit.connection.CachingConnectionFactory.createConnection(CachingConnectionFactory.java:573) ~[spring-rabbit-1.7.4.RELEASE.jar!/:na] at org.springframework.amqp.rabbit.core.RabbitTemplate.doExecute(RabbitTemplate.java:1430) ~[spring-rabbit-1.7.4.RELEASE.jar!/:na] at org.springframework.amqp.rabbit.core.RabbitTemplate.execute(RabbitTemplate.java:1411) ~[spring-rabbit-1.7.4.RELEASE.jar!/:na] at org.springframework.amqp.rabbit.core.RabbitTemplate.execute(RabbitTemplate.java:1387) ~[spring-rabbit-1.7.4.RELEASE.jar!/:na] at org.springframework.amqp.rabbit.core.RabbitAdmin.getQueueProperties(RabbitAdmin.java:336) ~[spring-rabbit-1.7.4.RELEASE.jar!/:na] at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.redeclareElementsIfNecessary(SimpleMessageListenerContainer.java:1171) ~[spring-rabbit-1.7.4.RELEASE.jar!/:na] at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:1422) [spring-rabbit-1.7.4.RELEASE.jar!/:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]Caused by: java.net.ConnectException: Connection refused (Connection refused) at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_111] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_111] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_111] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_111] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_111] at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_111] at com.rabbitmq.client.impl.SocketFrameHandlerFactory.create(SocketFrameHandlerFactory.java:50) ~[amqp-client-4.0.3.jar!/:4.0.3] at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:907) ~[amqp-client-4.0.3.jar!/:4.0.3] at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:859) ~[amqp-client-4.0.3.jar!/:4.0.3] at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:799) ~[amqp-client-4.0.3.jar!/:4.0.3] at org.springframework.amqp.rabbit.connection.AbstractConnectionFactory.createBareConnection(AbstractConnectionFactory.java:352) ~[spring-rabbit-1.7.4.RELEASE.jar!/:na] ... 8 common frames omitted 应用间网络还不完善，应该使用网络隔离，app在front，其余都在back。docker network create front命令是创建网络的。 默认的rabbitmq:3镜像没有management，应该用rabbitmq:management，我还怕是不是因为版本不一致导致的问题，专门用的rabbitmq:3.7.5-management，还是不行，还是报错。 123456789101112131415com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failureThe last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server. at sun.reflect.GeneratedConstructorAccessor31.newInstance(Unknown Source) ~[na:na] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_111] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_111] at com.mysql.jdbc.Util.handleNewInstance(Util.java:425) ~[mysql-connector-java-5.1.44.jar!/:5.1.44] at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989) ~[mysql-connector-java-5.1.44.jar!/:5.1.44] at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:341) ~[mysql-connector-java-5.1.44.jar!/:5.1.44] at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2189) ~[mysql-connector-java-5.1.44.jar!/:5.1.44] at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2222) ~[mysql-connector-java-5.1.44.jar!/:5.1.44] at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2017) ~[mysql-connector-java-5.1.44.jar!/:5.1.44] at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:779) ~[mysql-connector-java-5.1.44.jar!/:5.1.44] at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47) ~[mysql-connector-java-5.1.44.jar!/:5.1.44] at sun.reflect.GeneratedConstructorAccessor28.newInstance(Unknown Source) ~[na:na] 莫非是因为 在容器里边，他不认 localhost吗？？？必须写真实的IP才能找到。。。。 为什么没写到mysql中去呢 ，秒杀成功后吗，应该库存减少啊 但是并没有减少。。。。可能是因为发送了 秒杀请求，但是没有进行消息确认，所以 库存的数据量并不会减少。。。下一次又会读取到7 这个数量。。。。 现在的业务逻辑有个错误啊 ，我第一次秒杀成功后，在点击秒杀，确实说 非法请求了但是redis里还是减少了一个 商品 ，但是mysql里没有减少，说明没有发送成功秒杀的消息给mysql 所以并没有减少。 update s_goods set start_date=&#39;2019-01-22 13:55:50&#39; where id=1; 注意有八个小时的时差 数据库里是05点，页面上显示13点。 12345com.sjt.miaosha.rabbitmq.MQSender : send message:&#123;"goodsId":1,"user":&#123;"id":15812341234,"lastLoginDate":1525086640000,"loginCount":1,"nickname":"jack","password":"b631975e482e55b7692106f55a5b0a82","registerDate":1525086636000,"salt":"1a2b3c"&#125;&#125;2019-01-22 05:52:35.118 INFO 1 --- [io-65510-exec-8] o.s.a.r.c.CachingConnectionFactory : Created new connection: rabbitConnectionFactory#1ca81352:40/SimpleConnection@3f516387 [delegate=amqp://admin@172.21.6.57:5672/, localPort= 57984]2019-01-22 05:53:02.388 INFO 1 --- [io-65510-exec-3] c.s.miaosha.controller.LoginController : LoginVo [mobile=15812341234, password=ae6712ae454da07f5a86e2b5b21f4ea2]2019-01-22 05:53:10.551 INFO 1 --- [io-65510-exec-1] com.sjt.miaosha.rabbitmq.MQSender : send message:&#123;"goodsId":1,"user":&#123;"id":15812341234,"lastLoginDate":1525086640000,"loginCount":1,"nickname":"jack","password":"b631975e482e55b7692106f55a5b0a82","registerDate":1525086636000,"salt":"1a2b3c"&#125;&#125;]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[秒杀项目优化]]></title>
    <url>%2F2019%2F01%2F25%2F%E7%A7%92%E6%9D%80%E9%A1%B9%E7%9B%AE%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[逻辑 如何解决超卖问题（1）在sql加上判断防止数据边为负数（2）数据库加唯一索引防止用户重复购买（3）redis预减库存减少数据库访问 内存标记减少redis访问 请求先入队列缓冲，异步下单，增强用户体验 为什么threadlocal存储user对象，原理？ 并发编程中重要的问题就是数据共享，当你在一个线程中改变任意属性时，所有的线程都会因此受到影响，同时会看到第一个线程修改后的值。有时我们希望如此，比如：多个线程增大或减小同一个计数器变量。但是，有时我们希望确保每个线程，只能工作在它自己的线程实例的拷贝上，同时不会影响其他线程的数据。 举例： 举个例子，想象你在开发一个电子商务应用，你需要为每一个控制器处理的顾客请求，生成一个唯一的事务ID，同时将其传到管理器或DAO的业务方法中，以便记录日志。一种方案是将事务ID作为一个参数，传到所有的业务方法中。但这并不是一个好的方案，它会使代码变得冗余。你可以使用ThreadLocal类型的变量解决这个问题。首先在控制器或者任意一个预处理器拦截器中生成一个事务ID然后在ThreadLocal中 设置事务ID，最后，不论这个控制器调用什么方法，都能从threadlocal中获取事务ID而且这个应用的控制器可以同时处理多个请求，同时在框架 层面，因为每一个请求都是在一个单独的线程中处理的，所以事务ID对于每一个线程都是唯一的，而且可以从所有线程的执行路径获取运行结果可以看出每个线程都在维护自己的变量：123456789101112131415161718192021222324252627 Starting Thread: 0 : Fri Sep 21 23:05:34 CST 2018&lt;br&gt; Starting Thread: 2 : Fri Sep 21 23:05:34 CST 2018&lt;br&gt; Starting Thread: 1 : Fri Jan 02 05:36:17 CST 1970&lt;br&gt; Thread Finished: 1 : Fri Jan 02 05:36:17 CST 1970&lt;br&gt; Thread Finished: 0 : Fri Sep 21 23:05:34 CST 2018&lt;br&gt; Thread Finished: 2 : Fri Sep 21 23:05:34 CST 2018&lt;br&gt;``` 局部线程通常使用在这样的情况下，当你有一些对象并不满足线程安全，但是你想避免在使用synchronized关键字。 块时产生的同步访问，那么，让每个线程拥有它自己的对象实例 注意：局部变量是同步或局部线程的一个好的替代，它总是能够保证线程安全。唯一可能限制你这样做的是你的应用设计约束&lt;br&gt; 所以设计threadlocal存储user不会对对象产生影响，每次进来一个请求都会产生自身的线程变量来存储### Redis* Key的优化。```java 关键伪代码生成rediskey, objects包括ucid、用户输入入参、分页信息等等public static String builder(String prefix, Object... objects) &#123; String input = JSONObject.toJSONString(Arrays.asList(objects)); String output = Util.md5_16(input); return prefix+output;&#125;cacheRedis.setex(key,EXPIRE_TIME_2S,info); 设计优点：借鉴spring-data-redis将入参通用为objects…序列化，然后将JsonString Md5压缩为16位，这里主要由于在秒杀开始时，redis数据会出现大量缓存列表数据，redis储存100w个value长度为32位,key长度为16位的数据时，需要使用个130MB内存，如果key的长度为32位时需要160MB左右的内存，所以压缩key的长度在这种场景很有必要。 不要什么都放在redis中像列表页缓存，切勿为了减少redis的开销，将数据库每一列放到redis中，在redis中查询汇总，例如：每个秒杀资源都放在redis中，秒杀资源页需要10次redis链接才能完成一次列表页的组装。这样做会将服务器的qps成几何倍数的扩大到与redis的qps中造成系统获取不到redis连接资源 redis的库存如何与数据库的库存保持一致redis的数量不是库存,他的作用仅仅只是为了阻挡多余的请求透穿到DB，起到一个保护的作用因为秒杀的商品有限，比如10个，让1万个请求区访问DB是没有意义的，因为最多也就只能10个请求下单成功，所有这个是一个伪命题，我们是不需要保持一致的。 为什么redis数量会减少为负数 12345678 //预见库存 long stock = redisService.decr(GoodsKey.getMiaoshaGoodsStock,""+goodsId) ;if(stock &lt;0)&#123; localOverMap.put(goodsId, true);return Result.error(CodeMsg.MIAO_SHA_OVER);&#125;假如redis的数量为1,这个时候同时过来100个请求，大家一起执行decr数量就会减少成-99这个是正常的进行优化后改变了sql写法和内存写法则不会出现上述问题 redis 分布式锁实现方法 redis分布式锁解决什么问题1.一个进程中的多个线程,多个线程并发访问同一个资源的时候,如何解决线程安全问题。2.一个分布式架构系统中的两个模块同时去访问一个文件对文件进行读写操作3.多个应用对同一条数据做修改的时候,如何保证数据的安全性在但一个进程中,我们可以用到synchronized、lock之类的同步操作去解决,但是对于分布式架构下多进程的情况下,如何做到跨进程的锁。就需要借助一些第三方手段来完成 我用了四种方法 ，分别指出了不同版本的缺陷以及演进的过程 orderclosetaskV1—-&gt;&gt;版本没有操作，在分布式系统中会造成同一时间，资源浪费而且很容易出现并发问题V2—&gt;&gt;版本加了分布式redis锁，在访问核心方法前，加入redis锁可以阻塞其他线程访问,可以很好的处理并发问题,但是缺陷就是如果机器突然宕机，或者线路波动等，就会造成死锁，一直不释放等问题V3版本–&gt;&gt;很好的解决了这个问题v2的问题，就是加入时间对比如果当前时间已经大与释放锁的时间说明已经可以释放这个锁重新在获取锁，setget方法可以把之前的锁去掉在重新获取,旧值在于之前的值比较，如果无变化说明这个期间没有人获取或者操作这个redis锁，则可以重新获取V4—-&gt;&gt;采用成熟的框架redisson,封装好的方法则可以直接处理，但是waittime记住要这只为0 Mysql 需注意 因为秒杀，大促，打折等活动进行频繁，所以需要单独建立秒杀_….表来管理否则会经常进行回归 RabbitMQ 订单处理队列rabbitmq请求先入队缓冲，异步下单，增强用户体验请求出队，生成订单，减少库存客户端定时轮询检查是否秒杀成功 rabbitmq如何做到消息不重复不丢失即使服务器重启-1.exchange持久化-2.queue持久化-3.发送消息设置MessageDeliveryMode.persisent这个也是默认的行为-4.手动确认 事务 RPC事务补偿当集中式进行服务化RPC演进成分布式的时候，事务则成为了进行分布式的一个痛点，本项目的做法为：1.进行流程初始化，当分别调用不用服务化接口的时候，成功则进行流程，失败则返回并进行状态更新将订单状态变为回滚2.使用定时任务不断的进行处理rollback的订单进行回滚 Mavenmaven隔离就是在开发中，把各个环境的隔离开来，一般分为 本地（local） 开发(dev) 测试(test) 线上(prod) 在环境部署中为了防止人工修改的弊端！ spring.profiles.active=@activatedProperties@ 架构1234567891011121314151617181920212223242526项目进行dubbo+ZK改造 ├── miaosha-admin 登录模块 │ ├── pom.xml │ └── miaosha-admin-api │ └── miaosha-admin-service │ └── miaosha-admin-web │ └── miaosha-common │ │ ├── miaosha-order 订单秒杀模块 │ ├── pom.xml │ └── miaosha-order-api │ └── miaosha-order-service │ └── miaosha-order-web │ └── miaosha-order-common │ │ ├── miaosha-message 消息模块 │ ├── pom.xml │ └── miaosha-message-api │ └── miaosha-message-service │ └── miaosha-message-web │ └── miaosha-message-common │]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zero-Copy]]></title>
    <url>%2F2019%2F01%2F25%2FZero-Copy%2F</url>
    <content type="text"><![CDATA[这是一篇关于“零拷贝”的英文文献，写的非常好。有时间翻译一下。https://developer.ibm.com/articles/j-zerocopy/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker化秒杀项目九九八十一难]]></title>
    <url>%2F2019%2F01%2F20%2FDocker%E5%8C%96%E7%A7%92%E6%9D%80%E9%A1%B9%E7%9B%AE%E4%B9%9D%E4%B9%9D%E5%85%AB%E5%8D%81%E4%B8%80%E9%9A%BE%2F</url>
    <content type="text"><![CDATA[Docker化Mysql在我的Docker秒杀项目中，第一开始我都是把执行脚本和创建数据库文件都先弄到一个镜像中去，然后基于这个新镜像在起docker容器，但是后来一想，这种方法很麻烦，有没有一种更好的方法呢？通过一篇文章，有了灵感。 Volume持久化首先，我们可以在创建数据库容器的时候指定容器内数据库创建docker run --name mysqldock -e MYSQL_ROOT_PASSWORD=admin -e MYSQL_DATABASE=inst1 -d -p 3066:3066 mysql通过-e MYSQL_DATABASE这个配置就可以在创建容器时创建inst1这个数据库。 官方文档对docker commit的说明是：The commit operation will not include any data contained in volumes mounted inside the container. 意思是commit操作并不会包含容器内挂载数据卷中的数据变化。难道是因为mysql容器的挂载数据卷引起的？也就是说数据库容器/var/lib/mysql路径作为volume挂载在host主机上/var/lib/docker/volumes/该容器ID。 但是通过-e参数创建的数据库就还在，继续看docker commit官方文档It can be useful to commit a container’s file changes or settings into a new image.对于文件变更(在容器内新创建一个文件)和设置（-e MYSQL_DATABASE=inst1）有用。 到此，原来我的疑问也就明朗了。 那么我们如何来对数据库容器进行持久化呢？我们可以通过docker提供的数据挂载来实现。docker的数据挂载分为三种，volume, bind mount和tmpfs，关于三种的具体说明，强烈推荐大家看一下官网的文档。这边简单说明一下： volume是由docker默认及推荐的挂载方式，volume由docker直接管理，同一个volume可以共享给多个容器使用，volume和容器的生命周期完全独立，容器删除时volume仍然存在，除非使用docker volume相应命令删除volume；缺点是volume在宿主机上比较难定位，在宿主机上直接操作volume比较困难。 bind mount是直接将宿主机文件系统上的文件路径映射到容器中，两边双向同步，显而易见，有缺点也有优点，优点是可以直接访问，也可以被别的程序使用，比如我们打包一个本地应用到本地/target路径，我们就可以把这个路径使用bind mount的方式挂在到依赖他的应用的docker容器中，这样本地应用打包后，docker里的数据卷也会同时更新；缺点也是显而易见的，因为你可以把任何文件路径使用bind mount的方式绑定到容器中，这样有可能一些安全问题，比如把宿主机的系统文件绑定到容器中。 tmpfs这种方式是使用宿主机的内存作为存储，不会写到宿主机的文件系统中，和前两种区别较大。 什么是Volume呢？为了能够保存（持久化）数据以及共享容器间的数据，Docker提出了Volume的概念。简单来说，Volume就是目录或者文件，它可以绕过默认的联合文件系统，而以正常的文件或者目录的形式存在于宿主机上。 注意：-v 的时候不能使用同一个源地址，否则创建起来的容器会闪退（再创建一个data目录来区分） 下面是各种磨难 为什么我自己新创建的数据库镜像mysql默认是关闭的？必须的通过/etc/init.d/mysql start 才能启动呢？？？ 在脚本中还不行，必须得手动进入容器内进行执行该命令，百思不得姐啊 。。。。。通过docker logs发现MySQL Community Server 5.7.24 is started.mysql: [Warning] Using a password on the command line interface can be insecure./mysql/createDatabase.sh: 21: /mysql/createDatabase.sh: cannot open miaosha.sql: No such file发现数据库已经起来了，但是说不能打开sql文件，难道是因为权限问题，？？？？必须在容器内才能拥有权限？？？（经证实。不是权限问题，是sql文件路径问题） 这次我手动在容器内执行 脚本 就能成功，（第一次说找不到sql文件，第二次就成功了，到底是什么鬼？？？？？？）[info] A MySQL Server is already started.mysql: [Warning] Using a password on the command line interface can be insecure.mysql: [Warning] Using a password on the command line interface can be insecure. 最后，终于成功了，是因为路径的原因。原来为mysql -u${USERNAME} -p${PASSWORD} miaosha &lt; miaosha.sql改为 mysql -u${USERNAME} -p${PASSWORD} miaosha &lt; /mysql/miaosha.sql docker容器启动后马上退出经查阅资料：Docker容器同时只能管理一个进程，如果这个进程退出那么容器也就退出了，但这不表示容器只能运行一个进程(其他进程可在后台运行)，但是要使容器不退出必须有一个前台执行的进程。我这里通过，tail -f /dev/null这个命令 不让它退出。 挂载某一文件的问题 挂载之前需要改变文件权限为777，要不会引起修改宿主机上的文件 会引起内容不同步的问题参考https://blog.csdn.net/qq_21816375/article/details/78032521 成功，数据库启动正常docker run -d -p 3306:3306 --name miaoshaMysql -e MYSQL_ROOT_PASSWORD=123 -v / home/ubuntu16/miaosha-docker-compose/config/miaoshaMysql:/mysql -v /home/ubuntu16/miaosha-docker-compose/data/miaoshaMysql:/var/lib/mysql sjt157/miaoshamysql:v4 Docker化Rabbitmq遇到这个错误[error] Error when reading /var/lib/rabbitmq/.erlang.cookie: eacces 用这个命令解决chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie 最终成功启动命令docker run -d --name miaosharabbit -p 5672:5672 -p 15672:15672 -v /home/ubuntu16/miaosha-dock er-compose/config/miaoshaRabbit:/rabbitmq sjt157/miaosharabbitmq:v4 Docker化redis持久化参考mysql的持久化，我们也需要对redis进行持久化，这样我们的容器退出后才不会所有东西全部丢失。参考https://blog.csdn.net/haoxiaoyong1014/article/details/80241677 最终命令docker run -d -p 6379:6379 --name miaosharedis sjt157/miaosharedis:v2 docker-compose我们每次都docker run很麻烦，所以用docker-compose。1234567891011121314151617181920212223242526272829303132333435363738394041version: '2.1'services: mysql: image: sjt157/miaoshamysql:v4 ports: - "3306:3306" environment: - MYSQL_ROOT_PASSWORD=123 # - MYSQL_DATABASE=root volumes: - /home/ubuntu16/miaosha-docker-compose/config/miaoshaMysql:/mysql - /home/ubuntu16/miaosha-docker-compose/data/miaoshaMysql:/var/lib/mysql - /home/ubuntu16/miaosha-docker-compose/logs/miaoshaMysql:/logs restart: on-failure redis: image: sjt157/miaosharedis:v2 ports: - "6379:6379" volumes: - /home/ubuntu16/miaosha-docker-compose/data/miaoshaRedis:/data - /home/ubuntu16/miaosha-docker-compose/logs/miaoshaRedis:/logs restart: on-failure rabbitmq: image: sjt157/miaosharabbitmq:v4 ports: - "5672:5672" - "15672:15672" volumes: - /home/ubuntu16/miaosha-docker-compose/config/miaoshaRabbit:/rabbitmq - /home/ubuntu16/miaosha-docker-compose/logs/miaoshaRabbit:/logs restart: on-failure miaosha: image: sjt157/miaoshaapp:v2 ports: - "65534:65510" depends_on: - mysql - redis - rabbitmq restart: on-failure 通过docker-compose up -d启动成功123456CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7152d7c2f012 sjt157/miaoshaapp:v2 &quot;java -jar -Dloader.…&quot; 4 minutes ago Up 4 minutes 0.0.0.0:65534-&gt;65510/tcp miaosha-docker-compose_miaosha_1_6a294f3118b7f9fada42b2b9 sjt157/miaosharabbitmq:v4 &quot;docker-entrypoint.s…&quot; 4 minutes ago Up 4 minutes 4369/tcp, 0.0.0.0:5672-&gt;5672/tcp, 5671/tcp, 25672/tcp, 0.0.0.0:15672-&gt;15672/tcp miaosha-docker-compose_rabbitmq_1_b1a645775dbfda0590907e62 sjt157/miaosharedis:v2 &quot;docker-entrypoint.s…&quot; 4 minutes ago Up 4 minutes 0.0.0.0:6379-&gt;6379/tcp miaosha-docker-compose_redis_1_6d55cb3068980974f7a50423 sjt157/miaoshamysql:v4 &quot;docker-entrypoint.s…&quot; 4 minutes ago Up 4 minutes 0.0.0.0:3306-&gt;3306/tcp, 33060/tcp miaosha-docker-compose_mysql_1_321cab31d805 通过docker-compose down 停止。 参考https://www.jianshu.com/p/530d00f97cbfhttps://blog.csdn.net/haoxiaoyong1014/article/details/80241677]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty之高水位、低水位]]></title>
    <url>%2F2019%2F01%2F19%2FNetty%E4%B9%8B%E9%AB%98%E6%B0%B4%E4%BD%8D%E3%80%81%E4%BD%8E%E6%B0%B4%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[我们一听高低水位，肯定首先想到的肯定就是“大爸”（三峡大坝），我们都知道，三峡的水位曾经到达过172.85米，最高限制水位175米，其实这就是三峡的高水位，如果再进水，那么恐怕啥都不好用了。同理，Netty中有缓冲区，就相当于大坝起存储缓冲作用。当缓冲区达到一定大小时则不能写入，避免被撑爆。Netty中提供 了writeBufferLowWaterMark和writeBufferHighWaterMark选项用来控制高低水位。可以通过监控当前写缓冲区的水位状况，来避免占用大量的内存，因为ChannelOutboundBuffer本身是无界的，所以用的时候要注意。（感觉跟Netty提供的Traffic Shaping流量整形功能有点像）。我们来看一下源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * WriteBufferWaterMark is used to set low water mark and high water mark for the write buffer. * &lt;p&gt; * If the number of bytes queued in the write buffer exceeds the * &#123;@linkplain #high high water mark&#125;, &#123;@link Channel#isWritable()&#125; * will start to return &#123;@code false&#125;. * &lt;p&gt; * If the number of bytes queued in the write buffer exceeds the * &#123;@linkplain #high high water mark&#125; and then * dropped down below the &#123;@linkplain #low low water mark&#125;, * &#123;@link Channel#isWritable()&#125; will start to return * &#123;@code true&#125; again. */public final class WriteBufferWaterMark &#123; private static final int DEFAULT_LOW_WATER_MARK = 32 * 1024; private static final int DEFAULT_HIGH_WATER_MARK = 64 * 1024; public static final WriteBufferWaterMark DEFAULT = new WriteBufferWaterMark(DEFAULT_LOW_WATER_MARK, DEFAULT_HIGH_WATER_MARK, false); private final int low; private final int high; /** * Create a new instance. * * @param low low water mark for write buffer. * @param high high water mark for write buffer */ public WriteBufferWaterMark(int low, int high) &#123; this(low, high, true); &#125; /** * This constructor is needed to keep backward-compatibility. */ WriteBufferWaterMark(int low, int high, boolean validate) &#123; if (validate) &#123; if (low &lt; 0) &#123; throw new IllegalArgumentException(&quot;write buffer&apos;s low water mark must be &gt;= 0&quot;); &#125; if (high &lt; low) &#123; throw new IllegalArgumentException( &quot;write buffer&apos;s high water mark cannot be less than &quot; + &quot; low water mark (&quot; + low + &quot;): &quot; + high); &#125; &#125; this.low = low; this.high = high; &#125; /** * Returns the low water mark for the write buffer. */ public int low() &#123; return low; &#125; /** * Returns the high water mark for the write buffer. */ public int high() &#123; return high; &#125; @Override public String toString() &#123; StringBuilder builder = new StringBuilder(55) .append(&quot;WriteBufferWaterMark(low: &quot;) .append(low) .append(&quot;, high: &quot;) .append(high) .append(&quot;)&quot;); return builder.toString(); &#125;&#125; 从注释里头可以看到控制的是写缓冲，高低水位这两个参数控制的是Channel.isWritable()方法，当超过高水位时返回False，降到低水位之下后，又重新可写。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980private static final AtomicIntegerFieldUpdater&lt;ChannelOutboundBuffer&gt; UNWRITABLE_UPDATER = AtomicIntegerFieldUpdater.newUpdater(ChannelOutboundBuffer.class, &quot;unwritable&quot;); private volatile int unwritable; /** * Returns &#123;@code true&#125; if and only if &#123;@linkplain #totalPendingWriteBytes() the total number of pending bytes&#125; did * not exceed the write watermark of the &#123;@link Channel&#125; and * no &#123;@linkplain #setUserDefinedWritability(int, boolean) user-defined writability flag&#125; has been set to * &#123;@code false&#125;. */ public boolean isWritable() &#123; return unwritable == 0; &#125; /** * Get how many bytes must be drained from the underlying buffer until &#123;@link #isWritable()&#125; returns &#123;@code true&#125;. * This quantity will always be non-negative. If &#123;@link #isWritable()&#125; is &#123;@code true&#125; then 0. */ public long bytesBeforeWritable() &#123; long bytes = totalPendingSize - channel.config().getWriteBufferLowWaterMark(); // If bytes is negative we know we are writable, but if bytes is non-negative we have to check writability. // Note that totalPendingSize and isWritable() use different volatile variables that are not synchronized // together. totalPendingSize will be updated before isWritable(). if (bytes &gt; 0) &#123; return isWritable() ? 0 : bytes; &#125; return 0; &#125; /** * Decrement the pending bytes which will be written at some point. * This method is thread-safe! */ void decrementPendingOutboundBytes(long size) &#123; decrementPendingOutboundBytes(size, true, true); &#125; private void decrementPendingOutboundBytes(long size, boolean invokeLater, boolean notifyWritability) &#123; if (size == 0) &#123; return; &#125; long newWriteBufferSize = TOTAL_PENDING_SIZE_UPDATER.addAndGet(this, -size); if (notifyWritability &amp;&amp; newWriteBufferSize &lt; channel.config().getWriteBufferLowWaterMark()) &#123; setWritable(invokeLater); &#125; &#125; private void setWritable(boolean invokeLater) &#123; for (;;) &#123; final int oldValue = unwritable; final int newValue = oldValue &amp; ~1; if (UNWRITABLE_UPDATER.compareAndSet(this, oldValue, newValue)) &#123; if (oldValue != 0 &amp;&amp; newValue == 0) &#123; fireChannelWritabilityChanged(invokeLater); &#125; break; &#125; &#125; &#125; private void fireChannelWritabilityChanged(boolean invokeLater) &#123; final ChannelPipeline pipeline = channel.pipeline(); if (invokeLater) &#123; Runnable task = fireChannelWritabilityChangedTask; if (task == null) &#123; fireChannelWritabilityChangedTask = task = new Runnable() &#123; @Override public void run() &#123; pipeline.fireChannelWritabilityChanged(); &#125; &#125;; &#125; channel.eventLoop().execute(task); &#125; else &#123; pipeline.fireChannelWritabilityChanged(); &#125; &#125; bytesBeforeWritable方法先判断totalPendingSize是否大于lowWatermark，如果不大于则返回0，如果大于且isWritable返回true则返回0，否则返回差值decrementPendingOutboundBytes方法会判断，如果notifyWritability为true且newWriteBufferSize &lt; channel.config().getWriteBufferLowWaterMark()，则调用setWritablesetWritable(invokeLater)setWritable会更新unwritable，如果是从非0变为0，还会触发fireChannelWritabilityChanged进行通知123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081ChannelOutboundBuffer.setUnwritablenetty-all-4.1.25.Final-sources.jar!/io/netty/channel/ChannelOutboundBuffer.java private static final AtomicIntegerFieldUpdater&lt;ChannelOutboundBuffer&gt; UNWRITABLE_UPDATER = AtomicIntegerFieldUpdater.newUpdater(ChannelOutboundBuffer.class, &quot;unwritable&quot;); private volatile int unwritable; /** * Returns &#123;@code true&#125; if and only if &#123;@linkplain #totalPendingWriteBytes() the total number of pending bytes&#125; did * not exceed the write watermark of the &#123;@link Channel&#125; and * no &#123;@linkplain #setUserDefinedWritability(int, boolean) user-defined writability flag&#125; has been set to * &#123;@code false&#125;. */ public boolean isWritable() &#123; return unwritable == 0; &#125; /** * Get how many bytes can be written until &#123;@link #isWritable()&#125; returns &#123;@code false&#125;. * This quantity will always be non-negative. If &#123;@link #isWritable()&#125; is &#123;@code false&#125; then 0. */ public long bytesBeforeUnwritable() &#123; long bytes = channel.config().getWriteBufferHighWaterMark() - totalPendingSize; // If bytes is negative we know we are not writable, but if bytes is non-negative we have to check writability. // Note that totalPendingSize and isWritable() use different volatile variables that are not synchronized // together. totalPendingSize will be updated before isWritable(). if (bytes &gt; 0) &#123; return isWritable() ? bytes : 0; &#125; return 0; &#125; /** * Increment the pending bytes which will be written at some point. * This method is thread-safe! */ void incrementPendingOutboundBytes(long size) &#123; incrementPendingOutboundBytes(size, true); &#125; private void incrementPendingOutboundBytes(long size, boolean invokeLater) &#123; if (size == 0) &#123; return; &#125; long newWriteBufferSize = TOTAL_PENDING_SIZE_UPDATER.addAndGet(this, size); if (newWriteBufferSize &gt; channel.config().getWriteBufferHighWaterMark()) &#123; setUnwritable(invokeLater); &#125; &#125; private void setUnwritable(boolean invokeLater) &#123; for (;;) &#123; final int oldValue = unwritable; final int newValue = oldValue | 1; if (UNWRITABLE_UPDATER.compareAndSet(this, oldValue, newValue)) &#123; if (oldValue == 0 &amp;&amp; newValue != 0) &#123; fireChannelWritabilityChanged(invokeLater); &#125; break; &#125; &#125; &#125; private void fireChannelWritabilityChanged(boolean invokeLater) &#123; final ChannelPipeline pipeline = channel.pipeline(); if (invokeLater) &#123; Runnable task = fireChannelWritabilityChangedTask; if (task == null) &#123; fireChannelWritabilityChangedTask = task = new Runnable() &#123; @Override public void run() &#123; pipeline.fireChannelWritabilityChanged(); &#125; &#125;; &#125; channel.eventLoop().execute(task); &#125; else &#123; pipeline.fireChannelWritabilityChanged(); &#125; &#125; bytesBeforeUnwritable方法先判断highWatermark与totalPendingSize的差值，totalPendingSize大于等于highWatermark，则返回0；如果小于highWatermark，且isWritable为true，则返回差值，否则返回0incrementPendingOutboundBytes方法判断如果newWriteBufferSize &gt; channel.config().getWriteBufferHighWaterMark()，则调用setUnwritable(invokeLater)setUnwritable会更新unwritable，如果是从0变为非0，还会触发fireChannelWritabilityChanged进行通知 综上，lowWatermark及highWatermark分别在decrementPendingOutboundBytes及incrementPendingOutboundBytes方法里头用到(目前应该是这两个方法起作用)，当小于lowWatermark或者大于highWatermark的时候，分别触发setWritable及setUnwritable，更改ChannelOutboundBuffer的unwritable字段，进而影响isWritable方法；在isWritable为true的时候会立马执行写请求，当返回false的时候，写请求会被放入队列等待isWritable为true时才能执行这些堆积的写请求。 实践出真知在Netty的物联网网关中，就可以通过new WriteBufferWaterMark(32 1024 1024, 64 1024 1024)来设置水位线，防止服务器处理能力极其低下但连接正常时，造成channel中缓存大量数据影响网关性能。具体设置多大我们可以根据我们的应用需要支持多少连接数和系统资源进行合理规划。高水位线和低水位线是字节数，默认高水位是64K，低水位是32K。 参考https://www.jianshu.com/p/a1166c34ae46https://gitee.com/willbeahero/IOTGatehttps://www.jianshu.com/p/890525ff73cb]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MYSQL之binlog]]></title>
    <url>%2F2019%2F01%2F18%2FMYSQL%E4%B9%8Bbinlog%2F</url>
    <content type="text"><![CDATA[什么是binlogbinlog日志用于记录所有更新了数据或者已经潜在更新了数据（例如，没有匹配任何行的一个DELETE）的所有语句。语句以“事件”的形式保存，它描述数据更改。 binlog作用因为有了数据更新的binlog，所以可以用于实时备份，与master/slave主从复制结合。 binlog的形式二进制日志包括两类文件：二进制日志索引文件（文件名后缀为.index）用于记录所有的二进制文件；二进制日志文件（文件名后缀为.00000*）记录数据库所有的DDL和DML(除了数据查询语句)语句事件。 命令show variables like ‘%log_bin%’；show variables like ‘log_bin’;show variables like ‘%general_log%’;show variables like ‘%log_%’; 如何开启binlog在my.cnf中加入以下内容（加入的位置不对的话会报错，说不认识这些选项）：应该在[mysqld]选项下加入1234567server-id=1 #server-id表示单个结点的id，这里由于只有一个结点，所以可以把id随机指定为一个数，这里将id设置成1。若集群中有多个结点，则id不能相同binlog_format = MIXED #binlog日志格式，mysql默认采用statement，建议使用mixedlog-bin = /var/lib/mysql/mysql-bin #binlog日志文件expire_logs_days = 7 #binlog过期清理时间max_binlog_size = 100m #binlog每个日志文件大小binlog_cache_size = 4m #binlog缓存大小max_binlog_cache_size = 512m #最大binlog缓存大小 重新启动用 service mariadb restart就好了 Tips（艰难的mysql重启过程）（1）重启mysql遇到的问题1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768 （1）通过rpm包安装的MySQL通过下面这样的方式重启 service mysqld restart/etc/inint.d/mysqld start（2）从源码包安装的MySQL（我的服务器上是/usr/bin）// Linux关闭MySQL的命令$mysql_dir/bin/mysqladmin -uroot -p shutdown// linux启动MySQL的命令$mysql_dir/bin/mysqld_safe &amp;用这个命令启动的时候一直卡在[1] 11513[root@rabbitmq bin]# 190118 19:30:00 mysqld_safe Logging to &apos;/var/log/mariadb/mariadb.log&apos;.190118 19:30:00 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql为什么呢？？？？查看了一下日志，说没有找到mysql-bin.index文件190118 19:23:41 [Note] /usr/libexec/mysqld (mysqld 5.5.56-MariaDB) starting as process 11421 ...190118 19:23:41 mysqld_safe mysqld from pid file /var/run/mariadb/mariadb.pid ended190118 19:30:00 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql190118 19:30:00 [ERROR] mysqld: File &apos;/data/mysql/mysql-bin.index&apos; not found (Errcode: 2)190118 19:30:00 [ERROR] Aborting在/data/mysql/创建了一个mysql-bin.index文件再次启动，还是报错190118 19:30:00 [Note] /usr/libexec/mysqld (mysqld 5.5.56-MariaDB) starting as process 11740 ...190118 19:30:00 mysqld_safe mysqld from pid file /var/run/mariadb/mariadb.pid ended190118 19:39:04 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql190118 19:39:04 [ERROR] mysqld: File &apos;/data/mysql/mysql-bin.index&apos; not found (Errcode: 13)190118 19:39:04 [ERROR] Aborting190118 19:39:04 [Note] /usr/libexec/mysqld: Shutdown completeerrcode13，一般就是权限问题，通过命令 chown -R mysql:mysql /data/mysql/mysql-bin.index将原来为 -rw-r--r--. 1 root root 0 1月 18 19:38 mysql-bin.index改为-rw-r--r--. 1 mysql mysql 0 1月 18 19:38 mysql-bin.index再次启动，还是报错190118 19:45:05 [Note] /usr/libexec/mysqld (mysqld 5.5.56-MariaDB) starting as process 12690 ...190118 19:45:05 mysqld_safe mysqld from pid file /var/run/mariadb/mariadb.pid ended190118 19:45:38 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql190118 19:45:38 [ERROR] mysqld: File &apos;/data/mysql/mysql-bin.~rec~&apos; not found (Errcode: 13)190118 19:45:38 [ERROR] MYSQL_BIN_LOG::open_purge_index_file failed to open register file.190118 19:45:38 [ERROR] MYSQL_BIN_LOG::open_index_file failed to sync the index file.190118 19:45:38 [ERROR] Aborting190118 19:45:38 [Note] /usr/libexec/mysqld: Shutdown complete换一种思路，更改log日志存在地点log-bin = /var/lib/mysql/mysql-bin 这次启动时 抱如下信息190118 19:52:58 [Note] /usr/libexec/mysqld (mysqld 5.5.56-MariaDB) starting as process 13554 ...190118 19:52:58 mysqld_safe mysqld from pid file /var/run/mariadb/mariadb.pid ended190118 19:56:48 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql190118 19:56:48 [Note] /usr/libexec/mysqld (mysqld 5.5.56-MariaDB) starting as process 13855 ...190118 19:56:48 InnoDB: The InnoDB memory heap is disabled190118 19:56:48 InnoDB: Mutexes and rw_locks use GCC atomic builtins190118 19:56:48 InnoDB: Compressed tables use zlib 1.2.7190118 19:56:48 InnoDB: Using Linux native AIO190118 19:56:48 InnoDB: Initializing buffer pool, size = 128.0M190118 19:56:48 InnoDB: Completed initialization of buffer pool190118 19:56:48 InnoDB: highest supported file format is Barracuda.190118 19:56:48 InnoDB: Waiting for the background threads to start190118 19:56:49 Percona XtraDB (http://www.percona.com) 5.5.52-MariaDB-38.3 started; log sequence number 1597945190118 19:56:50 [Note] Plugin &apos;FEEDBACK&apos; is disabled.190118 19:56:50 [Note] Server socket created on IP: &apos;0.0.0.0&apos;.190118 19:56:50 [Note] /usr/libexec/mysqld: ready for connections.Version: &apos;5.5.56-MariaDB&apos; socket: &apos;/var/lib/mysql/mysql.sock&apos; port: 3306 MariaDB Server还是报错，是不是mysqld_safe这个命令有毒啊。。。。换 service mariadb restart这个命令。。。 （2）每次服务器（数据库）重启，服务器会调用flush logs;，会新创建一个binlog日志 （3）mysqld_safe脚本执行的基本流程:1234567891011121314151617181920212223241、查找basedir和ledir。2、查找datadir和my.cnf。3、对my.cnf做一些检查，具体检查哪些选项请看附件中的注释。4、解析my.cnf中的组[mysqld]和[mysqld_safe]并和终端里输入的命令合并。5、调用parse_arguments函数解析用户传递的所有参数($@)。6、对系统日志和错误日志的判断和相应处理具体可以参考附件中的注释，及选项--err-log参数的赋值。7、对选项--user，--pid-file，--socket及--port进行处理及赋值，保证启动时如果不给出这些参数它也会有值。8、启动mysqld.a)启动时会判断一个进程号是否存在，如果存在那么就在错误日志中记录&quot;A mysqld process already exists&quot;并且退出。b)如不存在就删除进程文件，如果删除不了，那么就在错误日志中记录&quot;Fatal error: Can&apos;t remove the pid file&quot;并退出。9、启动时对表进行检查。如果启动的时候检查表的话设置key_buffer and sort_buffer会提高速度并且减少磁盘空间的使用。也可以使用myisam-recover选项恢复出错的myisam表。10、如果启动时你什么参数都没有给，那么它会选用一些特定的参数启动，具体哪些参数请看附件注释。11、如果服务器异常关闭，那么会restart。最后用三步来总结检查环境检查配置选项启动及启动后的处理总结：选用mysqld_safe启动的好处。1、mysqld_safe增加了一些安全特性，例如当出现错误时重启服务器并向错误日志文件写入运行时间信息。2、如果有的选项是mysqld_safe 启动时特有的，那么可以终端指定，如果在配置文件中指定需要放在[mysqld_safe]组里面，放在其他组不能被正确解析。3、mysqld_safe启动能够指定内核文件大小 ulimit -c $core_file_size以及打开的文件的数量ulimit -n $size。4、MySQL程序首先检查环境变量，然后检查配置文件，最后检查终端的选项，说明终端指定选项优先级最高 （4）命令systemctl enable mysqld.service一直报错Failed to execute operation: No such file or directory在CentOS7中已经不在支持mysql，就算你已经安装了，CentOS7还是表示很嫌弃使用 systemctl status mariadb.serviceCentOS7不支持 mysqld，无法启动了，所以才需要装mariadb 到现在可以发现终于把binlog给启动了show variables like ‘%log_bin%’; 常用binlog操作命令12345show master logs;查看所有binlog日志列表show master status;查看master状态，即最后(最新)一个binlog日志的编号名称，及其最后一个操作事件pos结束点(Position)值flush logs; 刷新log日志，自此刻开始产生一个新编号的binlog日志文件reset master;重置(清空)所有binlog日志show binlog events in &apos;mysql-bin.000002&apos;;查看binlog日志内容（以表格形式） binlog的三种工作模式（1）Row level （我的数据库上默认的是ROW） ROW是基于行级别的,他会记录每一行记录的变化,就是将每一行的修改都记录到binlog里面,记录的非常详细，但sql语句并没有在binlog里。 日志中会记录每一行数据被修改的情况，然后在slave端对相同的数据进行修改。在replication里面也不会因为存储过程触发器等造成Master-Slave数据不一致的问题,但是有个致命的缺点日志量比较大.由于要记录每一行的数据变化,当执行update语句后面不加where条件的时候或alter table的时候,产生的日志量是相当的大。 （2）Statement level（默认） 每一条被修改数据的sql都会记录到master的bin-log中，slave在复制的时候sql进程会解析成和原来master端执行过的相同的sql再次执行 优点：解决了 Row level下的缺点，不需要记录每一行的数据变化，减少bin-log日志量，节约磁盘IO，提高新能 缺点：在statement模式下，由于他是记录的执行语句，所以，为了让这些语句在slave端也能正确执行，那么他还必须记录每条语句在执行的时候的一些相关信息，也就是上下文信息，以保证所有语句在slave端被执行的时候能够得到和在master端执行时候相同的结果。另外就是，由于mysql现在发展比较快，很多的新功能不断的加入，使mysql的复制遇到了不小的挑战，自然复制的时候涉及到越复杂的内容，bug也就越容易出现。在statement中，目前已经发现不少情况会造成Mysql的复制出现问题，主要是修改数据的时候使用了某些特定的函数或者功能的时候会出现，比如：sleep()函数在有些版本中就不能被正确复制，在存储过程中使用了last_insert_id()函数，可能会使slave和master上得到不一致的id等等。由于row是基于每一行来记录的变化，所以不会出现，类似的问题。 （3）Mixed（混合模式） 结合了Row level和Statement level的优点。 在默认情况下是statement,但是在某些情况下会切换到row状态，如当一个DML更新一个ndb引擎表，或者是与时间用户相关的函数等。在主从的情况下，在主机上如果是STATEMENT模式，那么binlog就是直接写now()，然而如果这样的话，那么从机进行操作的时间，也执行now()，但明显这两个时间不会是一样的，所以对于这种情况就必须把STATEMENT模式更改为ROW模式，因为ROW模式会直接写值而不是写语句（该案例是错误的，即使是STATEMENT模式也可以使用now()函数，具体原因以后再分析）。同样ROW模式还可以减少从机的相关计算，如在主机中存在统计写入等操作时，从机就可以免掉该计算把值直接写入从机。 MySQL的日志（主要是Binlog）对系统性能的影响（1）日志产生的性能影响由于日志的记录带来的直接性能损耗就是数据库系统中最为昂贵的IO资源。 MySQL的日志主要包括错误日志（ErrorLog），更新日志（UpdateLog），二进制日志（Binlog），查询日志（QueryLog），慢查询日志（SlowQueryLog）等。特别注意：更新日志是老版本的MySQL才有的，目前已经被二进制日志替代。 在默认情况下，系统仅仅打开错误日志，关闭了其他所有日志，以达到尽可能减少IO损耗提高系统性能的目的。但是在一般稍微重要一点的实际应用场景中，都至少需要打开二进制日志，因为这是MySQL很多存储引擎进行增量备份的基础，也是MySQL实现复制的基本条件。有时候为了进一步的mysql性能优化，定位执行较慢的SQL语句，很多系统也会打开慢查询日志来记录执行时间超过特定数值（由我们自行设置）的SQL语句。 一般情况下，在生产系统中很少有系统会打开查询日志。因为查询日志打开之后会将MySQL中执行的每一条Query都记录到日志中，会该系统带来比较大的IO负担，而带来的实际效益却并不是非常大。一般只有在开发测试环境中，为了定位某些功能具体使用了哪些SQL语句的时候，才会在短时间段内打开该日志来做相应的分析。所以，在MySQL系统中，会对性能产生影响的MySQL日志（不包括各存储引擎自己的日志）主要就是Binlog了。 一般企业binlog模式的选择：互联网公司使用MySQL的功能较少（不用存储过程、触发器、函数），选择默认的Statement level；用到MySQL的特殊功能（存储过程、触发器、函数）则选择Mixed模式；用到MySQL的特殊功能（存储过程、触发器、函数），又希望数据最大化一直则选择Row模式； mysql对于日志格式的选定原则:如果是采用 INSERT，UPDATE，DELETE 等直接操作表的情况，则日志格式根据 binlog_format 的设定而记录,如果是采用 GRANT，REVOKE，SET PASSWORD 等管理语句来做的话，那么无论如何 都采用 SBR 模式记录。 参考https://blog.csdn.net/z1988316/article/details/7883147?utm_source=blogxgwz2https://blog.csdn.net/intelrain/article/details/80451120https://blog.csdn.net/weixin_38187469/article/details/79273962https://blog.csdn.net/keda8997110/article/details/50895171]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之ShutDownHook]]></title>
    <url>%2F2019%2F01%2F18%2FJava%E4%B9%8BShutDownHook%2F</url>
    <content type="text"><![CDATA[线上JVM挂掉怎么办，不要怕，有优雅停机在线上Java程序中经常遇到进程程挂掉，一些状态没有正确的保存下来，这时候就需要在JVM关掉的时候执行一些清理现场的代码。Java中得ShutdownHook提供了比较好的方案。 什么时候可以用钩子1）程序正常退出2）使用System.exit()3）终端使用Ctrl+C触发的中断4）系统关闭5）使用Kill pid命令干掉进程（kill -9 pid不会调用钩子）6) OOM宕机 如何添加钩子Runtime.addShutdownHook(Thread hook) 12345678在JDK中方法的声明：public void addShutdownHook(Thread hook)参数hook -- 一个初始化但尚未启动的线程对象，注册到JVM钩子的运行代码。异常IllegalArgumentException -- 如果指定的钩已被注册，或如果它可以判定钩已经运行或已被运行IllegalStateException -- 如果虚拟机已经是在关闭的过程中SecurityException -- 如果存在安全管理器并且它拒绝的RuntimePermission（“shutdownHooks”） 12345678910111213141516/** * JVM的关闭钩子--JVM正常关闭才会执行 */ public static void addHook()&#123; Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() &#123; public void run() &#123; //清空缓存信息 System.out.println("网关正常关闭前执行 清空所有缓存信息..............................."); ClientChannelCache.clearAll(); CacheQueue.clearIpCountRelationCache(); CacheQueue.clearMasterChannelCache(); &#125; &#125;)); &#125; 注意的地方同一个JVM最好只使用一个关闭钩子，而不是每个服务都使用一个不同的关闭钩子，使用多个关闭钩子可能会出现当前这个钩子所要依赖的服务可能已经被另外一个关闭钩子关闭了。为了避免这种情况，建议关闭操作在单个线程中串行执行，从而避免了再关闭操作之间出现竞态条件或者死锁等问题。 参考https://www.cnblogs.com/shuo1208/p/5871224.htmlhttps://www.cnblogs.com/langtianya/p/4300282.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-CPU使用率高达100%]]></title>
    <url>%2F2019%2F01%2F18%2FNetty-CPU%E4%BD%BF%E7%94%A8%E7%8E%87%E9%AB%98%E8%BE%BE100%2F</url>
    <content type="text"><![CDATA[Netty做数据转发，但是CPU使用率太高了。 解决方案：System.setProperty(&quot;org.jboss.netty.epollBugWorkaround&quot;, &quot;true&quot;);//避免CPU使用率达到100% Googling for sun.nio.ch.WindowsSelectorImpl$SubSelector high cpu brings up a few hits from as last at 2015. Are you running an older version of Netty? Also see https://github.com/netty/netty/issues/3857 - you may want to try running with -Dorg.jboss.netty.epollBugWorkaround=true. https://stackoverflow.com/questions/37881109/netty-eats-100-of-cpu]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git团队协作中经常使用哪些术语]]></title>
    <url>%2F2019%2F01%2F18%2FGit%E5%9B%A2%E9%98%9F%E5%8D%8F%E4%BD%9C%E4%B8%AD%E7%BB%8F%E5%B8%B8%E4%BD%BF%E7%94%A8%E5%93%AA%E4%BA%9B%E6%9C%AF%E8%AF%AD%2F</url>
    <content type="text"><![CDATA[ACK — acknowledgement, i.e. agreed/accepted change AFAIK As Far As I Know （就我所知） ASAP abbr. As Soon As Possible 尽快; BTW By The Way 顺便 Ditto. 最经典的一句 出自《人鬼情未了》 ——I love you ——Ditto. EOL end of life 寿命终止 FYI abbr. For Your Information 供参考; IMHO In My Humble Opinion 恕我直言 IIRC — if I recall correctly IANAL — “ I am not a lawyer ”, but I smell licensing issues LGTM LOOKS GOOD TO ME Review完别人的PR，没有问题 NACK/NAK — negative acknowledgement, i.e. disagree with change and/or conceptOk, thanks :) PTAL Please Take A Look 帮我看下，一般都是请别人 review 自己的 PR RFC — request for comments, i.e. I think this is a good idea, lets discuss REVNO 修订版号 revision number 的缩写 thx Thanks That makes sense to me cc 对我来说有意义 TR,DR Too long,dont read 一翻到底 WIP work in progress 工作在进行中;]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql之s_profiling]]></title>
    <url>%2F2019%2F01%2F18%2FMysql%E4%B9%8Bs-profiling%2F</url>
    <content type="text"><![CDATA[今天在听Cetus课程的时候，看到了老师说了set profiling=1； 并不知道这是干什么的，来记录一下。 Query Profiler 来定位一条 Query 的性能瓶颈，这里我们再详细介绍一下 Profiling 的用途及使用方法。 要想优化一条 Query，我们就需要清楚的知道这条 Query 的性能瓶颈到底在哪里，是消耗的 CPU计算太多，还是需要的的 IO 操作太多？要想能够清楚的了解这些信息，在 MySQL 5.0 和 MySQL 5.1正式版中已经可以非常容易做到了，那就是通过 Query Profiler 功能。 MySQL 的 Query Profiler 是一个使用非常方便的 Query 诊断分析工具，通过该工具可以获取一条Query 在整个执行过程中多种资源的消耗情况，如 CPU，IO，IPC，SWAP 等，以及发生的 PAGE FAULTS，CONTEXT SWITCHE 等等，同时还能得到该 Query 执行过程中 MySQL 所调用的各个函数在源文件中的位置。 下面我们看看 Query Profiler 的具体用法。 开启 profiling 参数 执行Query 在开启 Query Profiler 功能之后，MySQL 就会自动记录所有执行的 Query 的 profile 信息了。 取系统中保存的所有 Query 的 profile 概要信息通过执行 “SHOW PROFILE” 命令获取当前系统中保存的多个 Query 的 profile 的概要信息。 针对单个 Query 获取详细的 profile 信息上面的例子中是获取 CPU 和 Block IO 的消耗，非常清晰，对于定位性能瓶颈非常适用。希望得到取其他的信息，都可以通过执行 “SHOW PROFILE *** FOR QUERY n” 来获取，各位读者朋友可以自行测试熟悉。 参考https://www.cnblogs.com/ggjucheng/archive/2012/11/15/2772058.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库分片、分区、分表、分库傻傻分不清楚]]></title>
    <url>%2F2019%2F01%2F18%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E7%89%87%E3%80%81%E5%88%86%E5%8C%BA%E3%80%81%E5%88%86%E8%A1%A8%E3%80%81%E5%88%86%E5%BA%93%E5%82%BB%E5%82%BB%E5%88%86%E4%B8%8D%E6%B8%85%E6%A5%9A%2F</url>
    <content type="text"><![CDATA[分片什么是分片在分布式存储系统中，数据需要分散存储在多台设备上，数据分片（Sharding）就是用来确定数据在多台存储设备上分布的技术。数据分片要达到三个目的： 分布均匀，即每台设备上的数据量要尽可能相近； 负载均衡，即每台设备上的请求量要尽可能相近； 扩缩容时产生的数据迁移尽可能少。分片相关概念 逻辑库(schema) 通常对实际应用来说，并不需要知道中间件的存在，业务开发人员只需要知道数据库的概念，所以数据库中间件可以被看做是一个或多个数据库集群构成的逻辑库。 逻辑表（table） 既然有逻辑库，那么就会有逻辑表，分布式数据库中，对应用来说，读写数据的表就是逻辑表。逻辑表，可以是数据切分后，分布在一个或多个分片库中，也可以不做数据切分，不分片，只有一个表构成。 分片表 是指那些原有的很大数据的表，需要切分到多个数据库的表，这样，每个分片都有一部分数据，所有分片构成了完整的数据。 总而言之就是需要进行分片的表。 非分片表 一个数据库中并不是所有的表都很大，某些表是可以不用进行切分的，非分片是相对分片表来说的，就是那些不需要进行数据切分的表。 分片节点(dataNode) 数据切分后，一个大表被分到不同的分片数据库上面，每个表分片所在的数据库就是分片节点（dataNode）。 节点主机(dataHost) 数据切分后，每个分片节点（dataNode）不一定都会独占一台机器，同一机器上面可以有多个分片数据库，这样一个或多个分片节点（dataNode）所在的机器就是节点主机（dataHost）,为了规避单节点主机并发数限制，尽量将读写压力高的分片节点（dataNode）均衡的放在不同的节点主机（dataHost）。 分片规则(rule) 前面讲了数据切分，一个大表被分成若干个分片表，就需要一定的规则，这样按照某种业务规则把数据分到某个分片的规则就是分片规则，数据切分选择合适的分片规则非常重要，将极大的避免后续数据处理的难度。 优点缺点应用场景任何技术都是在合适的场合下能发挥应有的作用。 Sharding 也一样。联机游戏、IM、BSP 都是比较适合 Sharding 的应用场景。其共性是抽象出来的数据对象之间的关联数据很小。比如IM ，每个用户如果抽象成一个数据对象，完全可以独立存储在任何一个地方，数据对象是 Share Nothing 的；再比如 Blog 服务提供商的站点内容，基本为用户生成内容(UGC)，完全可以把不同的用户隔离到不同的存储集合，而对用户来说是透明的。 这个”Share Nothing” 是从数据库集群中借用的概念，举例来说，有些类型的数据粒度之间就不是 “Share Nothing” 的，比如类似交易记录的历史表信息，如果一条记录中既包含卖家信息与买家信息，如果随着时间推移，买、卖家会分别与其它用户继续进行交易，这样不可避免的两个买卖家的信息会分布到不同的 Sharding DB 上，而这时如果针对买卖家查询，就会跨越更多的 Sharding ，开销就会比较大。 Sharding 并不是数据库扩展方案的银弹，也有其不适合的场景，比如处理事务型的应用就会非常复杂。对于跨不同DB的事务，很难保证完整性，得不偿失。所以，采用什么样的 Sharding 形式，不是生搬硬套的。我们知道每台机器无论配置多么好它都有自身的物理上限，所以当我们应用已经能触及或远远超出单台机器的某个上限的时候，我们惟有寻找别的机器的帮助或者继续升级的我们的硬件，但常见的方案还是横向扩展, 通过添加更多的机器来共同承担压力。我们还得考虑当我们的业务逻辑不断增长，我们的机器能不能通过线性增长就能满足需求？Sharding可以轻松的将计算，存储，I/O并行分发到多台机器上，这样可以充分利用多台机器各种处理能力，同时可以避免单点失败，提供系统的可用性，进行很好的错误隔离。 分片的种类数据的切分（Sharding）根据其切分规则的类型，可以分为两种切分模式。 （1）一种是按照不同的表（或者Schema）来切分到不同的数据库（主机）之上，这种切分可以称之为数据的垂直（纵向）切分 （2）另外一种则是根据表中的数据的逻辑关系，将同一个表中的数据按照某种条件拆分到多台数据库（主机）上面，这种切分称之为数据的水平（横向）切分。 分片的方法数据分片一般都是使用Key或Key的哈希值来计算Key的分布，常见的几种数据分片的方法如下： 划分号段。这种一般适用于Key为整型的情况，每台设备上存放相同大小的号段区间，如把Key为[1, 10000]的数据放在第一台设备上，把Key为[10001, 20000]的数据放在第二台设备上，依次类推。这种方法实现很简单，扩容也比较方便，成倍增加设备即可，如原来有N台设备，再新增N台设备来扩容，把每台老设备上一半的数据迁移到新设备上，原来号段为[1, 10000]的设备，扩容后只保留号段[1, 5000]的数据，把号段为[5001, 10000]的数据迁移到一台新增的设备上。此方法的缺点是数据可能分布不均匀，如小号段数据量可能比大号段的数据量要大，同样的各个号段的热度也可能不一样，导致各个设备的负载不均衡；并且扩容也不够灵活，只能成倍地增加设备。 取模。这种方法先计算Key的哈希值，再对设备数量取模（整型的Key也可直接用Key取模），假设有N台设备，编号为0~N-1，通过Hash(Key)%N就可以确定数据所在的设备编号。这种方法实现也非常简单，数据分布和负载也会比较均匀，可以新增任何数量的设备来扩容。主要的问题是扩容的时候，会产生大量的数据迁移，比如从N台设备扩容到N+1台，绝大部分的数据都要在设备间进行迁移。 检索表。在检索表中存储Key和设备的映射关系，通过查找检索表就可以确定数据分布，这里的检索表也可以比较灵活，可以对每个Key都存储映射关系，也可结合号段划分等方法来减小检索表的容量。这样可以做到数据均匀分布、负载均衡和扩缩容数据迁移量少。缺点是需要存储检索表的空间可能比较大，并且为了保证扩缩容引起的数据迁移量比较少，确定映射关系的算法也比较复杂。 一致性哈希。一致性哈希算法（Consistent Hashing）在1997年由麻省理工学院提出的一种分布式哈希（DHT）实现算法，设计目标是为了解决因特网中的热点(Hot Spot)问题，该方法的详细介绍参考此处http://blog.csdn.net/sparkliang/article/details/5279393。一致性哈希的算法简单而巧妙，很容易做到数据均分布，其单调性也保证了扩缩容的数据迁移是比较少的。 通过上面的对比，在这个系统选择一致性哈希的方法来进行数据分片。 分区什么是分区 数据分区是一种物理数据库的设计技术，它的目的是为了在特定的SQL操作中减少数据读写的总量以缩减响应时间。分区并不是生成新的数据表，而是将表的数据均衡分摊到不同的硬盘，系统或是不同服务器存储介子中，实际上还是一张表。另外，分区可以做到将表的数据均衡到不同的地方，提高数据检索的效率，降低数据库的频繁IO压力值 包括水平分区和垂直分区 优点 相对于单个文件系统或是硬盘，分区可以存储更多的数据； 数据管理比较方便，比如要清理或废弃某年的数据，就可以直接删除该日期的分区数据即可； 精准定位分区查询数据，不需要全表扫描查询，大大提高数据检索效率； 可跨多个分区磁盘查询，来提高查询的吞吐量； 在涉及聚合函数查询时，可以很容易进行数据的合并； 缺点什么时候分区 一张表的查询速度已经慢到影响使用的时候。 sql经过优化 数据量大 表中的数据是分段的 对数据的操作往往只涉及一部分数据，而不是所有的数据 分片和分区的区别与联系有的时候，Sharding 也被近似等同于水平分区(Horizontal Partitioning)，网上很多地方也用水平分区来指代 Sharding，但我个人认为二者之间实际上还是有区别的。的确，Sharding 的思想是从分区的思想而来，但数据库分区基本上是数据对象级别的处理，比如表和索引的分区，每个子数据集上能够有不同的物理存储属性，还是单个数据库范围内的操作，而 Sharding 是能够跨数据库，甚至跨越物理机器的。 Sharding 分区 存储依赖 可跨越DB、物理机器 可跨越表空间，不同的物理属性，不能跨DB存储 数据划分 时间、范围、面向服务等 范围、Hash、列表、混合分区等 存储方式 分布式 集中式 扩展性 Scale Out Scale Up 可用性 无单点 存在单点（DB本身） 价格 低廉 适中（DAS）甚至昂贵（SAN） 应用场景 常见于WEB2.0网站 多数传统应用 分表什么是分表就是把一张表按一定的规则分解成N个具有独立存储空间的实体表。系统读写时需要根据定义好的规则得到对应的字表明，然后操作它。 优点缺点什么时候分表一张表的查询速度已经慢到影响使用的时候。 sql经过优化 数据量大当频繁插入或者联合查询时，速度变慢 分区和分表的区别与联系分区从逻辑上来讲只有一张表，而分表则是将一张表分解成多张表。 分区和分表的目的都是减少数据库的负担，提高表的增删改查效率。 分区只是一张表中的数据的存储位置发生改变，分表是将一张表分成多张表。 当访问量大，且表数据比较大时，两种方式可以互相配合使用。 当访问量不大，但表数据比较多时，可以只进行分区。 分库什么是分库一旦分表，一个库中的表会越来越多 优点缺点什么时候分库单台DB的存储空间不够 随着查询量的增加单台数据库服务器已经没办法支撑 一般优化思路垂直分库–&gt;水平分库–&gt;读写分离 分库之后的问题事务的支持，分库分表，就变成了分布式事务 join时跨库，跨表的问题 分库分表，读写分离使用了分布式，分布式为了保证强一致性，必然带来延迟，导致性能降低，系统的复杂度变高。 问题解决方案对于不同的方式之间没有严格的界限，特点不同，侧重点不同。需要根据实际情况，结合每种方式的特点来进行处理。 选用第三方的数据库中间件（Atlas，Mycat，TDDL，DRDS），同时业务系统需要配合数据存储的升级。 参考https://blog.csdn.net/qq_28289405/article/details/80576614https://blog.csdn.net/weixin_38074050/article/details/78640004http://blog.sina.com.cn/s/blog_72ef7bea0101cjtb.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《码出高效》笔记]]></title>
    <url>%2F2019%2F01%2F17%2F%E8%AF%BB%E3%80%8A%E7%A0%81%E5%87%BA%E9%AB%98%E6%95%88%E3%80%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[基于 0与1的信号处理为我们带来了缤纷多彩的计算机世界，随着基础材料和信 号处理技术的发展，未来计算机能够处理的基础信号将不仅仅是二进制信息。比如， 三进制（ 高电平、低电平、断电），甚至十进制信息，届时计算机世界又会迎来一次 全新的变革。 在要求绝对精确表示的业务场景下，比如金融行业的货币表示，推荐使用整型存 储其最小单位的值，展示时可以转换成该货币的常用单位，比如人民币使用分存储， 美元使用美分存储。在要求精确表示小数点 位的业务场景下，比如圆周率要求存储 小数点后 1000 位数字，使用单精度和双精度浮点数类型保存是难以做到的，这时推 荐采用数组保存小数部分的数据。在比较浮点数时，由于存在误差，往往会出现意料 之外的结果，所以禁止通过判断两个浮点数是否相等来控制某些业务流程。在数据库 中保存小数时，推荐使用 decimal 类型，禁止使用 float类型和 double 类型。因为这 两种类型在存储的时候，存在精度损失的问题。 程序在发送消息时，应用层接既定的协议打包数据 随后由传输层加 上双方的端口 ，由网络层加上双方的 IP 地址，由链路层加上双方的 MAC 地址 将数据拆分成数据帧 经过多个路由器 网关后 到达目标机器。简而言之 就是按 端口→ IP 地址→ MAC 地址 这样的路径进行数据的封装和发送 解包的时候反过 来操作即可 从经验上来看，在数据库层面的请求应答时间必须在 100ms以内，秒级的 SQL 查询通常存在巨大的性能提升空间，有如下应对方案，（1）建立合适的索引（2）排查连接资源未显式关闭的情形。 要特别注意在 ThreadLocal 或流式计算 中使用数据库连接的地方。（3）合并短的请求（4）合理拆分多个表 join SQL 是超过三个表则禁止 join 如果表结构建 得不合理，应用逻辑处理不当，业务模型抽象有问题 那么三表 join 的数据量由于笛 卡儿积操作会呈几何级数增加，所以不推荐这样的做法。另外，对于需要join 的字段， 数据类型应保持绝对一致。多表关联查询时，应确保被关联的字段要有索引。（5）使用临时表（6）应用层优化。 包括进行数据结构优化、并发多线程改造等。 除了开发人员造成的漏洞，近年来出现了一种 Self-XSS 的攻击方式。 Sel -XSS 是利用部分非开发人员不懂技术，黑客通过红包、奖品或者优惠券等形式 诱导用户 复制攻击者提供的恶意代码 并非占贴到浏览器的 Console 中运行 从而导致 xss 。由 Self-XSS 属于社会工程学攻击，技术上目前尚无有效防范机制 因此只能通过在 Console 中展示提醒文案来阻止用户执行未知代码。 包装类的存在解决了基本数据类型无法做到的事情比如 泛型类型 参数、序列化、类型转换、高频区间数据缓存。尤其是最后－项，我们都知道 Integer 会缓存－ 128到 127 之间的值，对于 Integer var=？在－ 128 127 之间的赋值， Integer 象由 ntegerCache.cache 产生，会复用已有对象，这个区间内的 Integer 值可以直接使 用＝＝进行判断，但是这个区间之外的所有数据都会在堆上产生，并不会复用已有对象， 这是一个大问题。因此，推荐所有包装类对象之间值的比较 全部使用 equals （）方法。该例很好地说明了 Long只是缓存了 -128 到127 之间的值，而 1000L 没有被缓存； 在将 Integer 最大缓存值改为 7777 后， 1001被成功缓存。合理掌握包装类的缓存策略， 防止遇到问题是一个方面，使自己的程序性能最大化，更是程序员的情怀所在。在选择使用包装类还是基本数据类型时，推荐使用如下方式：（1） 所有的POJO类属性必须使用包装属性类型（2） RPC方法的返回值和参数必须使用包装数据类型（3） 所有的局部变量推荐使用基本数据类型]]></content>
      <categories>
        <category>Books</category>
      </categories>
      <tags>
        <tag>Books</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么不应该使用ZooKeeper做服务发现]]></title>
    <url>%2F2019%2F01%2F17%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%BA%94%E8%AF%A5%E4%BD%BF%E7%94%A8ZooKeeper%E5%81%9A%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[本文作者通过ZooKeeper与Eureka作为 Service发现服务（注：WebServices 体系中的UDDI就是个发现服务）的优劣对比，分享了Knewton在云计算平台部署服务的经验。本文虽然略显偏激，但是看得出Knewton在云平台方面是非常有经验的，这篇文章从实践角度出发分别从云平台特点、CAP原理以及运维三个方面对比了ZooKeeper与Eureka两个系统作为发布服务的优劣，并提出了在云平台构建发现服务的方法论。 背景很多公司选择使用 ZooKeeper作为Service发现服务（Service Discovery），但是在构建Knewton（Knewton是一个提供个性化教育平台的公司、学校和出版商可以通过Knewton平台为学生提供自适应的学习材料）平台时，我们发现这是个根本性的错误。在这边文章中，我们将用我们在实践中遇到的问题来说明，为什么使用ZooKeeper做Service发现服务是个错误。 请留意服务部署环境让我们从头开始梳理。我们在部署服务的时候，应该首先考虑服务部署的平台（平台环境），然后才能考虑平台上跑的软件系统或者如何在选定的平台上自己构建一套系统。例如，对于云部署平台来说，平台在硬件层面的伸缩（注：作者应该指的是系统的冗余性设计，即系统遇到单点失效问题，能够快速切换到其他节点完成任务）与如何应对网络故障是首先要考虑的。当你的服务运行在大量服务器构建的集群之上时（注：原话为大量可替换设备），则肯定会出现单点故障的问题。对于knewton来说，我们虽然是部署在AWS上的，但是在过往的运维中，我们也遇到过形形色色的故障；所以，你应该把系统设计成“故障开放型”（expecting failure）的。其实有很多同样使用AWS的公司跟我们遇到了（同时有很多 书是介绍这方面的）相似的问题。你必须能够提前预料到平台可能会出现的问题如：意外故障（注：原文为box failure，只能意会到作者指的是意外弹出的错误提示框），高延迟与网络分割问题（注：原文为network partitions。意思是当网络交换机出故障会导致不同子网间通讯中断）——同时我们要能构建足够弹性的系统来应对它们的发生。 永远不要期望你部署服务的平台跟其他人是一样的！当然，如果你在独自运维一个数据中心，你可能会花很多时间与钱来避免硬件故障与网络分割问题，这是另一种情况了；但是在云计算平台中，如AWS，会产生不同的问题以及不同的解决方式。当你实际使用时你就会明白，但是，你最好提前应对它们（注：指的是 上一节说的意外故障、高延迟与网络分割问题）的发生。 ZooKeeper作为发现服务的问题ZooKeeper（注：ZooKeeper是著名Hadoop的一个子项目，旨在解决大规模分布式应用场景下，服务协调同步（Coordinate Service）的问题；它可以为同在一个分布式系统中的其他服务提供：统一命名服务、配置管理、分布式锁服务、集群管理等功能）是个伟大的开源项目，它很成熟，有相当大的社区来支持它的发展，而且在生产环境得到了广泛的使用；但是用它来做Service发现服务解决方案则是个错误。 在分布式系统领域有个著名的CAP定理（C- 数据一致性；A-服务可用性；P-服务对网络分区故障的容错性，这三个特性在任何分布式系统中不能同时满足，最多同时满足两个）；ZooKeeper是个CP的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性；但是它不能保证每次服务请求的可用性（注：也就 是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果）。但是别忘了，ZooKeeper是分布式协调服务，它的职责是保证数据（注：配置数据，状态数据）在其管辖下的所有服务之间保持同步、一致；所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了，如果是AP的，那么将会带来恐怖的后果（注：ZooKeeper就像交叉路口的信号灯一样，你能想象在交通要道突然信号灯失灵的情况吗？）。而且， 作为ZooKeeper的核心实现算法Zab，就是解决了分布式系统下数据如何在多个服务之间保持同步问题的。 作为一个分布式协同服务，ZooKeeper非常好，但是对于Service发现服务来说就不合适了；因为对于Service发现服务来说就算是返回了包含不实的信息的结果也比什么都不返回要好；再者，对于Service发现服务而言，宁可返回某服务5分钟之前在哪几个服务器上可用的信息，也不能因为暂时的网络故障而找不到可用的服务器，而不返回任何结果。所以说，用ZooKeeper来做Service发现服务是肯定错误的，如果你这么用就惨了！ 而且更何况，如果被用作Service发现服务，ZooKeeper本身并没有正确的处理网络分割的问题；而在云端，网络分割问题跟其他类型的故障一样的确会发生；所以最好提前对这个问题做好100%的准备。就像Jepsen在 ZooKeeper网站上发布的博客中所说：在ZooKeeper中，如果在同一个网络分区（partition）的节点数（nodes）数达不到 ZooKeeper选取Leader节点的“法定人数”时，它们就会从ZooKeeper中断开，当然同时也就不能提供Service发现服务了。 如果给ZooKeeper加上客户端缓存（注：给ZooKeeper节点配上本地缓存）或者其他类似技术的话可以缓解ZooKeeper因为网络故障造成节点同步信息错误的问题。Pinterest与 Airbnb公司就使用了这个方法来防止ZooKeeper故障发生。这种方式可以从表面上解决这个问题，具体地说，当部分或者所有节点跟ZooKeeper断开的情况下，每个节点还可以从本地缓存中获取到数据；但是，即便如此，ZooKeeper下所有节点不可能保证任何时候都能缓存所有的服务注册信息。如果 ZooKeeper下所有节点都断开了，或者集群中出现了网络分割的故障（注：由于交换机故障导致交换机底下的子网间不能互访）；那么ZooKeeper 会将它们都从自己管理范围中剔除出去，外界就不能访问到这些节点了，即便这些节点本身是“健康”的，可以正常提供服务的；所以导致到达这些节点的服务请求被丢失了。（注：这也是为什么ZooKeeper不满足CAP中A的原因） 更深层次的原因是，ZooKeeper是按照CP原则构建的，也就是说它能保证每个节点的数据保持一致，而为ZooKeeper加上缓存的做法的目的是为了让ZooKeeper变得更加可靠（available）；但是，ZooKeeper设计的本意是保持节点的数据一致，也就是CP。所以，这样一来，你可能既得不到一个数据一致的（CP）也得不到一个高可用的（AP）的Service发现服务了；因为，这相当于你在一个已有的CP系统上强制栓了一个AP的系统，这在本质上就行不通的！一个Service发现服务应该从一开始就被设计成高可用的才行！ 如果抛开CAP原理不管，正确的设置与维护ZooKeeper服务就非常的困难；错误会经常发生，导致很多工程被建立只是为了减轻维护ZooKeeper的难度。这些错误不仅存在与客户端而且还存在于ZooKeeper服务器本身。Knewton平台很多故障就是由于ZooKeeper使用不当而导致的。那些看似简单的操作，如：正确的重建观察者（reestablishing watcher）、客户端Session与异常的处理与在ZK窗口中管理内存都是非常容易导致ZooKeeper出错的。同时，我们确实也遇到过 ZooKeeper的一些经典bug：ZooKeeper-1159 与ZooKeeper-1576； 我们甚至在生产环境中遇到过ZooKeeper选举Leader节点失败的情况。这些问题之所以会出现，在于ZooKeeper需要管理与保障所管辖服务 群的Session与网络连接资源（注：这些资源的管理在分布式系统环境下是极其困难的）；但是它不负责管理服务的发现，所以使用ZooKeeper当 Service发现服务得不偿失。 做出正确的选择：Eureka的成功我们把Service发现服务从ZooKeeper切换到了Eureka平台，它是一个开源的服务发现解决方案，由Netflix公司开发。（注：Eureka由两个组件组成：Eureka服务器和Eureka客户端。Eureka服务器用作 服务注册服务器。Eureka客户端是一个java客户端，用来简化与服务器的交互、作为轮询负载均衡器，并提供服务的故障切换支持。）Eureka一开 始就被设计成高可用与可伸缩的Service发现服务，这两个特点也是Netflix公司开发所有平台的两个特色。（他们都在讨论Eureka）。自从切换工作开始到现在，我们实现了在生产环境中所有依赖于Eureka的产品没有下线维护的记录。我们也被告知过，在云平台做服务迁移注定要遇到失败；但是我们从这个例子中得到的经验是，一个优秀的Service发现服务在其中发挥了至关重要的作用！]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你了解Node.js吗？]]></title>
    <url>%2F2019%2F01%2F17%2F%E4%BD%A0%E4%BA%86%E8%A7%A3Node-js%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[如果你去年注意过技术方面的新闻，我敢说你至少看到node.js不下一两次。那么问题来了“node.js是什么？”。有些人没准会告诉你“这是一种通过JavaScript语言开发web服务端的东西”。如果这种晦涩解释还没把你搞晕，你没准会接着问：“为什么我们要用node.js？”，别人一般会告诉你：node.js有非阻塞，事件驱动I/O等特性，从而让高并发（high concurrency）在的轮询（Polling）和comet构建的应用中成为可能。 当你看完这些解释觉得跟看天书一样的时候，你估计也懒得继续问了。不过没事。我这篇文章就是在避开高端术语的同时，帮助你你理解node.js的。 浏览器给网站发请求的过程一直没怎么变过。当浏览器给网站发了请求。服务器收到了请求，然后开始搜寻被请求的资源。如果有需要，服务器还会查询一下数据库，最后把响应结果传回浏览器。不过，在传统的web服务器中（比如Apache），每一个请求都会让服务器创建一个新的进程来处理这个请求。 后来有了Ajax。有了Ajax，我们就不用每次都请求一个完整的新页面了，取而代之的是，每次只请求需要的部分页面信息就可以了。这显然是一个进步。但是比如你要建一个FriendFeed这样的社交网站（类似人人网那样的刷朋友新鲜事的网站），你的好友会随时的推送新的状态，然后你的新鲜事会实时自动刷新。要达成这个需求，我们需要让用户一直与服务器保持一个有效连接。目前最简单的实现方法，就是让用户和服务器之间保持长轮询（long polling）。 HTTP请求不是持续的连接，你请求一次，服务器响应一次，然后就完了。长轮训是一种利用HTTP模拟持续连接的技巧。具体来说，只要页面载入了，不管你需不需要服务器给你响应信息，你都会给服务器发一个Ajax请求。这个请求不同于一般的Ajax请求，服务器不会直接给你返回信息，而是它要等着，直到服务器觉得该给你发信息了，它才会响应。比如，你的好友发了一条新鲜事，服务器就会把这个新鲜事当做响应发给你的浏览器，然后你的浏览器就刷新页面了。浏览器收到响应刷新完之后，再发送一条新的请求给服务器，这个请求依然不会立即被响应。于是就开始重复以上步骤。利用这个方法，可以让浏览器始终保持等待响应的状态。虽然以上过程依然只有非持续的Http参与，但是我们模拟出了一个看似持续的连接状态 我们再看传统的服务器（比如Apache）。每次一个新用户连到你的网站上，你的服务器就得开一个连接。每个连接都需要占一个进程，这些进程大部分时间都是闲着的（比如等着你好友发新鲜事，等好友发完才给用户响应信息。或者等着数据库返回查询结果什么的）。虽然这些进程闲着，但是照样占用内存。这意味着，如果用户连接数的增长到一定规模，你服务器没准就要耗光内存直接瘫了。 这种情况怎么解决？解决方法就是刚才上边说的：非阻塞和事件驱动。这些概念在我们谈的这个情景里面其实没那么难理解。你把非阻塞的服务器想象成一个loop循环，这个loop会一直跑下去。一个新请求来了，这个loop就接了这个请求，把这个请求传给其他的进程（比如传给一个搞数据库查询的进程），然后响应一个回调（callback）。完事了这loop就接着跑，接其他的请求。这样下来。服务器就不会像之前那样傻等着数据库返回结果了。 如果数据库把结果返回来了，loop就把结果传回用户的浏览器，接着继续跑。在这种方式下，你的服务器的进程就不会闲着等着。从而在理论上说，同一时刻的数据库查询数量，以及用户的请求数量就没有限制了。服务器只在用户那边有事件发生的时候才响应，这就是事件驱动。 FriendFeed是用基于Python的非阻塞框架Tornado (知乎也用了这个框架) 来实现上面说的新鲜事功能的。不过，Node.js就比前者更妙了。Node.js的应用是通过javascript开发的，然后直接在Google的变态V8引擎上跑。用了Node.js，你就不用担心用户端的请求会在服务器里跑了一段能够造成阻塞的代码了。因为javascript本身就是事件驱动的脚本语言。你回想一下，在给前端写javascript的时候，更多时候你都是在搞事件处理和回调函数。javascript本身就是给事件处理量身定制的语言。 Node.js还是处于初期阶段。如果你想开发一个基于Node.js的应用，你应该会需要写一些很底层代码。但是下一代浏览器很快就要采用WebSocket技术了，从而长轮询也会消失。在Web开发里，Node.js这种类型的技术只会变得越来越重要]]></content>
      <categories>
        <category>Node.js</category>
      </categories>
      <tags>
        <tag>Node.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你了解响应式编程吗？]]></title>
    <url>%2F2019%2F01%2F17%2F%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8B%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[响应式编程？其实说白了，和传统的编程方式没什么区别，都是一些函数调用，只不过是增加了很多适配模式等。响应式编程的英文名，Reactive Programming，那就是针对响应的呗。那啥叫响应呢？你烧水呢，水烧开了，水壶会叫，这就是一下响应了。不要想的太复杂，这些东西都是基于现实世界的需要而来的。响应式它是依赖于事件的，响应式的代码它的运行不是按代码的顺序，而是跟多个按时间发生的事件有关。可能你会想，依赖事件？这不就是“回调”嘛，但在响应式编程里，这些按时间排列的事件，被称为“流”，stream。 简单的讲，响应式中的事件序列类似于js的数组，它里面的事件流就是时间的序列。响应式编程，就是异步的数据流的开发。响应式编程，它的关注重点在于“大量的UI事件与数据的互相影响”。啥意思呢，就例如某篇文章，你点个赞，那么一、所有其它人能看到赞；二、作者本人赞数量增加；三、文章权重提升；四、作者排名可能变化；。。。更多，“一个数据的变化，它的影响可能是呈现网状扩散”。它的特点吧，一是速度响应快，低延迟；二是健壮性弹性，有故障也能尽量响应；三是资源弹性，访问量大自动加资源，少了自动减；四是有消息自动传递。响应式的思想，实际是观察者模式 + （stream与事件源的通信控制）。 函数式编程？函数式编程是一系列被不公平对待的编程思想的保护伞，它的核心思想是，它是一种将程序看成是数学方法的求值、不会改变状态、不会产生副作用（后面我们马上会谈到）的编程方式。 FP 核心思想强调： 声明式代码 —— 程序员应该关心是什么，让编译器和运行环境去关心怎样做。 明确性 —— 代码应该尽可能的明显。尤其是要隔离副作用避免意外。要明确定义数据流和错误处理，要避免 GOTO 语句和 异常，因为它们会将应用置于意外的状态。 并发 —— 因为纯函数的概念，大多数函数式代码默认都是并行的。由于CPU运行速度没有像以前那样逐年加快（(详见 摩尔定律)）， 普遍看来这个特点导致函数式编程渐受欢迎。以及我们也必须利用多核架构的优点，让代码尽量的可并行。 高阶函数 —— 函数和其他的语言基本元素一样是一等公民。你可以像使用 string 和 int 一样的去传递函数。 不变性 —— 变量一经初始化将不能修改。一经创建，永不改变。如果需要改变，需要创建新的。这是明确性和避免副作用之外的另一方面。如果你知道一个变量不能改变，当你使用时会对它的状态更有信心。 函数式编程：JS、Scala、Erlang 响应式编程与函数式编程的区别？它和函数式编程的区别，这个简单的说一下，函数式编程就是二个字，“不变”。啥都不变，一经创建永远不变。如果要变，再创建个新的。在它里面函数就是数据的通道。参数确定时，结果是可以预测的。 ##更进一步 怎么理解响应式的背压？可以理解为承上启下。比如洪水，大坝的作用就是背压。背压应该写在靠近生产者的地方，或者说是连接元素生产者和消费者的一个地方，即生产者和消费者的连线者。背压应该具有承载元素的能力，也就是其必须是一个容器的，而且元素的存储与下发应该具有先后的，可以使用队列来实现。打一个比喻：去电影院看电影，我们作为消费者消费电影，电影厂商提供生产电影，电影院负责下发电影，电影院作为生产者与消费者之间的连线者，我们不需要关心电影院会放映多少电影，我们只需要观看就行了。 怎么理解阻塞和异步？咖啡很烫，不能喝，只能等到冷却，这个冷却就作为了阻塞，因为其占据了一条线程，但我们可以看电视，也就是说你阻塞你的 ，但是我仍然可以看电视。看电视是主线程。于是异步就出现了，涉及到异步，就涉及到线程的状态。既然阻塞要占据一条线程来工作，那么必然会有线程状态的管理。 非阻塞更多体现在等待层面，现实中我们等一个人，往往都是由时间限制的，难道我们要用一生的时间去街口等待一个人而荒废一生？总有跳出阻塞的路子。在nio里就是timeout。 我们讲的非阻塞都是相对的，具体某个任务该阻塞还是得阻塞。 Socket下同步/异步和阻塞/非阻塞:同步/异步是属于操作系统级别的，指的是操作系统在收到程序请求的IO之后，如果IO资源没有准备好的话，该如何响应程序的问题，同步的话就是不响应，直到IO资源准备好；而异步的话则会返回给程序一个标志，这个标志用于当IO资源准备好后通过事件机制发送的内容应该发到什么地方。 阻塞/非阻塞是属于程序级别的，指的是程序在请求操作系统进行IO操作时，如果IO资源没有准备好的话，程序该怎么处理的问题，阻塞的话就是程序什么都不做，一直等到IO资源准备好，非阻塞的话程序则继续运行，但是会时不时的去查看下IO到底准备好没有呢； 我们通常见到的BIO是同步阻塞式的，同步的话说明操作系统底层是一直等待IO资源准备直到ok的，阻塞的话是程序本身也在一直等待IO资源准备直到ok，具体来讲程序级别的阻塞就是accept和read造成的，我们可以通过改造将其变成非阻塞式，但是操作系统层次的阻塞我们没法改变。 我们的NIO是同步非阻塞式的，其实它的非阻塞实现原理和我们上面的讲解差不多的，就是为了改善accept和read方法带来的阻塞现象，所以引入了Channel和Buffer的概念。 这里有一个形象的比喻：如果你想吃一份宫保鸡丁盖饭： 同步阻塞：你到饭馆点餐，然后在那等着，还要一边喊：好了没啊！ 同步非阻塞：在饭馆点完餐，就去遛狗了。不过溜一会儿，就回饭馆喊一声：好了没啊！ 异步阻塞：遛狗的时候，接到饭馆电话，说饭做好了，让您亲自去拿。 异步非阻塞：饭馆打电话说，我们知道您的位置，一会给你送过来，安心遛狗就可以了。 “一个IO操作其实分成了两个步骤：发起IO请求和实际的IO操作。同步IO和异步IO的区别就在于第二个步骤是否阻塞，如果实际的IO读写阻塞请求进程，那么就是同步IO。阻塞IO和非阻塞IO的区别在于第一步，发起IO请求是否会被阻塞，如果阻塞直到完成那么就是传统的阻塞IO，如果不阻塞，那么就是非阻塞IO。 同步和异步是针对应用程序和内核的交互而言的，同步指的是用户进程触发IO操作并等待或者轮询的去查看IO操作是否就绪，而异步是指用户进程触发IO操作以后便开始做自己的事情，而当IO操作已经完成的时候会得到IO完成的通知。而阻塞和非阻塞是针对于进程在访问数据的时候，根据IO操作的就绪状态来采取的不同方式，说白了是一种读取或者写入操作函数的实现方式，阻塞方式下读取或者写入函数将一直等待，而非阻塞方式下，读取或者写入函数会立即返回一个状态值。所以,IO操作可以分为3类：同步阻塞（即早期的IO操作）、同步非阻塞（NIO）、异步（AIO）。同步阻塞：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式。 同步非阻塞：在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。异步：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序。”]]></content>
      <categories>
        <category>Architecture</category>
      </categories>
      <tags>
        <tag>Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件锁]]></title>
    <url>%2F2019%2F01%2F17%2F%E6%96%87%E4%BB%B6%E9%94%81%2F</url>
    <content type="text"><![CDATA[背景我们都知道，Nginx是多进程程序，80端口是各进程所共享的，多进程同时listen 80端口，势必会产生竞争，也产生了所谓的“惊群”效应。当内核accept一个连接时，会唤醒所有等待中的进程，但实际上只有一个进程能获取连接，其他的进程都是被无效唤醒的。所以Nginx采用了自有的一套accept加锁机制，避免多个进程同时调用accept。Nginx多进程的锁在底层默认是通过CPU自旋锁来实现。如果操作系统不支持自旋锁，就采用文件锁。 Tips：查看占用某端口的进程命令lsof -i:80（更准确，master进程和woreker进程都显示了出来） 或者netstat -tunlp | grep 80（只显示出了worker进程） 这里提到了文件锁。那么什么是文件锁呢？ 文件锁在多任务操作系统环境中，如果一个进程尝试对正在被其他进程读取的文件进行写操作，可能会导致正在进行读操作的进程读取到一些被破坏或者不完整的数据；如果两个进程并发对同一个文件进行写操作，可能会导致该文件遭到破坏。因此，为了避免发生这种问题，必须要采用某种机制来解决多个进程并发访问同一个文件时所面临的同步问题，由此而产生了文件加锁方面的技术。文件锁是一种文件读写机制。在不论什么特定的时间仅仅同意一个进程访问一个文件。利用这样的机制可以使读写单个文件的过程变得更安全。 我们为什么需要文件锁？想像以下场景： 进程“A”打开和读取一个文件，此文件包括账户相关的一些信息。 进程“B”也打开了这个文件。并读取了文件里的信息。 如今，进程“A”更改了其副本中的一条剩余金额记录，并将其写入文件。 此时，进程“B”并不知道上次读取的文件已经被更改。它还保存着原始的文件副本。然后。进程“B”更改了“A”操作的那条同样的记录，并将记录写入文件。 此时。文件里将仅仅保存了进程“B”更改过的记录。 为了避免这样的事情发生，就要使用文件锁来确保操作的“序列化”。 Linux系统中两种经常使用的文件锁协同锁协同锁要求參与操作的进程之间协同合作。 如果进程“A”获得一个WRITE锁，并開始向文件里写入内容；此时，进程“B”并没有试图获取一个锁，它仍然能够打开文件并向文件里写入内容。 在此过程中，进程“B”就是一个非合作进程。如果进程“B”试图获取一个锁，那么整个过程就是一个合作的过程，从而能够保证操作的“序列化”。 仅仅有当參与操作的进程是协同合作的时候，协同锁才干发挥作用。协同锁有时也被称为“非强制”锁 强制锁强制锁不须要參与操作的进程之间保持协同合作。它利用内核来查检每一个打开、读取、写入操作，从而保证在调用这些操作时不违反文件上的锁规则。关于强制锁的很多其它信息，能够在kernal.org上找到。 为了能使用Linux中的强制锁功能。你须要在文件系统级别上打开它。同时在单个文件上打开它。其步骤是： 挂载文件系统时使用“-o mand”參数。 对于要打开强制锁功能的文件lock_file。必须打开set-group-ID位。关闭group-execute位。 （选择此方法的原因是，当你关闭group-execute时，设置set-group-ID就没有实际的意义了） ###代码1234567891011121314151617181920212223242526272829303132#include&lt;stdio.h&gt;#include&lt;fcntl.h&gt;#include&lt;stdlib.h&gt;int main(int argc, char **argv) &#123; if(argc &gt; 1) &#123; int fd = open(argv[1], O_WRONLY); if(fd == -1)&#123; printf("Unable to open the file\n"); exit(1); &#125; static struct flock lock; lock.l_type = F_WRLCK; lock.l_start = 0; lock.l_whence = SEEK_SET; lock.l_len = 0; lock.l_pid = getpid(); int ret = fcntl(fd, F_SETLKW, &amp;lock); printf("Return value of fcntl:%d\n", ret); if(ret == 0) &#123; while(1)&#123; scanf("%c", NULL);&#125; &#125; &#125;&#125; 12345678910111213141516gcc -o file_lock file_lock.cmount -oremount,mand / (使用mount命令带“mand”參数来又一次挂载根文件系统，例如以下所看到的。这将在文件系统级别使能强制锁功能。)touch advisory.txttouch mandatory.txtchmod g+s,g-x mandatory.txt./file_lock advisory.txtls &gt;&gt;advisory.txt在上面的样例中，ls命令会将其输出写入到advisory.txt文件中。即使我们获得了一个写入锁，仍然会有一些进程（非合作）可以往文件中写入数据。这就是所谓的“协同”锁。./file_lock mandatory.txtls &gt;&gt;mandatory.txt (ls命令在将其输出写入到mandatory.txt文件之前。会等待文件锁被删除。尽管它仍然是一个非合作进程。但强制锁起了作用) flock的使用场景：检测进程是否已经存在12345678910111213141516171819int checkexit(char* pfile)&#123; if (pfile == NULL) &#123; return -1; &#125; int lockfd = open(pfile,O_RDWR); if (lockfd == -1) &#123; return -2; &#125; int iret = flock(lockfd,LOCK_EX|LOCK_NB); if (iret == -1) &#123; return -3; &#125; return 0;&#125; flock()的限制flock()放置的锁有如下限制 只能对整个文件进行加锁。这种粗粒度的加锁会限制协作进程间的并发。假如存在多个进程，其中各个进程都想同时访问同一个文件的不同部分。通过flock()只能放置劝告式锁。很多NFS实现不识别flock()放置的锁。注释：在默认情况下，文件锁是劝告式的，这表示一个进程可以简单地忽略另一个进程在文件上放置的锁。要使得劝告式加锁模型能够正常工作，所有访问文件的进程都必须要配合，即在执行文件IO之前先放置一把锁。 加锁的原理从内核实现的角度来看，每当创建一把文件锁的时候，系统就会实例化一个struct file_lock对象，这个file_lock对象会记录锁的相关信息：如锁的类型（共享锁，独占锁）、拥有这把锁的进程号、锁的标识（租赁锁，阻塞锁，POSIX锁，FLOCK锁），等等。最后把这个file_lock对象插入到被锁文件的inode.i_flock链表中，就完成了对该文件的加锁功能。要是其它进程想要对同一个文件加锁，那么它在将file_lock对象插入到inode.i_flock之前，会遍历该链表，如果没有发现冲突的锁，就将其插入到链表尾，表示加锁成功，否则失败。至于为什么要将inode与file_lock以链表的形式关联起来，主要是考虑到用户有时可以对同一个文件加多个文件锁。例如：我们可以对同一个文件加多个共享锁；或者我们可以同时对文件加POSIX锁和FLOCK锁，这两种锁分别对应flock()和fcntl()两种系统调用函数；再或者可以通过多次调用fcntl()对同一个文件中的多个内容块加上POSIX记录锁。 POSIX锁和FLOCK锁的区别： POSIX锁和FLOCK锁分别是通过fcntl()和flock()系统调用完成的。虽然实现的原理上都差不多，都是生成一个file_lock对象并插入inode文件锁链表，但是POSIX锁是支持针对某一段文件内容进行加锁的，而FLOCK锁不支持。 POSIX锁可以重复加锁，即同一个进程，可以对同一个文件多次加同样一把锁。例如：第一次我对A文件的一个0~10的内容块加了一把独占锁，那么第二次同一个进程中我一样可以对这个A文件的0~10的内容块再加一把独占锁，这个有点像是递归加锁，但是我解锁时只需要解一次。FLOCK锁则不同，如果你第一次对A文件加了一把独占锁，那么在同一个进程中你就不能对A文件再加一把锁了。这个区别其实只不过是在加锁的时候，遍历inode.i_flock链表时，发现存在PID相同的锁时，系统对于POSIX锁和FLOCK锁的具体处理手段不一样罢了。 通过第2点，我们可以想象一下，POSIX锁和FLOCK锁在多线程环境下的不同。我们知道从Linux内核的视角来看，它是不区分所谓的进程和线程的，都不过是CPU调度队列中的一个个task_struct实例而已，所以不会对线程的场景进行专门的处理，也正以为如此，平时我们用的NPTL线程库也都是在用户态环境中模拟出来的，Linux内核并不直接支持。回到刚刚的话题，因为内核它在加锁的时候是看PID的，所以在内核看来多线程的加锁只不过是同一个进程（因为每个线程的PID都是一样的）在对同一个文件加多把锁。这样，多线程环境下的加锁行为就表现为：同一个进程中的多个线程可以对同一个文件加多次POSIX独占或共享锁，但是不可以对同一个文件加多次FLOCK独占锁（不过共享锁是可以加多次的）。 在一个项目中使用了GPFS共享文件系统，我们在开发过程中发现，对于FLOCK锁只支持本地，而POSIX锁则可以支持跨主机加锁。例如：我们有两台独立的机器A和B，在A机器上有某个进程对文件f加POSIX独占锁，然后在B机器上当有某个进程想对f加POSIX独占锁时，就会失败。可是当我们使用FLOCK锁时，就发现两台机器对同一个文件加FLOCK锁是互不影响的，即A和B机器都可以对f加独占锁。 参考文献https://www.ibm.com/developerworks/cn/linux/l-cn-filelock/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你知道女巫攻击吗？]]></title>
    <url>%2F2018%2F12%2F27%2F%E4%BD%A0%E7%9F%A5%E9%81%93%E5%A5%B3%E5%B7%AB%E6%94%BB%E5%87%BB%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[概述女巫攻击(Sybil Attack)是2002年由John R. Douceur在《the Sybil Attack》[1]文中提出的，它是作用于对等（Peer-to-Peer,简称P2P）网络中的一种攻击形式:攻击者利用单个节点来伪造多个身份存在于P2P网络中，从而达到削弱网络的冗余性，降低网络健壮性，监视或干扰网络正常活动等目的。在P2P网络中，为了解决来自恶意节点或者节点失效带来的安全威胁，通常会引入冗余备份机制，将运算或存储任务备份到多个节点上，或者将一个完整的任务分割存储在多个节点上。正常情况下，一个设备实体代表一个节点，一个节点由一个ID来标识身份。然而，在缺少可信赖的节点身份认证机构的P2P网络中，难以保证所备份的多个节点是不同的实体。攻击者可以通过只部署一个实体，向网络中广播多个身份ID，来充当多个不同的节点，这些伪造的身份一般被称为Sybil节点[2,3]。Sybil节点为攻击者争取了更多的网络控制权，一旦用户查询资源的路径经过这些Sybil节点，攻击者可以干扰查询、返回错误结果，甚至拒绝回复。 应用案例Sybil Attack的思想被广泛用于对抗P2P僵尸网络。以知名P2P僵尸网络Strom[2]为例,其采用了基于Kademlia的Overnet协议，正常节点的行为:1、每个加入网络中的节点会生成一个ID号用以标识自身；2、节点通过预设的算法每天生成32个不同key来查询控制命令；3、控制者会提前在网络中发布这32个key对应的命令&lt;key,command&gt;以供节点查询；4、根据Overnet协议，&lt;key,command&gt;会存放在K个与该key相邻ID的节点中，并通过递归的方式进行查询。Sybil节点行为：1、根据待攻击的key空间生成相应的ID（接近key的哈希值），使得查询请求能有较高概率被路由到Sybil节点；2、主动向网络中的其他节点广播自己的ID，使其出现在其他节点的路由表中；3、当查询key的路径经过Sybil节点时，Sybil节点返回错误信息，或者重路由到其他Sybil节点，使得正常节点无法进行C&amp;C通信获取控制命令。 怎么解决女巫攻击？一种方法是工作量证明机制，即证明你是一个节点，别只说不练，而是要用计算能力证明，这样极大地增加了攻击的成本。另一种方法是身份认证（相对于PoW协议，女巫攻击是基于BFT拜占庭使用容错协议的Blockchain需要考虑的问题，需要采用相应的身份认证机制）。认证机制分为二类：1）基于第三方的身份认证每加入一个新的节点都需要与某一个可靠的第三方节点进行身份验证。2）纯分布式的身份认证每加入一个新的节点都需要获得当前网络中所有可靠节点的认证，这种方法采用了随机密钥分发验证的公钥体制的认证方式，需要获得网络中大多数节点的认证才能加入该网络。 大话女巫攻击女巫攻击：分身诈骗术应对方法：1.干活 你即便分身千千万，唯有真心能干活。分心是虚幻的，没有力气，pow证明。2.发身份证 可靠第三方公安局给你发身份证，没有身份证都是分身妖怪。3.熟人社会 你迁户口到一个新的村子里，必须得到村子里，大部分人的认证，这就是中国传统社会的身份认证方法。群众的眼睛就是火眼金睛，照出一切妖魔鬼怪。]]></content>
      <categories>
        <category>Information Safety</category>
      </categories>
      <tags>
        <tag>Information Safety</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你知道可信计算吗]]></title>
    <url>%2F2018%2F12%2F19%2F%E4%BD%A0%E7%9F%A5%E9%81%93%E5%8F%AF%E4%BF%A1%E8%AE%A1%E7%AE%97%E5%90%97%2F</url>
    <content type="text"><![CDATA[安全和可信的区别？？安全分两条，有功能性和保障性 （保密性 完整性 可用性 可查性）可信本质是可预期 ，是保障安全四个特性的可预期 。利用密码学通过保障硬件系统的可预期 来保障 上层的可预期。安全就是加密 ，可信就是加密的程度 如何密。 TPCM（对标TPM）TPCM应该包含tcm模块和一个计算芯片，也就是说TPCM应该是包含可信根的，Cube-tcm 可以用来管理和调度虚拟可信根啥的，调度句柄 和上下文操作。Tpcm调用tcm密码模块（sm3）。在计算机 启动过程中， 先起这个TPCM，再起cpu。Tpcm里边有一个完整的访问策略，是一个核心的索引，必须通过他进行调用，保存的策略可能存在硬盘等其他地方，但是必须通过索引来访问。把TPCm放在哪呢？可以把 八核中的一核作为tpcm。Tpcm管理的是 人 、机器、 和密码之间的关联关系。Tcm空间 很有限，在里边解密 ，秘钥迁移过程 比如只能放4 个秘钥，但是现在要管理10个 可以用根密码 加密之后 放在外边。还有 秘钥授权 和权限管理 ，这一部分是 可信度量的部分先从 tcm里取出一个 放到内存里 和用户建设一个会话 才能使用 （类似于操作系统的资源调度算法进行切换）只不过是更严格。 什么是二进制证明？由于 PCR 中所存储的度量值是利用散列函数获得的二 进制摘要值, 因此 TCG 提供的这种对平台证明的方 式称为二进制证明。 等级保护？假设一个人想要一个很安全的地方。首先要有房子，修门、 锁 ，围墙，等级保护的意义就在这 ，安全严格就能，这样能让房子更加结实，房子更加坚固。可信加固技术。可信计算并不是新东西，可信也采用了很多过去的技术，可信就是老老实实的。其实可信计算不是原先没有，而且对原先技术的体系优化和构建化。利用防控 和加密机制进行防控。 Cube之改造？制定一个安全策略，投入产出不平衡的领域，可以利用区块链技术把cube 改成区块链形式的。区块上放的是配置文件。我们的东西 ，比区块链纯密码学的更灵活的多， 可信系统如何保证安全？禁止上读和禁止下写是信息单向流动之本，也是安全之本，而可信系统正是通过这二者来实现安全的，利用可信系统来保护系统要比单纯的访问控制更安全，因为漏洞更少，即使有也很不容易钻。]]></content>
      <categories>
        <category>TrustComputing</category>
      </categories>
      <tags>
        <tag>TrustComputing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[当SDN遇见NFV]]></title>
    <url>%2F2018%2F12%2F19%2F%E5%BD%93SDN%E9%81%87%E8%A7%81NFV%2F</url>
    <content type="text"><![CDATA[SDN从2012年开始，在学术界受到了广泛的关注。在阅读了部分国外大牛写的相关综述性文章若干之后，发现其中似乎并没有看到NFV的影子。提到SDN，能想到的基本上绕不过“控制转发分离、可编程接口、集中控制”，这三个特点。固然这三个特定很重要，也是SDN存在的价值。但除此之外，伴随着SDN一起成长的还有NFV，即网络功能虚拟化。 SDN出身于斯坦福实验室，算是学术界吧。而NFV出身于工业界，相对而言，NFV是一种技术。 SDN和NFV是可以相互独立存在的，据相关研究表明，二者结合起来的效果更优，但是需要处理的问题也更多。 从大的方面讲，SDN和NFV都提出将软件和硬件分离的概念。但是细化之后： SDN侧重于将设备层面的控制模块分离出来，简化底层设备，进行集中控制，底层设备仅仅只负责数据的转发。目的在于降低网络管理的复杂度、协议部署的成本和灵活、以及网络创新。而NFV则看中将设备中的功能提取出来，通过虚拟化的技术在上层提供虚拟功能模块。也就是，NFV希望能够使用通用的x86体系结构的机器替代底层的各种异构的专用设备，然后通过虚拟化技术，在虚拟层提供不同的功能，允许功能进行组合和分离。 SDN中也存在虚拟化技术，但是和NFV有本质上的区别。SDN虚拟的是设备，而NFV虚拟的是功能，当然NFV也包括对基础设备的虚拟，即NFVI。 目前ETSI组织已经于2015-1完成了对NFV的第一阶段的工作，主要包括对NFV的架构设计，各层之间的接口以及管理。并且计划在未来两年内实现对NFV第二阶段的规划，据ETSI ISG给出的白皮书介绍，第二阶段将主要关注于解决NFV中的互操作性问题（应该是VNF之间的以及与VM之间的通信、协作等关系）。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谷歌之Percolator模型]]></title>
    <url>%2F2018%2F12%2F15%2F%E8%B0%B7%E6%AD%8C%E4%B9%8BPercolator%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[论文今天读了一篇论文https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf 思考 在谷歌的索引系统中。通过MapReduce将爬取下来的页面弄到Percolator中，该模型提供用户随机访问一个超大的数据库，避免了MapReduce的全局scan。该模型中因为有很多线程并发，所以提供了ACID。 Percolator由一系列observers组成。每个observer可以完成一个任务而且可以为下游的observers指派更多的任务。 Percolator使用于增量处理。 该模型如下图，A Percolator system由这三部分组成。还需要依赖两个服务the timestamp oracle and the lightweight lock service。该模型的事务：Percolator provides cross-row, cross-table transactions with ACID snapshot-isolation semantics. 提出了snapshot isolation。意思如图 这里边还讲了个很有意思的比喻。扫描线程一多可能会导致parallelism的减少，拿公交车来作比喻，公交车一慢，会导致等的乘客过多，从而导致车更慢。该模型中为了解决这个问题，采取了如下做法：当一个线程发现别的线程慢的时候，它选择一个随机的位置进行扫描。 TiDB通过查询知道TiKV 的事务采用的就是是 Percolator 模型，并且做了大量的优化。事务的细节这里不详述，大家可以参考论文。这里只提一点，TiKV 的事务采用乐观锁，事务的执行过程中，不会检测写写冲突，只有在提交过程中，才会做冲突检测，冲突的双方中比较早完成提交的会写入成功，另一方会尝试重新执行整个事务。当业务的写入冲突不严重的情况下，这种模型性能会很好，比如随机更新表中某一行的数据，并且表很大。但是如果业务的写入冲突严重，性能就会很差，举一个极端的例子，就是计数器，多个客户端同时修改少量行，导致冲突严重的，造成大量的无效重试。 Percolator原理比较简单，总体来说就是一个经过优化的二阶段提交的实现，进行了一个二级锁的优化。TiDB 的事务模型沿用了 Percolator 的事务模型。 总体的流程如下：123456789101112131415161718192021读写事务1) 事务提交前，在客户端 buffer 所有的 update/delete 操作。 2) Prewrite 阶段:首先在所有行的写操作中选出一个作为 primary，其他的为 secondaries。PrewritePrimary: 对 primaryRow 写入 L 列(上锁)，L 列中记录本次事务的开始时间戳。写入 L 列前会检查:1. 是否已经有别的客户端已经上锁 (Locking)。2. 是否在本次事务开始时间之后，检查 W 列，是否有更新 [startTs, +Inf) 的写操作已经提交 (Conflict)。在这两种种情况下会返回事务冲突。否则，就成功上锁。将行的内容写入 row 中，时间戳设置为 startTs。将 primaryRow 的锁上好了以后，进行 secondaries 的 prewrite 流程:1. 类似 primaryRow 的上锁流程，只不过锁的内容为事务开始时间及 primaryRow 的 Lock 的信息。2. 检查的事项同 primaryRow 的一致。当锁成功写入后，写入 row，时间戳设置为 startTs。3) 以上 Prewrite 流程任何一步发生错误，都会进行回滚：删除 Lock，删除版本为 startTs 的数据。4) 当 Prewrite 完成以后，进入 Commit 阶段，当前时间戳为 commitTs，且 commitTs&gt; startTs :1. commit primary：写入 W 列新数据，时间戳为 commitTs，内容为 startTs，表明数据的最新版本是 startTs 对应的数据。2. 删除L列。如果 primary row 提交失败的话，全事务回滚，回滚逻辑同 prewrite。如果 commit primary 成功，则可以异步的 commit secondaries, 流程和 commit primary 一致， 失败了也无所谓。事务中的读操作1. 检查该行是否有 L 列，时间戳为 [0, startTs]，如果有，表示目前有其他事务正占用此行，如果这个锁已经超时则尝试清除，否则等待超时或者其他事务主动解锁。注意此时不能直接返回老版本的数据，否则会发生幻读的问题。2. 读取至 startTs 时该行最新的数据，方法是：读取 W 列，时间戳为 [0, startTs], 获取这一列的值，转化成时间戳 t, 然后读取此列于 t 版本的数据内容。由于锁是分两级的，primary 和 seconary，只要 primary 的行锁去掉，就表示该事务已经成功 提交，这样的好处是 secondary 的 commit 是可以异步进行的，只是在异步提交进行的过程中 ，如果此时有读请求，可能会需要做一下锁的清理工作。 初体验TiDB 通过docker-compose来部署，目前为止，所以组件已经准备完毕，如图 123访问集群: mysql -h 127.0.0.1 -P 4000 -u root访问集群 Grafana 监控页面: http://localhost:3000 默认用户名和密码均为 admin。集群数据可视化： http://localhost:8010 效果如图：]]></content>
      <categories>
        <category>TiDB</category>
      </categories>
      <tags>
        <tag>TiDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VM与HOST之间传输文件]]></title>
    <url>%2F2018%2F12%2F12%2FVM%E4%B8%8EHOST%E4%B9%8B%E9%97%B4%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[今天老师有一个需求，是从host向qemu创建的虚拟机中传输文件，调研了几个方法，起初是通过网络传输，但是在新的这个平台上没有搭建网桥等设备，而且想还没有别的其他的方式。经过查询，主要有以下方式： 共享文件夹的方式Docker中也有此方法https://blog.csdn.net/zhongbeida_xue/article/details/80747212?utm_source=blogxgwz9 把数据存到usb设备（可真可假）中https://blog.csdn.net/kingtj/article/details/82952783 总结：qemu-kvm虚拟机与宿主机之间实现文件传输，大概两类方法： 虚拟机与宿主机之间，使用网络来进行文件传输。这个需要先在宿主机上配置网络桥架，在qemu-kvm启动配置网卡就可以实现文件传输。 使用9psetup协议实现虚拟机与宿主机之间文件传输。该方法先要宿主机需要在内核中配置了9p选项，即：12345CONFIG_NET_9P=yCONFIG_net_9P_VIRTIO=yCONFIG_NET_9P_DEBUG=y (可选项)CONFIG_9P_FS=yCONFIG_9P_FS_POSIX_ACL=y 另外，qemu在编译时需要支持ATTR/XATTR。 综上，两类方法配置起来都比较麻烦。其实有一个比较简单的方法123456789101112131415161718在虚拟机环境下，我们可能会遇到在宿主机和客户机之间传输文件的需求，目前有几种方法可以实现这个例如通过9p协议，或者为客户机和宿主机之间搭建一个网络等。这些都太不容易实现，下面我介绍一种简单的方法。1. 使用dd创建一个文件，作为虚拟机和宿主机之间传输桥梁dd if=/dev/zero of=/var/lib/libvirt/images/share.img bs=1M count=3502. 格式化share.img文件mkfs.ext4/var/lib/libvirt/images/share.img3. 在宿主机上创建一个文件夹，mkdir /tmp/sharemount -o loop/var/lib/libvirt/images/share.img /tmp/share这样，在宿主机上把需要传输给虚拟机的文件放到/tmp/share 下即可。4. 启动qemu-kvm虚拟机，可以额外为客户机添加上一块硬盘。-drive file=/var/lib/libvirt/images/share.img,if=virtio5. 在虚拟机中 mount上添加的一块硬盘。即可以获得宿主机上放在/tmp/share文件夹下的文件，具体做法是：通过dmesg的输出找到新挂在的硬盘是什么，然后将硬盘直接mount上来。mount -t ext4 /dev/vdb /mnt/ 当然，该方法虽然简单，但它也有缺点, 宿主机和虚拟机文件传输不能实时传输。如果需要传输新文件，需要重启虚拟机。 有没有其他的方式呢还？待补充。。。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[国密算法之SM4]]></title>
    <url>%2F2018%2F12%2F12%2F%E5%9B%BD%E5%AF%86%E7%AE%97%E6%B3%95%E4%B9%8BSM4%2F</url>
    <content type="text"><![CDATA[为什么用SM4加密后文件会略长于明文文件？ 首先了解什么是SM4算法 参考： https://blog.csdn.net/Soul_Programmer_Swh/article/details/80263822 https://www.cnblogs.com/TaiYangXiManYouZhe/p/4317519.html https://blog.csdn.net/cg129054036/article/details/83016958 原因就在于sm4 算法加解密时，采用的是 TCM_ES_SM4_CBC 模式，该模式会填 充加密数据以保证其长度为 16 的整数倍，因此加密后文件会略长于明文文件，解 密后文件长度将恢复。 那么什么是又CBC？在密码学中，分组加密（英语：Block cipher），又称分块加密或块密码，是一种对称密钥算法。它将明文分成多个等长的模块（block），使用确定的算法和对称密钥对每组分别加密解密。分组加密是极其重要的加密协议组成，其中典型的如DES和AES作为美国政府核定的标准加密算法，应用领域从电子邮件加密到银行交易转帐，非常广泛。现代分组加密创建在迭代的思想上产生密文。其思想由克劳德·香农在他1949年的重要论文《保密系统的通信理论》（Communication Theory of Secrecy Systems）中提出，作为一种通过简单操作如替代和排列等以有效改善保密性的方法。[1] 迭代产生的密文在每一轮加密中使用不同的子密钥，而子密钥生成自原始密钥。DES加密在1977年由前美国国家标准局（今“美国国家标准与技术研究所”）颁布，是现代分组加密设计的基础思想。它同样也影响了密码分析的学术进展。 https://zh.wikipedia.org/wiki/%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81%E5%B7%A5%E4%BD%9C%E6%A8%A1%E5%BC%8F#%E5%AF%86%E7%A0%81%E5%9D%97%E9%93%BE%E6%8E%A5%EF%BC%88CBC%EF%BC%89 SM9算法 SM9标识密码算法是由国密局发布的一种IBE(Identity-Based Encryption)算法。IBE算法以用户的身份标识作为公钥，不依赖于数字证书。 IBC(基于标示的密码技术) 基于证书的公钥体制在应用中面临诸多问题，特别是证书使用过程的复杂性使得不具备相关知识的普通用户难以驾驭。为了降低公钥系统中密钥管理和使用的复杂性，Shamir在1984[S84]年提出了基于标识的密码技术(Identity-Based Cryptography - IBC)：即用户的标识就可以用做用户的公钥（更加准确地说是用户的公钥可以从用户的标识和系统指定的一个方法计算得出）。 在这种情况下，用户不需要申请和交换证书，从而极大地简化了密码系统管理的复杂性。用户的私钥由系统中的一受信任的第三方（密钥生成中心）使用标识私钥生成算法计算生成。这样的系统具有天然的密码委托功能，适合于有监管的应用环境。 NTLS(下一代安全传输协议)]]></content>
      <categories>
        <category>TrustComputing</category>
      </categories>
      <tags>
        <tag>TrustComputing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker化我的秒杀项目]]></title>
    <url>%2F2018%2F12%2F09%2FDocker%E5%8C%96%E6%88%91%E7%9A%84%E7%A7%92%E6%9D%80%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[Docker化秒杀项目 首先构建jar包，因为该秒杀基于maven的，第一开始想把配置文件啥的都放一起，可以用一些eclipse插件啥的，但是后来一想这样不好，因为如果我想改配置的话需要重新构建，所以在网上找了个方法，构建好maven install 之后是这样子的，参考的这篇文章http://https://blog.csdn.net/qq_22857293/article/details/79416165，最后构建之后的结果如图，可以看出第三方依赖，配置文件，资源分离开来： （当把jar包通过Xshell上传文件的时候出现了不能上传的问题，这是因为目标目录没有相应写权限造成的，通过chmod o+w 赋予相应权限即可。） * 启动jar包的命令是java -jar -Dloader.path=.,config,resources,3rd-lib miaosha.jar然后我写了Dockerfile，内容如下：构建镜像 docker build -t sjt157/miaoshaapp:v2 .（注意后边有个.）运行docker容器：docker run -d -p 65534:65510 sjt157/miaoshaapp:v2 Docker化Rabbit1234首先pull官方镜像，Docker pull rabbitmq:3启动docker run -d --name miaosharabbit -p 5672:5672 -p 15672:15672 rabbitmq:3当时启动rabbit容器的时候出现了异常 An unexpected connection driver error occurred的话，执行这个 Docker化Mysql 首先拉取镜像，docker pull mysql:5.7 使用镜像创建容器 docker run -p 3306:3306 --name miaoshaMysql -e MYSQL_ROOT_PASSWORD=123 -d mysql:5.7 进入容器 docker exec -it miaoshaMysql bash 进入mysql -uroot -p 输入 GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123' WITH GRANT OPTION; FLUSH PRIVILEGES; 复制sql文件到容器 docker inspect -f '{{.ID}}' miaoshaMysql docker cp 本地路径 容器长ID:容器路径 我把文件考到了根目录下 docker cp /home/ubuntu16/docker/miaoshaMysql/seckill.sql 561e41a0347d:/ 首先进入mysql 创建数据库 create database miaosha; 在执行mysql -uroot -p miaosha &lt; miaosha.sql命令导入文件 我要把原镜像 重新弄一个我的数据库镜像 docker commit -m "added miaosha.sql" -a "sjt157" 561e41a0347d sjt157/miaoshamysql:v2 运行我自己的镜像 docker run -p 3306:3306 --name miaoshamysql -e MYSQL_ROOT_PASSWORD=123 -d sjt157/miaoshamysql:v2 此处有个疑问？？？为什么在官方mysql镜像上 我做了相应操作，存了文件，执行了命令，只有文件被保存了，但是 我做的其他命令 却没有保存上？？？比如创建数据库表等命令 这是为什么？那如果是用Dockerfile创建的自己的镜像会不会也是这样呢，相关命令不保存？？？还有一个，为什么容器已经是退出状态了，却也不能重名呢？？必须要rm掉吗。因为已经是退出状态了，所以不能kill。 Docker化Redis不用 redis.password=123456这个加密码的配置了，因为默认的官方镜像是没有的。docker pull redis:4（有个疑问。不能制定详细的版本信息吗？？应该可以把）启动redis容器docker run -p 6379:6379 –name miaosharedis -d redis最后可以看到4个容器均运行成功。 下一步工作这样太麻烦了，每次都要手动执行，能不能自动编排呢？利用compose来操作。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-vm-cannot-ssh]]></title>
    <url>%2F2018%2F12%2F09%2Fopenstack-vm-cannot-ssh%2F</url>
    <content type="text"><![CDATA[云之迷雾 今天发现了一件奇事，过程是这样的，我想SSH一个虚拟机，但是怎么也出不来，排除了防火墙等等一系列原因都无果，结果想到了在VNC界面上查看一下，不看还好，一看吓了自己一跳，为什么在3月份创建虚拟机的时候明明指定的IP是172.21.4.68，但是不知道怎么自己偷摸摸的变成了172.21.4.197。我也不能在现在再改该虚拟机IP，因为按openstack的规定：虚拟机的IP默认都是DHCP分配的，如果要设置固定IP，需要在创建虚拟机时就指定，直接在虚拟机上更改IP是会被过滤的，所以此法无效。 现在的情况就是除了通过在VNC界面查看ip是172.21.4.197 以外 ， 在电科华云界面上 和 通过openstack的命令查看的ip 都是 172.21.4.68 ，也就是说虚拟机的IP是真的改了，但是openstack还未知道。到底是哪的原因呢？按照重启大法重启了两次，发现自己又变成了172.21.4.68的网址，后来左思右想都不对，再仔细一看，原来Bcast的地方变成了172.21.6.255，按道理来讲正常应该为172.21.4.255，为什么会变成172.21.6.255呢？ 首先，我查了一下什么是Bcast：broadcast address 即广播地址。Broadcast Address(广播地址)是专门用于同时向网络中所有工作站进行发送的一个地址。在使用TCP/IP 协议的网络中，主机标识段host ID 为全1 的IP 地址为广播地址，广播的分组传送给host ID段所涉及的所有计算机。例如，对于172.21.4.0（255.255.255.0 ）网段，其广播地址为172.21.4.255 （255 即为2 进制的11111111 ），当发出一个目的地址为172.21.4.255 的分组（封包）时，它将被分发给该网段上的所有计算机。按正常来讲，这些信息都是由DHCP服务器来分配的，那么是不是也就是说可能存在别的DHCP服务器（比如172.21.6.X）扰乱了这个虚拟机的网络，而且按正常来讲，B类私有网址（172.21.4.X）的网络掩码应该是255.255.0.0，现在是255.255.255.0，这样做是为了能多一些子网。多8位掩码，就是多了2的8次方个子网（256个），毕竟学校没有那么多网络资源。在虚拟机正常的情况下我查看了一下该虚拟机上的syslog情况，发现了如下： 可知该虚拟机正常情况下与172.21.4.3这个dhcp服务器进行了 交互，DHCP租约过程就是DHCP客户机动态获取IP地址的过程。DHCP租约过程分为4步：①客户机请求IP（客户机发DHCPDISCOVER广播包）；②服务器响应（服务器发DHCPOFFER广播包）；③客户机选择IP（客户机发DHCPREQUEST广播包）；④服务器确定租约（服务器发DHCPACK/DHCPNAK广播包）。我又查看了一下出问题的这个虚拟机的日志（出问题的时候）发现了问题 可以发现这个出问题的虚拟机早先通过了172.21.4.254进行了DHCP交互，获得了有问题的IP地址，因为这个iP地址是它分配的，所以也ping不通就很正常了，因为172.21.4.254也ping不通，这个是我们学校的网关，还有一个网关是172.21.6.254，可得出结论：172.21.4.254不仅是一个网关还是一个dhcp服务器，还有一张图，如下 172.21.201.1 是一个DHCP服务器也。而且不单单是4网段的会与之交互，连6网段的也会与之交互，难道说172.21.201.1是一个学校总的DHCp服务器？经过查看别的服务器可确定172.21.201.1确实是一个DHCP服务器。是不是可能172.21.4.254是一个中继DHCP服务呢？DHCP服务器是可以分配不同网段的IP地址的，可以通过两个网卡，比如一个网卡可以提供4网段的，一个可以提供6网段的，这样也和日志上的证据对上了。 最后，经过查看一批虚拟机发现所有的都会与172.21.4.3（既有DNS也有DHCP）交互，难道是那个有问题的虚拟机与172.21.4.254，然后再与172.21.201.1 进行DHCP交互，扰乱了正常的应该与172.21.4.3的交互？最后经与云提供商询问，可能就是这个原因 结论 最后的猜想：是不是因为172.21.201.1扰乱了我们的虚拟机，毕竟广播地址是172.21.6.255，现在我有个问题就是Bcast由谁来决定？？？？？]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BloomFilter]]></title>
    <url>%2F2018%2F09%2F14%2FBloomFilter%2F</url>
    <content type="text"><![CDATA[简介BloomFilter（布隆过滤器）是一种可以高效地判断元素是否在某个集合中的算法。 在很多日常场景中，都大量存在着布隆过滤器的应用。例如：检查单词是否拼写正确、网络爬虫的URL去重、黑名单检验，微博中昵称不能重复的检测。在工业界中，Google著名的分布式数据库BigTable也用了布隆过滤器来查找不存在的行或列，以减少磁盘查找的IO次数；Google Chrome浏览器使用BloomFilter来判断一个网站是否为恶意网站。 对于以上场景，可能很多人会说，用HashSet甚至简单的链表、数组做存储，然后判断是否存在不就可以了吗？ 当然，对于少量数据来说，HashSet是很好的选择。但是对于海量数据来说，BloomFilter相比于其他数据结构在空间效率和时间效率方面都有着明显的优势。 但是，布隆过滤器具有一定的误判率，有可能会将本不存在的元素判定为存在。因此，对于那些需要“零错误”的应用场景，布隆过滤器将不太适用。具体的原因将会在第二部分中介绍。 在本文的第二部分，本文将会介绍BloomFilter的基本算法思想；第三部分将会基于Google开源库Guava来讲解BloomFilter的具体实现；在第四部分中，将会介绍一些开源的BloomFilter的扩展，以解决目前BloomFilter的不足。 算法讲述布隆过滤器是基于Hash来实现的，在学习BloomFilter之前，也需要对Hash的原理有基本的了解。个人认为，BloomFilter的总体思想实际上和bitmap很像，但是比bitmap更节省空间，误判率也更低。 BloomFilter的整体思想并不复杂，主要是使用k个Hash函数将元素映射到位向量的k个位置上面，并将这k个位置全部置为1。当查找某元素是否存在时，查找该元素所对应的k位是否全部为1即可说明该元素是否存在。 缺点BloomFilter 由于并不存储元素，而是用位的01来表示元素是否存在，并且很有可能一个位时被多个元素同时使用。所以无法通过将某元素对应的位置为0来删除元素。 幸运的是，目前学术界和工业界都有很多方法扩展已解决以上问题。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#include "bloomfilter.h"#include "hashs.h"//#include "md5.h" #include "crc32.h"//#include "sha1.h"#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;#define HASH_FUNC_NUM 8#define BLOOM_SIZE 1000000#define BITSIZE_PER_BLOOM 32#define LIMIT (BLOOM_SIZE * BITSIZE_PER_BLOOM)/* * m=10n, k=8 when e=0.01 (m is bitsize, n is inputnum, k is hash_func num, e is error rate) * here m = BLOOM_SIZE*BITSIZE_PER_BLOOM = 32,000,000 (bits) * so n = m/10 = 3,200,000 (urls) * enough for crawling a website */static int bloom_table[BLOOM_SIZE] = &#123;0&#125;;pthread_mutex_t bt_lock = PTHREAD_MUTEX_INITIALIZER;//static MD5_CTX md5;//static SHA1_CONTEXT sha; static unsigned int encrypt(char *key, unsigned int id)&#123; unsigned int val = 0; switch(id)&#123; case 0: val = times33(key); break; case 1: val = timesnum(key,31); break; case 2: val = aphash(key); break; case 3: val = hash16777619(key); break; case 4: val = mysqlhash(key); break; case 5: //basically multithreads supported val = crc32((unsigned char *)key, strlen(key)); break; case 6: val = timesnum(key,131); break; /* int i; unsigned char decrypt[16]; MD5Init(&amp;md5); MD5Update(&amp;md5, (unsigned char *)key, strlen(key)); MD5Final(&amp;md5, decrypt); for(i = 0; i &lt; 16; i++) val = (val &lt;&lt; 5) + val + decrypt[i]; break; */ case 7: val = timesnum(key,1313); break; /* sha1_init(&amp;sha); sha1_write(&amp;sha, (unsigned char *)key, strlen(key)); sha1_final(&amp;sha); for (i=0; i &lt; 20; i++) val = (val &lt;&lt; 5) + val + sha.buf[i]; break; */ default: // should not be here abort(); &#125; return val;&#125;int search(char *url)&#123; unsigned int h, i, index, pos; int res = 0; pthread_mutex_lock(&amp;bt_lock); for (i = 0; i &lt; HASH_FUNC_NUM; i++) &#123; h = encrypt(url, i); h %= LIMIT; index = h / BITSIZE_PER_BLOOM; pos = h % BITSIZE_PER_BLOOM; if (bloom_table[index] &amp; (0x80000000 &gt;&gt; pos)) res++; else bloom_table[index] |= (0x80000000 &gt;&gt; pos); &#125; pthread_mutex_unlock(&amp;bt_lock); return (res == HASH_FUNC_NUM);&#125;]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>DataStructure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux可以Ping通但不能traceroute]]></title>
    <url>%2F2018%2F07%2F10%2FLinux%E5%8F%AF%E4%BB%A5Ping%E9%80%9A%E4%BD%86%E4%B8%8D%E8%83%BDtraceroute%2F</url>
    <content type="text"><![CDATA[今天闲来无事，想弄清楚学校至百度服务器的网络问题，结果不试不知道，一试吓一跳。完全出乎我的意料。我们学校的网关是172.21.6.254,172.21.4.254和172.21.7.254。发现可以ping通学校网关，却不能traceroute。如下图： 这是为什么？查了一下资料： windows的tracert预设是走ICMP协议，而linux的traceroute则预设走UDP协议，若两端点之间的UDP connection被任何firewall挡掉, 那 traceroute 就不行了. 原因好像大概知道了，就是有firewall把udp给挡掉了。解决方法：traceroute -I 加I参数改用ICMP协议。即下图，果然成功了。可以发现，从我的服务器到达百度服务器经过了21跳。 12345678910111213141516171819202122traceroute to baidu.com (123.125.115.110), 30 hops max, 60 byte packets 1 192.168.1.1 (192.168.1.1) 实验室路由器 2 172.21.6.254 (172.21.6.254) 学校网关内接口 3 172.21.200.5 (172.21.200.5) 学校网关外接口 4 172.30.201.6 (172.30.201.6) 本地局域网 5 211.71.94.251 (211.71.94.251) 北京市朝阳区 教育网 6 124.207.38.253 (124.207.38.253) 北京市 鹏博士宽带 7 * * * (有的就是这么设置，便于隐藏) 8 10.10.1.1 (10.10.1.1) 4.997 ms 本地局域网 9 218.241.251.105 (218.241.251.105) 北京市 鹏博士宽带10 218.241.253.241 (218.241.253.241) 北京市 鹏博士宽带11 218.241.245.181 (218.241.245.181) 北京市 鹏博士宽带12 202.99.1.173 (202.99.1.173) 北京市 鹏博士宽带13 * * *14 * * *15 202.106.42.97 (202.106.42.97) 北京市北京市 联通16 61.148.154.97 (61.148.154.97) 北京市 联通17 * * *18 61.148.146.194 (61.148.146.194) 北京市 联通19 61.49.168.98 (61.49.168.98) 北京市 联通20 * * *21 123.125.115.110 (123.125.115.110) 北京市 联通 (百度服务器) 那么我们就来了解一下traceroute的工作原理：Traceroute是用来侦测主机到目的主机之间所经路由情况的重要工具，也是最便利的工具。前面说到，尽管ping工具也可以进行侦测，但是，因为ip头的限制，ping不能完全的记录下所经过的路由器。所以Traceroute正好就填补了这个缺憾。Traceroute的原理是非常非常的有意思，它受到目的主机的IP后，首先给目的主机发送一个TTL=1（还记得TTL是什么吗？）的UDP(后面就 知道UDP是什么了)数据包，而经过的第一个路由器收到这个数据包以后，就自动把TTL减1，而TTL变为0以后，路由器就把这个包给抛弃了，并同时产生 一个主机不可达的ICMP数据报给主机。主机收到这个数据报以后再发一个TTL=2的UDP数据报给目的主机，然后刺激第二个路由器给主机发ICMP数据 报。如此往复直到到达目的主机。这样，traceroute就拿到了所有的路由器ip。从而避开了ip头只能记录有限路由IP的问题。有人要问，我怎么知道UDP到没到达目的主机呢？这就涉及一个技巧的问题，TCP和UDP协议有一个端口号定义，而普通的网络程序只监控少数的几个号码较 小的端口，比如说80,比如说23,等等。而traceroute发送的是端口号&gt;30000(真变态)的UDP报，所以到达目的主机的时候，目的 主机只能发送一个端口不可达的ICMP数据报给主机。主机接到这个报告以后就知道，主机到了，所以，说Traceroute是一个骗子一点也不为过Traceroute程序里面提供了一些很有用的选项，甚至包含了IP选路的选项。 当我以为终于弄懂得时候，我发现还是太年轻了，在windows我又手贱的试了一下，又发现了问题。为什么windows下也能ping通，但不能traceroute呢？ 欲知结果如何，还是待我知道以后。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器不能上网之谜]]></title>
    <url>%2F2018%2F05%2F11%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8D%E8%83%BD%E4%B8%8A%E7%BD%91%E4%B9%8B%E8%B0%9C%2F</url>
    <content type="text"><![CDATA[因为服务器一重启很多配置就要重新配，很麻烦，那么有没有一种方式可以实现开机自启动。当然有了，那就是配置rc.local文件了！ 那什么是rc.local脚本嗯？rc.local脚本是一个Ubuntu开机后会自动执行的脚本，我们可以在该脚本内添加命令行指令。该脚本位于/etc/路径下，需要root权限才能修改。该脚本具体格式如下： 1234567891011121314#!/bin/sh -e## rc.local## This script is executed at the end of each multiuser runlevel.# Make sure that the script will "exit 0" on success or any other# value on error.## In order to enable or disable this script just change the execution# bits.## By default this script does nothing.exit 0 注意: 一定要将命令添加在exit 0之前。里面可以直接写命令或者执行Shell脚本文件sh。 下面这个是用来开机自启动网络的命令 12345678# add by SongJianTao 2018-5-8# use to config network autoip addr del dev enp11s0 192.168.1.50/24tunctl -t tap0brctl addif be-ex tap0ifconfig tap0 0.0.0.0 promisc up#end by SongJianTao 注意如果不生效的话，可以尝试rc.local文件头部/bin/sh修改为/bin/bash，还可以增加日志输出功能，来查看最终为什么这个脚本不启动的原因，代码如下所示： 12345#logexec 2&gt; /tmp/rc.local.log # send stderr from rc.local to a log file exec 1&gt;&amp;2 # send stdout to the same log file set -x # tell sh to display commands before execution 今天正在奋笔疾书、焦头烂额的写报告，老胡说他要上网，但是服务器怎么也上不去网，查了很多原因也不知道（mmp，原来没出现过这种情况啊），经过层层排查，最后可能是因为交换机长期没有关的的问题，导致不能用（交换机的质量这么差的吗）， 所以直接把网线接在了路由器上，那就要更改配置了，不要直接在/etc/resolve.conf那设置，因为重启会消失，要在/etc/resolvconf/resolv.conf.d/base 这个文件下改，加上nameserver 192.168.1.1 这样就可以了，然后果然可以了，所以这个不能赖我了，要赖交换机！这个锅我不背！]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序]]></title>
    <url>%2F2018%2F04%2F27%2F%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[快速排序的基本思想快速排序（Quicksort）是对冒泡排序的一种改进。它的基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 快速排序的三个步骤1.选择基准：在待排序列中，按照某种方式挑出一个元素，作为 “基准”（pivot）；2.分割操作：以该基准在序列中的实际位置，把序列分成两个子序列。此时，在基准左边的元素都比该基准小，在基准右边的元素都比基准大；3.递归地对两个序列进行快速排序，直到序列为空或者只有一个元素； 选择基准元的方式对于分治算法，当每次划分时，算法若都能分成两个等长的子序列时，那么分治算法效率会达到最大。也就是说，基准的选择是很重要的。选择基准的方式决定了两个分割后两个子序列的长度，进而对整个算法的效率产生决定性影响。最理想的方法是，选择的基准恰好能把待排序序列分成两个等长的子序列。1.固定基准元：如果输入序列是随机的，处理时间是可以接受的。如果数组已经有序时，此时的分割就是一个非常不好的分割。因为每次划分只能使待排序序列减一，此时为最坏情况，快速排序沦为冒泡排序，时间复杂度为Θ(n^2)。而且，输入的数据是有序或部分有序的情况是相当常见的。因此，使用第一个元素作为基准元是非常糟糕的，应该立即放弃这种想法。2.随机基准元：这是一种相对安全的策略。由于基准元的位置是随机的，那么产生的分割也不会总是会出现劣质的分割。在整个数组数字全相等时，仍然是最坏情况，时间复杂度是O(n^2）。实际上，随机化快速排序得到理论最坏情况的可能性仅为1/(2^n）。所以随机化快速排序可以对于绝大多数输入数据达到O(nlogn）的期望时间复杂度。3.三数取中:引入的原因：虽然随机选取基准时，减少出现不好分割的几率，但是还是最坏情况下还是O(n^2），要缓解这种情况，就引入了三数取中选取基准。分析：最佳的划分是将待排序的序列分成等长的子序列，最佳的状态我们可以使用序列的中间的值，也就是第N/2个数。可是，这很难算出来，并且会明显减慢快速排序的速度。这样的中值的估计可以通过随机选取三个元素并用它们的中值作为基准元而得到。事实上，随机性并没有多大的帮助，因此一般的做法是使用左端、右端和中心位置上的三个元素的中值作为基准元。显然使用三数中值分割法消除了预排序输入的不好情形，并且减少快排大约5%的比较次数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576 #include&lt;stdio.h&gt; //交换子表的记录，使枢轴记录到位，并返回枢轴所在的位置 int Partition(int array[], int low, int high)&#123; /*三数中值分割法*/ int m = low + (high - low) / 2;//数组中间元素的下标 if (array[low]&gt;array[high]) //保证左端较小 swap(array, low, high); if (array[m] &gt; array[high]) //保证中间较小 swap(array, high, m); if (array[m] &gt; array[low]) swap(array, m, low); //保证左端最小 //此时array[low]已经为整个序列左中右三个关键字的中间值 int pivotkey = array[low]; /*固定基准元 int pivotkey = array[low]; */ /*随机基准元 int randomIndex = rand() % (high - low) + low;//取数组中随机下标 swap(array, randomIndex, low); //与第一个数交换 int pivotkey = array[low]; */ int i = low, j = high; while(i&lt;j) //从表的两端交替向中间扫描,当没有相遇 &#123; while (array[j] &gt;= pivotkey&amp;&amp;i&lt;j)&#123; j--; &#125; while (array[i] &lt;= pivotkey&amp;&amp;i&lt;j)&#123; i++; &#125; if (i&lt;j) &#123; swap(array, i, j); &#125; &#125; //最终将基准数归位 swap(array, low, i); return i; //返回枢轴所在的位置 &#125; void QSort(int array[], int low, int high)&#123; int pivot; if (low&lt;high) &#123; pivot = Partition(array, low, high);//算出枢轴值 QSort(array, low, pivot - 1); //对低子表递归排序 QSort(array, pivot + 1, high); //对高子表递归排序 &#125; &#125; void swap(int array[], int i, int j)&#123; //0∧0=0,0∧1=1,1∧1=0 //i,j为需要交换数值的数组下标 if(i == j) return;//下标相同直接返回 array[i] = array[i]^array[j]; array[j] = array[i]^array[j]; array[i] = array[i]^array[j];&#125;//对array做快速排序 int main()&#123; int array[] = &#123;9, 2, 4, 3, 5, 1, 0, 7, 8, 6&#125;; int i; int n = sizeof(array)/sizeof(int); QSort(array, 0, n - 1); for( i = 0; i &lt; n; i++) printf("%d", array[i]); return 0;&#125; 快速排序的优化对于很小的数组（N&lt;=20）,快速排序不如插入排序好。不仅如此，因为快速排序是递归的，所以这样的情况经常发生。通常的解决办法是对于小的数组不递归的使用快速排序，而代之以诸如插入排序这样的对小数组有效的排序算法。使用这种策略实际上可以节省大约15%的（相对于自始至终使用快速排序时）的运行时间。一种好的截止范围是N=10，虽然在5到20之间任一截止范围都有可能产生类似的结果。下面是代码： 123456789101112 void QSort(int array[], int low, int high)&#123; int pivot; if (high-low+1&gt;=10) &#123; pivot = Partition(array, low, high);//算出枢轴值 QSort(array, low, pivot - 1); //对低子表递归排序 QSort(array, pivot + 1, high); //对高子表递归排序 &#125; else&#123; InsertSort(array+low, high-low+1); //插入排序 &#125; &#125; 插入排序代码： 123456789101112void InsertSort(int array[], int n)&#123;int j;for (int i = 1; i &lt; n;++i)&#123; int key = array[i]; for (j = i; j&gt;0 &amp;&amp; array[j - 1] &gt; key;j--) &#123; array[j] = array[j - 1]; &#125; array[j] = key;&#125;&#125;]]></content>
      <categories>
        <category>Interview</category>
      </categories>
      <tags>
        <tag>InterView</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[守护进程]]></title>
    <url>%2F2018%2F04%2F21%2F%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[什么是守护进程 守护进程（Daemon）是运行在后台的一种特殊进程。它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。守护进程是一种很有用的进 程。 Linux的大多数服务器就是用守护进程实现的。比如，Internet服务器inetd，Web服务器httpd等。同时，守护进程完成许多系统任务。 比如，作业规划进程crond，打印进程lpd等。 守护进程最重要的特性是后台运行。在这一点上DOS下的常驻内存程序TSR与之相似。其次，守护进程必须与其运行前的环境隔离开来。这些环 境包括未关闭的文件描述符，控制终端，会话和进程组，工作目录以及文件创建掩模等。这些环境通常是守护进程从执行它的父进程（特别是shell）中继承下 来的。最后，守护进程的启动方式有其特殊之处。它可以在Linux系统启动时从启动脚本/etc/rc.d中启动，可以由作业规划进程crond启动，还 可以由用户终端（通常是 shell）执行。 一个守护进程的父进程是init进程，因为它真正的父进程在fork出子进程后就先于子进程exit退出了，所以它是一个由init继承的孤儿进程。守护进程是非交互式程序，没有控制终端，所以任何输出，无论是向标准输出设备stdout还是标准出错设备stderr的输出都需要特殊处理。实现守护进程要注意的地方 在后台运行为避免挂起控制终端将Daemon放入后台执行。方法是在进程中调用fork使父进程终止，让Daemon在子进程中后台执行。if(pid=fork())exit(0); //是父进程，结束父进程，子进程继续 脱离控制终端，登录会话和进程组 有必要先介绍一下Linux中的进程与控制终端，登录会话和进程组之间的关系：进程属于一个进程组，进程组号（GID）就是进程组长的进程号（PID）。登录会话可以包含多个进程组。这些进程组共享一个控制终端。这个控制终端通常是创建进程的登录终端。 控制终端，登录会话和进程组通常是从父进程继承下来的。我们的目的就是要摆脱它们，使之不受它们的影响。方法是在第1点的基础上，调用setsid()使进程成为会话组长： setsid(); 说明：当进程是会话组长时setsid()调用失败。但第一点已经保证进程不是会话组长。setsid()调用成功后，进程成为新的会话组长和新的进程组长，并与原来的登录会话和进程组脱离。由于会话过程对控制终端的独占性，进程同时与控制终端脱离。 禁止进程重新打开控制终端现在，进程已经成为无终端的会话组长。但它可以重新申请打开一个控制终端。可以通过使进程不再成为会话组长来禁止进程重新打开控制终端： if(pid=fork()) exit(0); //结束第一子进程，第二子进程继续（第二子进程不再是会话组长） 关闭打开的文件描述符 进程从创建它的父进程那里继承了打开的文件描述符。如不关闭，将会浪费系统资源，造成进程所在的文件系统无法卸下以及引起无法预料的错误。按如下方法关闭它们： for(i=0;i 关闭打开的文件描述符close(i); 改变当前工作目录进程活动时，其工作目录所在的文件系统不能卸下。一般需要将工作目录改变到根目录。对于需要转储核心，写运行日志的进程将工作目录改变到特定目录如 /tmpchdir(“/“) 重设文件创建掩模进程从创建它的父进程那里继承了文件创建掩模。它可能修改守护进程所创建的文件的存取位。为防止这一点，将文件创建掩模清除：umask(0); 处理SIGCHLD信号处理SIGCHLD信号并不是必须的。但对于某些进程，特别是服务器进程往往在请求到来时生成子进程处理请求。如果父进程不等待子进程结 束，子进程将成为僵尸进程（zombie）从而占用系统资源。如果父进程等待子进程结束，将增加父进程的负担，影响服务器进程的并发性能。在Linux下 可以简单地将 SIGCHLD信号的操作设为SIG_IGN。 signal(SIGCHLD,SIG_IGN); 这样，内核在子进程结束时不会产生僵尸进程。这一点与BSD4不同，BSD4下必须显式等待子进程结束才能释放僵尸进程。 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;fcntl.h&gt;int Create_Daemon()&#123; int pid; //屏蔽一些控制终端信号 signal(SIGTTOU,SIG_IGN); signal(SIGTTIN,SIG_IGN); signal(SIGTSTP,SIG_IGN); signal(SIGHUP ,SIG_IGN); //设置文件掩码 umask(0); //调用fork函数，父进程退出 pid = fork(); if(pid &lt; 0 ) &#123; printf("error fork"); return -1; &#125; else if(pid &gt; 0) &#123;//father exit(0); &#125; //设置新会话 setsid(); //处理SIGCHlD信号 signal(SIGCHLD,SIG_IGN); //禁止进程重新打开控制终端 if(pid = fork()) &#123;//father exit(0); &#125; else if(pid &lt;0) &#123; perror("fork"); exit(-1); &#125; //关闭打开的文件描述符 close(0);close(1);close(2); //改变当前的工作目录 chdir("/"); return 0;&#125;int main()&#123; Create_Daemon(); while(1); return 0;&#125; 总结创建过程： 创建子进程，父进程退出。 即创建子进程后，显示退出父进程，造成在终端这一进程已运行完毕的假象。之后的操作都由子进程完成。形式上做到与控制终端脱离。 孤儿进程：父进程先于子进程退出，则称为孤儿进程。系统发现一个孤儿进程后，就自动有1号进程（init进程）收养，即该子进程称为init进程的子进程。 在子进程中创建新会话。 继承：调用fork()函数，子进程会拷贝父进程的所有会话期、进程组、控制终端等。需要重设这些，才使子进程真正的与控制终端脱离。 进程组：一个或多个进程集合。每个进程有进程pid，进程组有组ID。pid和进程组ID都是一个进程的必备属性。每个进程组都有组长进程，其进程号等于进程组ID。当进程组ID不受组长进程退出的影响。 会话期：一个或多个进程组集合。通常，一个会话始于用户登录，终于用户退出，这之间的所有进程都属于该会话期。 setsid：创建新会话，并担任该会话组组长。有3个作用：让进程摆脱原会话的控制让进程摆脱原会话组的控制让进程摆脱原控制终端的控制 改变当前目录为根目录 继承：fork()创建的子进程还拷贝了父进程的当前工作目录。需要重设。 进程运行中，当前目录所在文件系统是不能卸载的，即原工作目录无法卸载。可能造成很多麻烦，如需要进入单用户模式。所以必须重设当前目录。 chdir(“/“)：重设为根目录 重设文件权限掩码 文件权限掩码：屏蔽掉文件权限中的对应位。有个文件权限掩码是050，它就屏蔽了文件组拥有者的可读与可执行权限。 继承：fork()创建的子进程还继承了父进程的文件权限掩码。 umask(0)：重设为0，灵活性更强。 关闭文件描述符 继承：fork()创建的子进程从父进程继承了一些已经打开了的文件。被打开的进程可能永远不会被守护进程使用，却消耗资源。所以必须手动关闭文件描述符为0、1和2 的3个文件（常说的输入、输出和报错）。 守护进程退出处理 可能需要支持用户在外部手动停止守护进程运行，通常使用kill命令。编码实现kill发出的signal信号处理，达到线程正常退出。 创建进程Linux上创建子进程的方式有三种(极其重要的概念)：一种是fork出来的进程，一种是exec出来的进程，一种是clone出来的进程。 fork是复制进程，它会复制当前进程的副本(不考虑写时复制的模式)，以适当的方式将这些资源交给子进程。所以子进程掌握的资源和父进程是一样的，包括内存中的内容，所以也包括环境变量和变量。但父子进程是完全独立的，它们是一个程序的两个实例。 exec是加载另一个应用程序，替代当前运行的进程，也就是说在不创建新进程的情况下加载一个新程序。exec还有一个动作，在进程执行完毕后，退出exec所在环境(实际上是进程直接跳转到exec上，执行完exec就直接退出。而非exec加载程序的方式是：父进程睡眠，然后执行子进程，执行完后回到父进程，所以不会立即退出当前环境)。所以为了保证进程安全，若要形成新的且独立的子进程，都会先fork一份当前进程，然后在fork出来的子进程上调用exec来加载新程序替代该子进程。例如在bash下执行cp命令，会先fork出一个bash，然后再exec加载cp程序覆盖子bash进程变成cp进程。但要注意，fork进程时会复制所有内存页，但使用exec加载新程序时会初始化地址空间，意味着复制动作完全是多余的操作，当然，有了写时复制技术不用过多考虑这个问题。 clone用于实现线程。clone的工作原理和fork相同，但clone出来的新进程不独立于父进程，它只会和父进程共享某些资源，在clone进程的时候，可以指定要共享的是哪些资源。 如何创建一个子进程？每次fork一个进程的时候，虽然调用一次fork()，会分别为两个进程返回两个值：对子进程的返回值为0，对父进程的返回值是子进程的pid。所以，可以使用下面的shell伪代码来描述运行一个ls命令时的过程：123456fpid=`fork()`if [ $fpid = 0 ]&#123; exec(ls) || echo "Can't exec ls" exit&#125;wait($fpid) 假设上面是在shell脚本中执行ls命令，那么fork的是shell脚本进程。fork后，父进程将继续执行，且if语句判断失败，于是执行wait；而子进程执行时将检测到fpid=0，于是执行exec(ls)，当ls执行结束，子进程因为exec的原因将退出。于是父进程的wait等待完成，继续执行后面的代码。 如果在这个shell脚本中某个位置，执行exec命令(exec命令调用的其实就是exec家族函数)，shell脚本进程直接切换到exec命令上，执行完exec命令，就表示进程终止，于是exec命令后面的所有命令都不会再执行。123456789101112131415161718192021222324252627282930313233+--------+| pid=7 || ppid=4 || bash |+--------+ | | calls fork V+--------+ +--------+| pid=7 | forks | pid=22 || ppid=4 | ----------&gt; | ppid=7 || bash | | bash |+--------+ +--------+ | | | waits for pid 22 | calls exec to run ls | V | +--------+ | | pid=22 | | | ppid=7 | | | ls | V +--------++--------+ || pid=7 | | exits| ppid=4 | &lt;---------------+| bash |+--------+ | | continues V一般情况下，兄弟进程之间是相互独立、互不可见的，但有时候通过特殊手段，它们会实现进程间通信。例如管道协调了两边的进程，两边的进程属于同一个进程组，它们的PPID是一样的，管道使得它们可以以&quot;管道&quot;的方式传递数据。进程是有所有者的，也就是它的发起者，某个用户如果它非进程发起者、非父进程发起者、非root用户，那么它无法杀死进程。且杀死父进程(非终端进程)，会导致子进程变成孤儿进程，孤儿进程的父进程总是init/systemd。 进程信号123456789101112131415161718192021222324252627282930313233343536- SIGHUP 数值1 终端退出时，此终端内的进程都将被终止- SIGQUIT 数值2 从键盘输入 Ctrl+&apos;\&apos;可以产生此信号- SIGILL 数值4 非法指令- SIGABRT 数值6 abort调用- SIGSEGV 数值11 非法内存访问- SIGTRAP 数值5 调试程序时使用的断点- SIGINT 2 中断进程，可被捕捉和忽略，几乎等同于sigterm，所以也会尽可能的释放执行clean-up，释放资源，保存状态等(CTRL+C)- SIGQUIT 3 从键盘发出杀死(终止)进程的信号- SIGKILL 9 强制杀死进程，该信号不可被捕捉和忽略，进程收到该信号后不会执行任何clean-up行为，所以资源不会释放，状态不会保存- SIGTERM 15 杀死(终止)进程，可被捕捉和忽略，几乎等同于sigint信号，会尽可能的释放执行clean-up，释放资源，保存状态等- SIGCHLD 17 当子进程中断或退出时，发送该信号告知父进程自己已完成，父进程收到信号将告知内核清理进程列表。所以该信号可以解除僵尸进程，也可以让非正常退出的进程工作得以正常的clean-up，释放资源，保存状态等。 - SIGSTOP 19 该信号是不可被捕捉和忽略的进程停止信息，收到信号后会进入stopped状态- SIGTSTP 20 该信号是可被忽略的进程停止信号(CTRL+Z)- SIGCONT 18 发送此信号使得stopped进程进入running，该信号主要用于jobs，例如bg &amp; fg 都会发送该信号。可以直接发送此信号给stopped进程使其运行起来 - SIGUSR1 10 用户自定义信号1- SIGUSR2 12 用户自定义信号2 其中SIGSEGV应该是我们最常接触的，尤其是使用C/C++程序的同学更是常见。main.c#include &lt;stdio.h&gt;int main()&#123; int *p = NULL; printf(&quot;hello world! \n&quot;); printf(&quot;this will cause core dump p %d&quot;, *p);&#125;使用下面命名编译即可产生运行时触发非法内存访问SIGSEGV信号，其中-g选项是编译添加调试信息，对于gdb调试非常有用。# gcc -g main.c -o test除了上述产生信号的方法外，使用我们经常使用的kill命令可以非常方便的产生这些信号，另外还有gcore命令可以在不终止进程的前提下产生core文件。 fork进程时资源的深拷贝和浅拷贝linux（和 unix）将进程的概念说的很大，而且很细，进程不再仅仅拥有一个执行流，而是有了一个容器，其实某种意义上它本身就是一个容器。unix传统将进程想成 了一个执行绪，概念真的就是如此简单，简单的东西往往是好的东西，复杂的反而会更加糟糕。进程概念的简单性使得fork可以如此美妙如此简单的实现，使得 创建一个可执行映像可以分为fork和exec，正如我的前文所述。更加重要的是，传统unix将执行绪和执行过程中需要的资源分开了，如此就可以将资源 作为一件物品在进程之间传递，这丝毫没有问题。 linux继承了unix一切好的东西，它的进程由task_struct表示，内部有很多表示资源的字段，比如files_struct表示打开的文 件，unix中的fork的意义就是进程复制，复制是如此的简单且直观，使得你只需要复制当前的一切就可以了，复制往往比创造要简单得多，就像我们小的时 候总喜欢抄作业一样，后来又有了写时复制，更加节约了时间增加了效率，于是fork的意义就是复制一切资源，而对于执行绪内部的资源比如地址空间采取写时 复制的策略。因此，文件描述符作为打开文件的索引其实也是一种资源，这样的话在fork的时候就可以传递给子进程了。linux就是这样像叉子一样不断的 fork最终形成一片天地。 linux一向以地址空间的隔离作为其安全的根本，但是为何却可以拿资源传来传去呢？地址空间不也是一种资源吗？互相传递资源不会引起不安全吗？当然不会 不安全，linux的进程结构设计的非常好，资源的共享完全在可控范围内，也就是说你可以选择传递或者不传递，一切由你决定，如果出了问题就是你自己造成 的而不是操作系统造成的。内核当然知道什么东西是可以安全传递的，比如文件描述符，该描述符对应的文件可以被子进程随意读写，而且文件描述符可以不变，对 于地址空间，它是进程的根本，在fork的时候是完全复制的（现在是写时复制），其实地址空间和文件描述符都是资源，那为什么内核对待它们的态度却不同 呢？对于文件描述符的复制是浅拷贝而对于地址空间的拷贝却是深拷贝，why？这就涉及到一个资源性质的问题。我们看一下文件和地址空间作为资源有何不同， 其实很容易就可以看出它们的不同，对于文件是共享资源，它并不是进程的内禀属性，进程可拥有可不拥有，因此它当然可以被共享，而对于地址空间，它是进程的 内禀属性，进程的定义中明确规定地址空间不能和别的进程共享，因此它就必须被进程独享，因此在fork的时候要深拷贝。举个例子，丈夫这个定义，它表示此 人拥有一个妻子，妻子就是他的一个资源，而且他也可以拥有一支钢笔，但是对于丈夫而言妻子是他的内禀属性，因此妻子是他独有的，但是钢笔和丈夫并没有必然 关系，因此钢笔是可以让别人用的。 fork出的子进程和父进程谁先运行？？linux中的进程是个很重要的概念，这个就不必多说了，linux中进程创建的fork机制继承了unix的基因，是操作系统中最重要的东西，fork中的写时复制机制是fork的精髓，是进程机制的精髓，它不仅仅代表了这些，它的实现还帮了另一个忙，这就是一般说来，linux在fork之后一般让新进程先运行，这是为了避免不必要的写时复制操作，因为新进程往往不再操作父进程的地址空间而是马上进行新的逻辑或者进行exec调用，但是却复制了父进程的地址空间，如果父进程优先运行，那么父进程的每一步运行只要是写操作都会导致写时复制，这是个根本没有必要的操作，系统的机制虽然要求写时复制，但是策略上却是很少会有子进程操作父进程地址空间的情况，父进程操作其地址空间却是一定的，因为它们共享一个地址空间，所以会导致没有用的写时复制，所以解决的办法就是让子进程先运行，最起码一旦子进程进行了exec，写时复制就再也么有必要了 参考https://www.cnblogs.com/f-ck-need-u/p/7058920.html#auto_id_0&lt;https://blog.csdn.net/dog250/article/details/5302859 &gt;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python之闭包]]></title>
    <url>%2F2018%2F04%2F17%2Fpython%E4%B9%8B%E9%97%AD%E5%8C%85%2F</url>
    <content type="text"><![CDATA[无意间，看到这么一道Python面试题：以下代码将输出什么？12345def Fun(): temp = [lambda x:i*x for i in range(4)] return tempfor everyLambda in Fun(): print(everyLambda(2)) 不是0,2,4,6，而是6,6,6,6 Python 的闭包的后期绑定导致的 late binding，这意味着在闭包中的变量是在内部函数被调用的时候被查找。因为在 for 里面 i 的值是不断改写的，但是 lambda 里面只是储存了 i 的符号，调用的时候再查找。所以结果是，当任何 testFun() 返回的函数被调用，在那时，i 的值是在它被调用时的周围作用域中查找，到那时，无论哪个返回的函数被调用，for 循环都已经完成了，i 最后的值是 3，因此，每个返回的函数 testFun 的值都是 3。因此一个等于 2 的值被传递进以上代码，它们将返回一个值 6 （比如： 3 x 2）。 那如何才能是0,2,4,6呢？ 创建一个闭包，通过使用默认参数立即绑定它的参数。为什么你加了默认参数就成功了呢？因为在创建函数的时候就要获取默认参数的值，放到 lambda 的环境中，所以这里相当于存在一个赋值，从而 lambda 函数环境中有了一个独立的 i。 12345 def Fun(): temp = [lambda x,i=i:i*x for i in range(4)] return tempfor everyLambda in Fun(): print(everyLambda(2)) 使用functools.partial 函数，把函数的某些参数（不管有没有默认值）给固定住（也就是相当于设置默认值） 123456 from functools import partialfrom operator import muldef Fun(): return [partial(mul,i) for i in range(4)]for everyLambda in Fun(): print(everyLambda(2)) 用生成器 1234 def Fun(): return (lambda x,i=i:i*x for i in range(4))for everyLambda in Fun(): print(everyLambda(2)) 利用yield的惰性求值的思想 12345 def Fun(): for i in range(4): yield lambda x: i*xfor everyLambda in Fun(): print(everyLambda(2))]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协议的简单C/S程序]]></title>
    <url>%2F2018%2F04%2F15%2F%E5%8D%8F%E8%AE%AE%E7%9A%84%E7%AE%80%E5%8D%95C-S%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[C/S 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;sys/socket.h&gt;#include &lt;string.h&gt; #define MAX_LINE 100 int main(int argc,char **argv)&#123; struct sockaddr_in sin; char buf[MAX_LINE]; int s_fd; int port = 8000; char *str = "test string"; char *serverIP = "127.0.0.1"; int n; if(argc &gt; 1) &#123; str = argv[1]; &#125; //服务器端填充 sockaddr结构 bzero(&amp;sin , sizeof(sin)); sin.sin_family = AF_INET; inet_pton(AF_INET,serverIP,(void *)&amp;sin.sin_addr); sin.sin_port = htons(port); int bReuseaddr=1; struct timeval nNetTimeout=&#123;0,1000&#125;;//1绉? if((s_fd = socket(AF_INET,SOCK_STREAM,0)) == -1) &#123; perror("fail to create socket"); exit(1); &#125; /*if(setsockopt(s_fd,SOL_SOCKET ,SO_REUSEADDR,&amp;bReuseaddr,sizeof(int)) == -1) &#123; perror("fail to set opt reuseaddr"); exit(1); &#125; */ if(setsockopt(s_fd,SOL_SOCKET ,SO_RCVTIMEO,&amp;nNetTimeout,sizeof(nNetTimeout)) == -1) &#123; perror("fail to set opt rcvtimeout"); exit(1); &#125; if(connect(s_fd,(struct sockaddr *)&amp;sin,sizeof(sin)) == -1) &#123; perror("fail to connect server"); exit(1); &#125; n = send(s_fd, str , strlen(str) + 1, 0); if(n == -1) &#123; perror("fail to send"); exit(1); &#125; n = recv(s_fd ,buf , MAX_LINE, 0); if(n == -1) &#123; perror("fail to recv"); exit(1); &#125; printf("the length of str = %s\n" , buf); if(close(s_fd) == -1) &#123; perror("fail to close"); exit(1); &#125; return 0;&#125; server123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;ctype.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;string.h&gt; #define INET_ADDR_STR_LEN 1024#define MAX_LINE 100 int main(int argc,char **argv)&#123; struct sockaddr_in sin; struct sockaddr_in cin; int l_fd; int c_fd; socklen_t len; char buf[MAX_LINE]; char addr_p[INET_ADDR_STR_LEN]; int port = 8000; int n; bzero(&amp;sin , sizeof(sin)); sin.sin_family = AF_INET; sin.sin_addr.s_addr = INADDR_ANY; sin.sin_port = htons(port); //参数设置 int bReuseaddr=1; struct timeval nNetTimeout=&#123;0,10000&#125;;//10秒 if((l_fd = socket(AF_INET,SOCK_STREAM,0)) == -1) &#123; perror("fail to create socket"); exit(1); &#125; if(setsockopt(l_fd,SOL_SOCKET ,SO_REUSEADDR,&amp;bReuseaddr,sizeof(int)) == -1) &#123; perror("fail to set opt reuseaddr"); exit(1); &#125; if(setsockopt(l_fd,SOL_SOCKET ,SO_RCVTIMEO,&amp;nNetTimeout,sizeof(nNetTimeout)) == -1) &#123; perror("fail to set opt rcvtimeout"); exit(1); &#125; if(bind(l_fd,(struct sockaddr *)&amp;sin ,sizeof(sin) ) == -1) &#123; perror("fail to bind"); exit(1); &#125; if(listen(l_fd,10) == -1) &#123; perror("fail to listen"); exit(1); &#125; printf("waiting.....\n"); while(1) &#123; //if((c_fd = accept(l_fd,(struct sockaddr *)&amp;cin, &amp;len)) == -1) if((c_fd = accept(l_fd,NULL, 0)) == -1) &#123; // perror("fail to accept"); // exit(1); continue; &#125; n = recv(c_fd , buf, MAX_LINE, 0); if(n == -1) &#123; perror("fail to recv"); exit(1); &#125; else if(n == 0) &#123; printf("the connect has been closed\n"); close(c_fd); continue; &#125; //inet_ntop(AF_INET,&amp;cin.sin_addr,addr_p,sizeof(addr_p)); printf("content is : %s\n",buf); n = strlen(buf); sprintf(buf,"%d",n); n = send(c_fd , buf, sizeof(buf) + 1 , 0); if( n == -1) &#123; perror("fail to send"); exit(1); &#125; if(close(c_fd) == -1) &#123; perror("fail to close"); exit(1); &#125; &#125; if(close(l_fd) == -1) &#123; perror("fail to close"); exit(1); &#125; return 0;&#125; linux下基于简单socket编程实现C/S12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;netinet/in.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;string.h&gt; #define MAX_LINE 1024#define INET_ADDR_STR 16 void my_fun(char *p)&#123; if(p == NULL) &#123; return; &#125; for( ; *p != '\0' ; p++) &#123; if((*p &gt;= 'a') &amp;&amp; (*p &lt;= 'z')) &#123; *p = *p - 32; &#125; &#125; return;&#125;int main(int argc,char **argv)&#123; struct sockaddr_in sin; //服务器通信地址结构 struct sockaddr_in cin; //保存客户端通信地址结构 int l_fd; int c_fd; socklen_t len; char buf[MAX_LINE]; //存储传送内容的缓冲区 char addr_p[INET_ADDR_STR]; //存储客户端地址的缓冲区 int port = 8000; int n; bzero((void *)&amp;sin,sizeof(sin)); sin.sin_family = AF_INET; //使用IPV4通信域 sin.sin_addr.s_addr = INADDR_ANY; //服务器可以接受任意地址 sin.sin_port = htons(port); //端口转换为网络字节序 l_fd = socket(AF_INET,SOCK_STREAM,0); //创建套接子,使用TCP协议 bind(l_fd,(struct sockaddr *)&amp;sin,sizeof(sin)); listen(l_fd,10); //开始监听连接 printf("waiting ....\n"); while(1) &#123; c_fd = accept(l_fd,(struct sockaddr *)&amp;cin,&amp;len); n = read(c_fd,buf,MAX_LINE); //读取客户端发送来的信息 inet_ntop(AF_INET,&amp;cin.sin_addr,addr_p,INET_ADDR_STR); //将客户端传来地址转化为字符串 printf("client IP is %s,port is %d\n",addr_p,ntohs(cin.sin_port)); printf("content is : %s\n", buf); //打印客户端发送过来的数据 my_fun(buf); write(c_fd,buf,n); //转换后发给客户端 close(c_fd); &#125; printf("buf = %s\n",buf); if((close(l_fd)) == -1) &#123; perror("fail to close\n"); exit(1); &#125; return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;string.h&gt;#include &lt;netinet/in.h&gt; #define MAX_LINE 1024 int main(int argc,char **argv)&#123; struct sockaddr_in sin; //服务器的地址 char buf[MAX_LINE]; int sfd; int port = 8000; char *str = "test string"; char *serverIP = "127.0.0.1"; if(argc &gt; 1) &#123; str = argv[1]; //读取用户输入的字符串 &#125; bzero((void *)&amp;sin,sizeof(sin)); sin.sin_family = AF_INET; //使用IPV4地址族 inet_pton(AF_INET,serverIP,(void *)&amp;(sin.sin_addr)); sin.sin_port = htons(port); /*理论上建立socket时是指定协议，应该用PF_xxxx，设置地址时应该用AF_xxxx。当然AF_INET和PF_INET的值是相同的，混用也不会有太大的问题。也就是说你socket时候用PF_xxxx，设置的时候用AF_xxxx也是没关系的，这点随便找个TCPIP例子就可以验证出来了。如下，不论是AF_INET还是PF_INET都是可行的，只不过这样子的话，有点不符合规范。*/ sfd = socket(AF_INET,SOCK_STREAM,0); connect(sfd,(struct sockaddr *)&amp;(sin),sizeof(sin)); printf("str = %s\n" , str); write(sfd , str , strlen(str) + 1); read(sfd , buf , MAX_LINE); printf("recive from server: %s\n" , buf); close(sfd); return 0;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack中的RPC请求分析]]></title>
    <url>%2F2018%2F04%2F08%2Fopenstack%E4%B8%AD%E7%9A%84RPC%E8%AF%B7%E6%B1%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[openstack中的RPC请求分析 首先，什么是RPC请求？我们都知道，rpc就是远程过程调用，是Openstack中一种用来实现跨进程(或者跨机器)的通信机制。Openstack中同项目内(如nova, neutron, cinder…)各服务(service)及通过RPC实现彼此间通信。Openstack中还有另外两种跨进程的通信方式：数据库和Rest API。一般情况下，openstack各个项目之间通过RestAPI接口进行相互访问，而项目内部服务之间则通过RPC请求的方式进行通信。 其他参考官网详细介绍了Openstack RPC中的基本概念及API设计http://https://wiki.openstack.org/wiki/Oslo/Messaging，其实它的rpc的设计参考了Sun RPC的设计，Sun RPC的介绍可以参看http://http://en.wikipedia.org/wiki/Open_Network_Computing_Remote_Procedure_Call这篇文章。 RPC的使用场景 随机调用某server上的一个方法：Invoke Method on One of Multiple Servers这个应该是Openstack中最常用的一种RPC调用，每个方法都会有多个server来提供，client调用时由底层机制选择一个server来处理这个调用请求。像nova-scheduler, nova-conductor都可以以这种多部署方式提供服务。这种场景通过AMQP的topic exchange实现。所有server在binding中为binding key指定一个相同的topic， client在调用时使用这个topic既可实现。 调用某特定server上的一个方法：Invoke Method on a Specific Server 一般Openstack中的各种scheduler会以这种方式调用。通常scheduler都会先选定一个节点，然后调用该节点上的服务。这种场景通过AMQP的topic exchange实现。每个server在binding中为其binding key指定一个自己都有的topic， client在调用时使用这个topic既可实现。 调用所有server上的一个方法：Invoke Method on all of Multiple Servers 这种其实就是一个广播系统。就像开会议，台上的人讲话，台下的人都能听到。Openstack中有些rpcapi.py的某些方法带有fanout=True参数，这些都是让所有server处理某个请求的情况。例子： neutron中所有plugin都会有一个AgentNotifierApi，这个rpc是用来调用安装在compute上的L2 agent。因为存在多个L2 agent(每个compute上都会有)，所以要用广播模式。这种场景通过AMQP的fanout exchange实现。每个server在binding中将其队列绑定到一个fanout exchange， client在调用时指定exchange类型为fanout即可。server和client使用同一个exchange。 rpc.call和rpc.cast的区别： RPC.call：发送请求到消息队列，等待返回最终结果。 RPC.cast：发送请求到消息队列，不需要等待最终返回的结果。其实还有一种rpc调用，也就是RPC.Notifier:发送各类操作消息到队列，不需要等待最终的返回结果。RPC.call、RPC.cast一般用于同一个项目下的服务之间进行的“内部“请求；RPC.Notifier发送的操作消息，目前被ceilometer notification服务所接收。 举个栗子：比如虚拟机创建过程，创建虚拟机等TaskAPI任务，已经由nova-conductor承担，因此nova-api监听到创建虚拟机的HTTP请求后，会通过RPC调用（是cast）nova.conductor.manager.ComputeTaskManager中的build_instance()方法。nova-conductor会在build_instances()中生成request_spec字典，其中包括了详细的虚拟机信息，nova-conductor通过rpc.call方法向nova-scheduler发出请求，nova-scheduler依据这些信息为虚拟机选择一个最佳的主机，然后返回给nova-conductor。（为什么有返回，因为是call）然后nova-conductor再通过nova-compute创建虚拟机。nova-compute首先会使用Resource Tracker的Claim机制检测一下主机的可用资源是否能够满足新建虚拟机的需要，然后通过具体的virt Driver创建虚拟机。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python之intern机制]]></title>
    <url>%2F2018%2F04%2F08%2Fpython%E4%B9%8Bintern%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[python之intern机制(字符串驻留机制) 在主流面向对象的编程语言中intern 机制对于处理字符串已经成为一种标配，通过 intern 机制可以提高字符串的处理效率，当然，解释器内部很对 intern 机制的使用策略是有考究的，有些场景会自动使用 intern ，有些地方需要通过手动方式才能启动。python中如果创建的对象一样，那么就会自动启动intern机制，所有对象会指向同一个地址。先看个栗子：123456a = "helloworld"b = "helloworld"c = "helloworld"print(id(a))print(id(b))print(id(c)) # 三个对象的内存地址一样 其实这就是python的intern机制，这样可以节省内存。看一下源码12345678910111213141516171819202122232425262728293031323334353637static PyObject *interned;void PyString_InternInPlace(PyObject **p)&#123; register PyStringObject *s = (PyStringObject *)(*p); PyObject *t; if (s == NULL || !PyString_Check(s)) Py_FatalError("PyString_InternInPlace: strings only please!"); /* If it's a string subclass, we don't really know what putting it in the interned dict might do. */ if (!PyString_CheckExact(s)) return; if (PyString_CHECK_INTERNED(s)) return; if (interned == NULL) &#123; interned = PyDict_New(); if (interned == NULL) &#123; PyErr_Clear(); /* Don't leave an exception */ return; &#125; &#125; t = PyDict_GetItem(interned, (PyObject *)s); if (t) &#123; Py_INCREF(t); Py_DECREF(*p); *p = t; return; &#125; if (PyDict_SetItem(interned, (PyObject *)s, (PyObject *)s) &lt; 0) &#123; PyErr_Clear(); return; &#125; /* The two references in interned are not counted by refcnt. The string deallocator will take care of this */ Py_REFCNT(s) -= 2; PyString_CHECK_INTERNED(s) = SSTATE_INTERNED_MORTAL;&#125; 可以看到interned的定义是一个PyObject,但从下面的代码可以看出，在interned=nul的时候，interned = PyDict_New();所以它实际上是一个PyDictObject，我们可以暂时理解为c++里面的map对象。对一个PyStringObject对象进行intern机制处理的时候，会通过PyDict_GetItem去从Interned对象中查找有没有一样的已经创建的对象，有的话就直接拿来用，没有的话就说明这种对象是第一次创建，用PyDict_SetItem函数把相应的信息存到interned里面，下次再创建一样的就能从中找到了。 Python解释器会缓冲256个字符串, 第257个字符串多次赋值不同的变量名, id()查看的结果就不同了。 intern机制的优点是。须要值同样的字符串的时候（比方标识符）。直接从池里拿来用。避免频繁的创建和销毁。提升效率，节约内存。缺点是，拼接字符串、对字符串改动之类的影响性能。 由于是不可变的。所以对字符串改动不是inplace操作。要新建对象。 这也是为什么拼接多字符串的时候不建议用+而用join()。join()是先计算出全部字符串的长度，然后一一拷贝，仅仅new一次对象。 须要小心的坑。并非全部的字符串都会採用intern机制。仅仅包括下划线、数字、字母的字符串才会被intern。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python之协程]]></title>
    <url>%2F2018%2F04%2F07%2Fpython%E4%B9%8B%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Generator 其中一个特性就是不是一次性生成数据，而是生成一个可迭代的对象，在迭代时，根据我们所写的逻辑来控制其启动时机。 Generator 另一个很大作用可以说就是当做协程使用。协程就是你可以暂停执行的函数。简而言之，协程是比线程更为轻量的一种模型，我们可以自行控制启动与停止的时机。知乎上说的，最易懂的解释：你行你就上，不行旁边等着让别人上，啥时候行了你再上。在 Python 中其实没有专门针对协程的这个概念，社区一般而言直接将 Generator 作为一种特殊的协程看待，想想，我们可以用 next 或 next() 方法或者是 send() 方法唤醒我们的 Generator ，在运行完我们所规定的代码后， Generator 返回并将其所有状态冻结。这是不是很让我们 Excited 呢！！ 总而言之，协程比线程更节省资源，效率更高，并且更安全。如果使用线程做过重要的编程，你就知道写出程序有多么困难，因为调度程序任何时候都能中断线程。必须记住保留锁，去保护程序中的重要部分，防止多步操作在执行的过程中中断，防止数据处于无效状态。而协程默认会做好全方位保护，以防止中断。我们必须显式产出才能让程序的余下部分运行。对协程来说，无需保留锁，在多个线程之间同步操作，协程自身就会同步，因为在任意时刻只有一个协程运行。想交出控制权时，可以使用 yield 或 yield from 把控制权交还调度程序。这就是能够安全地取消协程的原因：按照定义，协程只能在暂停的 yield处取消，因此可以处理 CancelledError 异常，执行清理操作。 这是一个异步编程的例子，将代码与事件循环及其相关的函数一一对应起来。这个例子里包含的几个协程，代表着火箭发射的倒计时，并且看起来是同时开始的。这是通过并发实现的异步编程；3个不同的协程将分别独立运行，并且都在同一个线程内完成。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import datetimeimport heapqimport typesimport timeclass Task: """Represent how long a coroutine should before starting again. Comparison operators are implemented for use by heapq. Two-item tuples unfortunately don't work because when the datetime.datetime instances are equal, comparison falls to the coroutine and they don't implement comparison methods, triggering an exception. Think of this as being like asyncio.Task/curio.Task. """ def __init__(self, wait_until, coro): self.coro = coro self.waiting_until = wait_until def __eq__(self, other): return self.waiting_until == other.waiting_until def __lt__(self, other): return self.waiting_until &lt; other.waiting_untilclass SleepingLoop: """An event loop focused on delaying execution of coroutines. Think of this as being like asyncio.BaseEventLoop/curio.Kernel. """ def __init__(self, *coros): self._new = coros self._waiting = [] def run_until_complete(self): # Start all the coroutines. for coro in self._new: wait_for = coro.send(None) heapq.heappush(self._waiting, Task(wait_for, coro)) # Keep running until there is no more work to do. while self._waiting: now = datetime.datetime.now() # Get the coroutine with the soonest resumption time. task = heapq.heappop(self._waiting) if now &lt; task.waiting_until: # We're ahead of schedule; wait until it's time to resume. delta = task.waiting_until - now time.sleep(delta.total_seconds()) now = datetime.datetime.now() try: # It's time to resume the coroutine. wait_until = task.coro.send(now) heapq.heappush(self._waiting, Task(wait_until, task.coro)) except StopIteration: # The coroutine is done. pass@types.coroutinedef sleep(seconds): """Pause a coroutine for the specified number of seconds. Think of this as being like asyncio.sleep()/curio.sleep(). """ now = datetime.datetime.now() wait_until = now + datetime.timedelta(seconds=seconds) # Make all coroutines on the call stack pause; the need to use `yield` # necessitates this be generator-based and not an async-based coroutine. actual = yield wait_until # Resume the execution stack, sending back how long we actually waited. return actual - nowasync def countdown(label, length, *, delay=0): """Countdown a launch for `length` seconds, waiting `delay` seconds. This is what a user would typically write. """ print(label, 'waiting', delay, 'seconds before starting countdown') delta = await sleep(delay) print(label, 'starting after waiting', delta) while length: print(label, 'T-minus', length) waited = await sleep(1) length -= 1 print(label, 'lift-off!')def main(): """Start the event loop, counting down 3 separate launches. This is what a user would typically write. """ loop = SleepingLoop(countdown('A', 5), countdown('B', 3, delay=2), countdown('C', 4, delay=1)) start = datetime.datetime.now() loop.run_until_complete() print('Total elapsed time is', datetime.datetime.now() - start)if __name__ == '__main__': main()# A waiting 0 seconds before starting countdown#B waiting 2 seconds before starting countdown#C waiting 1 seconds before starting countdown#A starting after waiting 0:00:00.001000#A T-minus 5#C starting after waiting 0:00:01.000058#C T-minus 4#A T-minus 4#B starting after waiting 0:00:02.000115#B T-minus 3#C T-minus 3#A T-minus 3#B T-minus 2#C T-minus 2#A T-minus 2#B T-minus 1#C T-minus 1#A T-minus 1#B lift-off!#C lift-off!#A lift-off!#Total elapsed time is 0:00:05.001286 但是基于async的协程和基于生成器的协程会在对应的暂停表达式上面有所不同？主要原因是出于最优化Python性能的考虑，确保你不会将刚好有同样API的不同对象混为一谈。由于生成器默认实现协程的API，因此很有可能在你希望用协程的时候错用了一个生成器。而由于并不是所有的生成器都可以用在基于协程的控制流中，你需要避免错误地使用生成器。但是由于 Python 并不是静态编译的，它最好也只能在用基于生成器定义的协程时提供运行时检查。这意味着当用types.coroutine时，Python 的编译器将无法判断这个生成器是用作协程还是仅仅是普通的生成器（记住，仅仅因为types.coroutine这一语法的字面意思，并不意味着在此之前没有人做过types = spam的操作），因此编译器只能基于当前的情况生成有着不同限制的操作码。关于基于生成器的协程和async定义的协程之间的差异，我想说明的关键点是只有基于生成器的协程可以真正的暂停执行并强制性返回给事件循环。你可能不了解这些重要的细节，因为通常你调用的像是asyncio.sleep() function 这种事件循环相关的函数，由于事件循环实现他们自己的API，而这些函数会处理这些小的细节。对于我们绝大多数人来说，我们只会跟事件循环打交道，而不需要处理这些细节，因此可以只用async定义的协程。但是如果你和我一样好奇为什么不能在async定义的协程中使用asyncio.sleep()，那么这里的解释应该可以让你顿悟。 Generator迭代的就是通过内建的next（）或next()方法调用内建的send（）方法。 与其它特性一起，PEP 342 为生成器引入了 send() 方法。这让我们不仅可以暂停生成器，而且能够传递值到生成器暂停的地方。还是以我们的 range() 为例，你可以让序列向前或向后跳过几个值。 123456789101112131415161718192021def jumping_range(up_to): """Generator for the sequence of integers from 0 to up_to, exclusive. Sending a value into the generator will shift the sequence by that amount. """ index = 0 while index &lt; up_to: jump = yield index if jump is None: jump = 1 index += jumpif __name__ == '__main__': iterator = jumping_range(5) print(next(iterator)) # 0 print(iterator.send(2)) # 2 print(next(iterator)) # 3 print(iterator.send(-1)) # 2 for x in iterator: print(x) # 3, 4 openstack中就是用了协程模型，利用Python库Eventlet可以产生很多协程，这些协程之间只有在调用到了某些特殊的Eventlet库函数的时候（比如睡眠sleep、IO调用等）才会发生切换。协程的实现主要是在协程休息时把当前的寄存器保存起来，然后重新工作时将其恢复，可以简单的理解为，在单个线程内部有多个栈去保存切换时的线程上下文，因此，协程可以理解为一个线程内的伪并发方式。但是由于Eventlet本身的一些局限性，目前openstack考虑用AsynclIO来代替他。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C之内存对齐]]></title>
    <url>%2F2018%2F04%2F06%2FC%E4%B9%8B%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90%2F</url>
    <content type="text"><![CDATA[内存对齐123456789101112131415161718192021222324252627282930313233#include&lt;stdio.h&gt;typedef struct bb&#123; int id; //[0]-[3] 第一个数据成员放在offset为0的地方 double weight;//从该成员的大小的整数倍位置开始存，也就是[8]-[15] float height; //[16]-[19],结构体的总大小必须是内部最大成员的整数倍，不足的补齐，所以补上[20]-[23] &#125;BB;typedef struct aa&#123; char name[2]; //[0]-[1] int id; //[4]-[7] double score; //[8]-[15] short grade; // [16]-[17] 从该成员的大小的整数倍开始存，也就是24 BB b; //[24]-[47] &#125;AA;int main()&#123; AA a; BB b; printf("%d\n",sizeof(a)); printf("%d",sizeof(b)); return 0;&#125;// #pragma pack(1) 不内存对齐 32 16 // #pragma pack(2) 32 16// #pragma pack(4) 36 16// #pragma pack(8) 48 24 如果#pragma pack (n)中指定的n大于结构体中最大成员的size，则其不起作用，结构体仍然按照size最大的成员进行对界。 为什么要进行内存对齐？ 一种提高内存访问速度的策略，cpu在访问未对其的内存需要经过两次内存访问，而经过内存对齐一次就可以了。 平台原因（移植原因）：不是所有的硬件平台都能访问任意地址上的任意数据，某些硬件平台只能在某些地址处取某些特定类型的数据，否则抛出硬件异常 其实数据在内存中存放时，是否对齐并不重要，重要的是你怎样去访问它。memcpy的实现本身并不简单(你在源码里看到的通过while每次拷贝一个char的只是一个例子，并不是真实的memcpy)，它考虑了是否对齐。当检测到内存是对齐时，memcpy调用合适的指令(比较这里拷贝一个int，就调用LDR)，一次拷贝多个字节，以提高效率。当检测到不对齐时，先调用LDRB遂个字节拷贝，直到对齐部分后再调用合适的指令拷贝。因此，在上面的例子中，它是先调用LDRB的，因为LDRB是按1byte对齐(所有的内存都按这个对齐)，所以不会触发报错。但效率就要慢一点了，毕竟要拷贝几次。内存对齐本身对程序员来说是透明的，即程序员该取变量就取变量，该存就存，编译程序时编译器会把变量按本身的平台进行对齐。况且现在的CPU都很高级，别说服务器，台式机的CPU，ARM 7以上应该也支持内存不对齐访问了。但如果你要写一个内存池(boost的ordered_pool有对齐的例子)，或者使用了reinterpret_cast这种对内存直接进行操作的函数，这方面还是要注意一下，即使CPU支持，效率也会受到影响。 #pragma pack(push,1) #pragma pack(pop)强制把结构体按照1byte对齐。]]></content>
      <categories>
        <category>C</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网易18实习算法题]]></title>
    <url>%2F2018%2F04%2F02%2F%E7%BD%91%E6%98%9318%E5%AE%9E%E4%B9%A0%E7%AE%97%E6%B3%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1.小Q正在给一条长度为n的道路设计路灯安置方案。 为了让问题更简单,小Q把道路视为n个方格,需要照亮的地方用’.’表示, 不需要照亮的障碍物格子用’X’表示。 小Q现在要在道路上设置一些路灯, 对于安置在pos位置的路灯, 这盏路灯可以照亮pos - 1, pos, pos + 1这三个位置。 小Q希望能安置尽量少的路灯照亮所有’.’区域, 希望你能帮他计算一下最少需要多少盏路灯。123456789101112131415161718192021222324252627 # 算法思路 # x的位置也是可以放置路灯的 其实是贪心 #遍历路灯字符串，遇见“.”，就给计数器+1，然后往后挪 三个位置。如果遇到“X”，就直接往后挪一个位置。 #编程思路 #路灯个数放入数组n中，路灯对应的字符串放入数组#lantern中，要放路灯的个数放入lantern_count中。这三#个数组是一一对应的。双重循环来遍历lantern中的字符#串，如果遇到“.”，对应的lantern_count+=1，j+=3(挪三#个位置)。如果遇到“X”，j+=1(挪一个位置)。if __name__ =='__main__': count = int(input()) # 测试用例个数 n = [] lantern = [] for i in range(count): n_tmp = int(input()) # 路灯个数 n.append(n_tmp) lantern_tmp = input() # 路灯分布字符串 lantern.append(lantern_tmp) # ['.x.', '...xxx...xx'] lantern_count = [0 for i in range(count)] # [0, 0] for i in range(len(lantern)): j = 0 while (j &lt; len(lantern[i])): if lantern[i][j] == '.': j += 3 lantern_count[i] +=1 else: j += 1 print(lantern_count[0]) for i in range(len(lantern_count)-1): print(lantern_count[i+1]) 牛牛去犇犇老师家补课，出门的时候面向北方，但是现在他迷路了。虽然他手里有一张地图，但是他需要知道自己面向哪个方向，请你帮帮他。输入描述:每个输入包含一个测试用例。每个测试用例的第一行包含一个正整数，表示转方向的次数N(N&lt;=1000)。接下来的一行包含一个长度为N的字符串，由L和R组成，L表示向左转，R表示向右转。 1234567891011 # 比如输入3 LRL 输出Wn = input()m = input()dict1 = &#123;'1':'E','2':'S','3':'W','0':'N'&#125;init = 0for i in range(int(n)): if m[i]=='L': init-=1 else: init+=1print(dict1[str(init%4)]) # -1%4是3 牛牛总是睡过头，所以他定了很多闹钟，只有在闹钟响的时候他才会醒过来并且决定起不起床。从他起床算起他需要X分钟到达教室，上课时间为当天的A时B分，请问他最晚可以什么时间起床输入描述:每个输入包含一个测试用例。每个测试用例的第一行包含一个正整数，表示闹钟的数量N(N&lt;=100)。接下来的N行每行包含两个整数，表示这个闹钟响起的时间为Hi(0&lt;=A&lt;24)时Mi(0&lt;=B&lt;60)分。接下来的一行包含一个整数，表示从起床算起他需要X(0&lt;=X&lt;=100)分钟到达教室。接下来的一行包含两个整数，表示上课时间为A(0&lt;=A&lt;24)时B(0&lt;=B&lt;60)分。数据保证至少有一个闹钟可以让牛牛及时到达教室。 1234567891011121314151617181920# 3# 5 0# 6 0# 7 0# 59 # 6 59import sysif __name__ =='__main__': n = int(sys.stdin.readline().strip()) values= [] for i in range(n): line = sys.stdin.readline().strip().split(' ') values.append(line) # [['5', '0'], ['6', '0'], ['7', '0']] dst_time = int(sys.stdin.readline().strip()) class_time = sys.stdin.readline().strip().split(' ') b = list(map(lambda x :int(x[0])*60+int(x[1]), values)) deadline = int(class_time[0]) * 60 + int(class_time[1]) - dst_time c = [n for n in b if n &lt;= deadline ] print((str(max(c)//60)) + ' ' + str(max(c)%60)) # 输出6 0]]></content>
      <categories>
        <category>InterView</category>
      </categories>
      <tags>
        <tag>InterView</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python知识点（21-30）]]></title>
    <url>%2F2018%2F04%2F01%2FPython%E7%9F%A5%E8%AF%86%E7%82%B9%EF%BC%8821-30%EF%BC%89%2F</url>
    <content type="text"><![CDATA[super函数 123456789101112131415class T(object): a = 0class A(T): passclass B(T): a = 2class D(T): a = 3class C(A, D, B): passc = C()print(super(C, c).a) # c继承的哪个值是根据MRO顺序来的，按照广度优先，根据ADB的顺序找到第一个定义a的类# 然后就是它，所以super(C,c).a 是3print(c.a) 在循环中获取索引 1234# 在循环中获取索引(数组下标),用enumerateints = [8, 23, 45 ,12, 78]for idx, val in enumerate(ints): print(idx, val) 如何移除换行符?&#39;test string\n&#39;.rstrip() 合并列表中的列表,一共有三种方法，用列表推导式最快 原因：当有L个子串的时候用+(即sum)的时间复杂度是O(L2)–每次迭代的时候作为中间结果的列表的长度就会越来越长,而且前一个中间结果的所有项都会再拷贝一遍给下一个中间结果.所以当你的列表l含有L个字串:l列表的第一项需要拷贝L-1次,而第二项要拷贝L-2次,以此类推;所以总数为I * (L2)/2.列表推导式(list comprehension)只是生成一个列表,每次运行只拷贝一次(从开始的地方拷贝到最终结果). 123456$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' '[item for sublist in l for item in sublist]'10000 loops, best of 3: 143 usec per loop$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'sum(l, [])'1000 loops, best of 3: 969 usec per loop$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'reduce(lambda x,y: x+y,l)'1000 loops, best of 3: 1.1 msec per loop 反转字符串 &#39;hello world&#39;[::-1] 通常每一个实例x都会有一个dict属性，用来记录实例中所有的属性和方法，也是通过这个字典，可以让实例绑定任意的属性。而slots属性作用就是，当类C有比较少的变量，而且拥有slots属性时，类C的实例 就没有dict属性，而是把变量的值存在一个固定的地方。如果试图访问一个slots中没有的属性，实例就会报错。这样操作有什么好处呢？slots属性虽然令实例失去了绑定任意属性的便利，但是因为每一个实例没有dict属性，却能有效节省每一个实例的内存消耗，有利于生成小而精干的实例。 为什么需要这样的设计呢？在一个实际的企业级应用中，当一个类生成上百万个实例时，即使一个实例节省几十个字节都可以节省一大笔内存，这种情况就值得使用slots属性。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python知识点（11-20）]]></title>
    <url>%2F2018%2F03%2F31%2FPython%E7%9F%A5%E8%AF%86%E7%82%B9%EF%BC%8811-20%EF%BC%89%2F</url>
    <content type="text"><![CDATA[new和init的区别 new是一个静态方法,而init是一个实例方法. new方法会返回一个创建的实例,而init什么都不返回. 只有在new返回一个cls的实例时后面的init才能被调用. 当创建一个新实例时调用new,初始化一个实例时用init. metaclass是创建类时起作用.所以我们可以分别使用metaclass,new和init来分别在类创建,实例创建和实例初始化的时候做一些小手脚. 单例模式的四种方式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 使用__new__方法class Singleton(object): @staticmethod def __new__(cls, *args, **kwargs): if not hasattr(cls, '_instance'): # 若还没有实例 orig = super(Singleton, cls) cls._instance = orig.__new__(cls, *args, **kwargs) # 创建一个实例 return cls._instance # 返回一个实例class MyClass(Singleton): a = 1# 共享属性 创建实例时把所有实例的__dict_ #_指向同一个字典,这样它们具有相同的属性和方法.class Borg(object): _state = &#123;&#125; def __new__(cls, *args, **kwargs): ob = super(Borg, cls).__new__(cls, *args, **kwargs) ob.__dict__ = cls._state return obclass MyClass2(Borg): a = 1# 装饰器版本def singleton(cls, *args, **kwargs): instance = &#123;&#125; def getinstance(): if cls not in instance: instance[cls] = cls(*args, **kwargs) return instance[cls] return getinstance@singleton()class MyClass: pass# import方法# mysingleton.pyclass My_Singleton(object): def foo(self): passmy_singleton = My_Singleton()# to usefrom mysingleton import my_singletonmy_singleton.foo() Python中的作用域: 当 Python 遇到一个变量的话他会按照这样的顺序进行搜索：本地作用域（Local）→当前作用域被嵌入的本地作用域（Enclosing locals）→全局/模块作用域（Global）→内置作用域（Built-in） Lambda 表达式:你在某处就真的只需要一个能做一件事情的函数而已，连它叫什么名字都无关紧要。Lambda 表达式就可以用来做这件事。 123456789a=map(lambda x: x*x, [y for y in range(10)])print(list(a))# 这样写不如Lambda，因为有污染环境的函数sqdef sq(x): return x*xa=map(sq, [y for y in range(10)])print(list(a)) 我们习以为常的复制就是深复制，即将被复制对象完全再复制一遍作为独立的新个体单独存在。而浅复制并不会产生一个独立的对象单独存在。 1234567891011121314151617181920212223# 对于普通对象深浅复制一样，而对于下面这样的复杂对象就不同了# 就是列表里嵌套列表import copya = [1, 2, 3, 4, ['a', 'b']]b = a # 赋值，传对象的引用c = copy.copy(a) # 浅拷贝 对于['a','b']只是引用#在浅拷贝中对于子对象，python会把它当作一个公共镜像存储起来，所有对他的复制都被当成一个引用d = copy.deepcopy(a) # 深拷贝print(a is b) # a和b是同一个objectprint(a is c) # 也不是同一个objectprint(a is d) # 可以发现a和d并不是一个objecta.append(5) # 修改对象aa[4].append('c')print(b)print(c)print(d)TrueFalseFalse[1, 2, 3, 4, ['a', 'b', 'c'], 5][1, 2, 3, 4, ['a', 'b', 'c']][1, 2, 3, 4, ['a', 'b']] Python垃圾回收机制Python GC主要使用引用计数（reference counting）来跟踪和回收垃圾。在引用计数的基础上，通过“标记-清除”（mark and sweep）解决容器对象可能产生的循环引用问题，通过“分代回收”（generation collection）以空间换时间的方法提高垃圾回收效率。1 引用计数PyObject是每个对象必有的内容，其中ob_refcnt就是做为引用计数。当一个对象有新的引用时，它的ob_refcnt就会增加，当引用它的对象被删除，它的ob_refcnt就会减少.引用计数为0时，该对象生命就结束了。优点:简单 实时性缺点:维护引用计数消耗资源 循环引用2 标记-清除机制基本思路是先按需分配，等到没有空闲内存的时候从寄存器和程序栈上的引用出发，遍历以对象为节点、以引用为边构成的图，把所有可以访问到的对象打上标记，然后清扫一遍内存空间，把所有没标记的对象释放。3 分代技术分代回收的整体思想是：将系统中的所有内存块根据其存活时间划分为不同的集合，每个集合就成为一个“代”，垃圾收集频率随着“代”的存活时间的增大而减小，存活时间通常利用经过几次垃圾回收来度量。Python默认定义了三代对象集合，索引数越大，对象存活时间越长。举例：当某些内存块M经过了3次垃圾收集的清洗之后还存活时，我们就将内存块M划到一个集合A中去，而新分配的内存都划分到集合B中去。当垃圾收集开始工作时，大多数情况都只对集合B进行垃圾回收，而对集合A进行垃圾回收要隔相当长一段时间后才进行，这就使得垃圾收集机制需要处理的内存少了，效率自然就提高了。在这个过程中，集合B中的某些内存块由于存活时间长而会被转移到集合A中，当然，集合A中实际上也存在一些垃圾，这些垃圾的回收会因为这种分代的机制而被延迟。 read,readline和readlines read 读取整个文件 readline 读取下一行,使用生成器方法 readlines 读取整个文件到一个迭代器以供我们遍历 生成器 1234567891011121314151617181920212223# 所有你可以用在for...in...语句中的都是可迭代的:比如lists,strings,files...# 因为这些可迭代的对象你可以随意的读取所以非常方便易用,# 但是你必须把它们的值放到内存里,当它们有很多值时就会消耗太多的内存.mylist = [x*x for x in range(3)]for i in mylist: print(i)# 生成器也是迭代器的一种,但是你只能迭代它们一次.原因很简单,因为它们不是# 全部存在内存里,它们只在要调用的时候在内存里生成:# 生成器和迭代器的区别就是用()代替[],还有你不能用for i in mygenerator第二次# 调用生成器:首先计算0,然后会在内存里丢掉0去计算1,直到计算完4.mygenerator = (x*x for x in range(3) )for i in mygenerator: print(i)# 当你的函数要返回一个非常大的集合并且你希望只读一次的话,那么用下面的这个就非常的方便了.def createGenerator(): mlist = range(3) for i in mlist: yield i*i # 函数运行并没有碰到yeild语句就认为生成器已经为空了. # 原因有可能是循环结束或者没有满足if/else之类的.mgenerator = createGenerator()for i in mgenerator: print(i) Python中调用外部命令 12345678910# 这是一个简单的对于给定目录下的目录tar的脚本，有坑！注意输入空格这样的sehll字符！！！！#!/usr/bin/env python# a python script to auto backup a directory file by SJTimport osDirectory=raw_input("Please enter directory you want to backup:")dirs=os.listdir(Directory)for filename in dirs: fulldirfile=os.path.join(Directory,filename) if os.path.isdir(fulldirfile): os.system("tar -czvf "+fulldirfile+".tar.gz "+ fulldirfile) 对字典进行排序是不可能的,只有把字典转换成另一种方式才能排序.字典本身是无序的,但是像列表元组等其他类型是有序的.所以你需要用一个元组列表来表示排序的字典. 12345678import operatorx = &#123;1:2, 3:4, 4:3, 2:1, 0:0&#125;sorted_x_by_value = sorted(x.items(), key=operator.itemgetter(1))print(sorted_x_by_value)# 结果为[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]sorted_x_by_key = sorted(x.items(), key=operator.itemgetter(0))print(sorted_x_by_key)# 结果为[(0, 0), (1, 2), (2, 1), (3, 4), (4, 3)]]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python知识点（1-10）]]></title>
    <url>%2F2018%2F03%2F30%2FPython%E7%9F%A5%E8%AF%86%E7%82%B9%EF%BC%881-10%EF%BC%89%2F</url>
    <content type="text"><![CDATA[python知识点 在python中，strings, tuples, 和numbers是不可更改的对象，而list,dict等则是可以修改的对象。当一个引用传递给函数的时候,函数自动复制一份引用,这个函数里的引用和外边的引用没有半毛关系了.所以第一个例子里函数把引用指向了一个不可变对象,当函数返回的时候,外面的引用没半毛感觉.而第二个例子就不一样了,函数内的引用指向的是可变对象,对它的操作就和定位了指针地址一样,在内存里进行修改. 1234567891011121314151617181920212223a = 1b = []c = [] #If you pass an immutable object to a method, you still can't rebind #the outer reference, and you can't even mutate the object.def fun(a): a = 2 #If you pass a mutable object into a method, #the method gets a reference to that same object #and you can mutate itdef fun2(b): b.append(1) #if you rebind the reference in the method, the outer scope #will know nothing about it, and after you're done, #the outer reference will still point at the original object.def fun3(c): c = [1]fun(a)fun2(b)fun3(c)print(a) #1print(b) #[1]print(c) #[] 元类（metaclass），经常用在ORM这种复杂的结构！参考深刻理解Python中的元类(metaclass) 在类里每次定义方法的时候都需要绑定这个实例,就是foo(self, x),为什么要这么做呢?因为实例方法的调用离不开实例,我们需要把实例自己传给函数,调用的时候是这样的a.foo(x)(其实是foo(a, x)).类方法一样,只不过它传递的是类而不是实例,A.class_foo(x).注意这里的self和cls可以替换别的参数,但是python的约定是这俩,还是不要改的好. 1234567891011121314151617181920212223242526272829303132x = 1def foo(x): print("executing foo(%s)" % (x))class A(object): # self和cls是对类或者实例的绑定 def foo(self, x): print("executing foo(%s,%s)" % (self, x)) @classmethod def class_foo(cls, x): print("executing class_foo(%s,%s)" % (cls, x)) @staticmethod def static_foo(x): print("executing static_foo(%s)" % x)a = A()# 普通方法的调用foo(x)# 实例方法的调用a.foo(x)# 类方法的调用a.class_foo(x)A.class_foo(x)# 静态方法的调用a.static_foo(x) A.static_foo(x) # 程序结果为：executing foo(1)executing foo(&lt;__main__.A object at 0x0000000002A5A518&gt;,1)executing class_foo(&lt;class '__main__.A'&gt;,1)executing class_foo(&lt;class '__main__.A'&gt;,1)executing static_foo(1)executing static_foo(1) 类变量和实例变量:类变量就是供类使用的变量,实例变量就是供实例使用的.这里p1.name=”bbb”是实例调用了类变量,这其实和上面第一个问题一样,就是函数传参的问题,p1.name一开始是指向的类变量name=”aaa”,但是在实例的作用域里把类变量的引用改变了,就变成了一个实例变量,self.name不再引用Person的类变量name了. 12345678910 class Person: name="aaa" p1=Person()p2=Person()p1.name="bbb"print p1.name # bbbprint p2.name # aaaprint Person.name # aaa 类变量就是供类使用的变量,实例变量就是供实例使用的. 这里p1.name=”bbb”是实例调用了类变量,这其实和上面第一个问题一样,就是函数传参的问题,p1.name一开始是指向的类变量name=”aaa”,但是在实例的作用域里把类变量的引用改变了,就变成了一个实例变量,self.name不再引用Person的类变量name了. 可以看看下面的例子: 123456789 class Person: name=[] p1=Person()p2=Person()p1.name.append(1)print p1.name # [1]print p2.name # [1]print Person.name # [1] Python自省自省就是面向对象的语言所写的程序在运行时,所能知道对象的类型.简单一句就是运行时能够获得对象的类型.比如type(),dir(),getattr(),hasattr(),isinstance(). python共有三种推导式，那什么是推导式呢？推导式是可以从一个数据序列构建另一个新的数据序列的结构体。 12345678910111213141516171819202122232425262728293031323334353637383940 # 列表推导式multiples = [i for i in range(30) if i % 3 is 0]print(multiples)# Output: [0, 3, 6, 9, 12, 15, 18, 21, 24, 27]def squared(x): return x*xmultiples = [squared(i) for i in range(30) if i % 3 is 0]print multiples# Output: [0, 9, 36, 81, 144, 225, 324, 441, 576, 729]# 将俩表推导式的[]改成()即可得到生成器。multiples = (i for i in range(30) if i % 3 is 0)print(type(multiples))# Output: &lt;type 'generator'&gt; # 字典推导式 # 字典推导和列表推导的使用方法是类似的，只不中括号该改成大括号。直接举例说明： # 通过把key大小写合并 合并值mcase = &#123;'a': 10, 'b': 34, 'A': 7, 'Z': 3&#125;mcase_frequency = &#123; k.lower(): mcase.get(k.lower(), 0) + mcase.get(k.upper(), 0) for k in mcase.keys() if k.lower() in ['a','b']&#125;print(mcase_frequency)# Output: &#123;'a': 17, 'b': 34&#125; # 快速更换key和valuemcase = &#123;'a': 10, 'b': 34, 'c': 30&#125;mcase_frequency = &#123;v: k for k, v in mcase.items()&#125;print(mcase_frequency)# Output: &#123;10: 'a', 34: 'b'&#125; # 集合推导式squared = &#123;x**2 for x in [1, 1, 2]&#125;print(squared)# Output: set([1, 4]) 单下划线和双下划线首先是单下划线开头，这个被常用于模块中，在一个模块中以单下划线开头的变量和函数被默认当作内部函数，如果使用 from a_module import * 导入时，这部分变量和函数不会被导入。不过值得注意的是，如果使用 import a_module 这样导入模块，仍然可以用 a_module._some_var 这样的形式访问到这样的对象。在 Python 的官方推荐的代码样式中，还有一种单下划线结尾的样式，这在解析时并没有特别的含义，但通常用于和 Python 关键词区分开来，比如如果我们需要一个变量叫做 class，但 class 是 Python 的关键词，就可以以单下划线结尾写作 class_。双下划线开头的命名形式在 Python 的类成员中使用表示名字改编 (Name Mangling)，即如果有一 Test 类里有一成员 x，那么 dir(Test) 时会看到 _Testx 而非 x。这是为了避免该成员的名称与子类中的名称冲突。但要注意这要求该名称末尾没有下划线。双下划线开头双下划线结尾的是一些 Python 的“魔术”对象，如类成员的 init__、del、add、getitem 等，以及全局的 file、name 等。 Python 官方推荐永远不要将这样的命名方式应用于自己的变量或函数，而是按照文档说明来使用。另外单下划线开头还有一种一般不会用到的情况在于使用一个 C 编写的扩展库有时会用下划线开头命名，然后使用一个去掉下划线的 Python 模块进行包装。如 struct 这个模块实际上是 C 模块 _struct 的一个 Python 包装。 12345678910111213141516171819class MyClass(): def __init__(self): self.__superprivate = "Hello" self._semiprivate = ",World！"class OtherClass(MyClass): def __init__(self): self.__superprivate = "hhaaa" self.b = "ssss"mc = MyClass()oc = OtherClass()print(mc._semiprivate) # 只有mc能访问该私有变量print(mc.__dict__)print(oc.__dict__)# print(oc.__superprivate) # 访问不了，提示没有该属性print(oc._OtherClass__superprivate) # 这样就可以访问# ,World！#&#123;'_MyClass__superprivate': 'Hello', #'_semiprivate': ',World！'&#125;#&#123;'b': 'ssss', '_OtherClass__superprivate': #'hhaaa'&#125;#hhaaa 当你不确定你的函数里将要传递多少参数时你可以用*args **kwargs允许你使用没有事先定义的参数名 面向切面编程AOP和装饰器装饰器是一个很著名的设计模式，经常被用于有切面需求的场景，较为经典的有插入日志、性能测试、事务处理等。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。 12345678910# 字体变粗装饰器def makebold(fn): # 装饰器将返回新的函数 def warpper(): return "&lt;b&gt;"+ fn() + "&lt;/b&gt;" return warpper@makebolddef say(): return "hello"print(say()) # &lt;b&gt;hello&lt;/b&gt; 函数重载主要是为了解决两个问题。可变参数类型。可变参数个数。一个基本的设计原则是，仅仅当两个函数除了参数类型和参数个数不同以外，其功能是完全相同的，此时才使用函数重载，如果两个函数的功能其实不同，那么不应当使用重载，而应当使用一个名字不同的函数。但是因为python可以接受任何类型的参数、和任何数量的参数，所以不需要重载。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的eval函数]]></title>
    <url>%2F2018%2F03%2F29%2Fpython%E4%B8%AD%E7%9A%84eval%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[python中的eval() 这个函数意义在哪？在编译语言里要动态地产生代码，基本上是不可能的，但动态语言是可以，意味着软件已经部署到服务器上了，但只要作很少的更改，只好直接修改这部分的代码，就可立即实现变化，不用整个软件重新加载。 先举个栗子：123a = 1g = &#123;&apos;a&apos;: 20&#125;print(eval(&quot;a+1&quot;, g)) #返回21 再举个栗子1234567891011121314151617# test eval() and locals()x = 1y = 1num1 = eval(&quot;x+y&quot;)print(num1)def g():x = 2y = 2num3 = eval(&quot;x+y&quot;)print(num3)num2 = eval(&quot;x+y&quot;, globals()) # 搜全局num4 = eval(&quot;x+y&quot;,globals(),locals()) # 先搜索局部，搜索到停止print(num2)print(num4)g() # 值依次为2 4 2 4 eval在字符串对象和list、dictinoary、tuple对象之间互相转换123456789def evala(): l = &apos;[1,2,3,4,[5,6,7,8,9]]&apos; d = &quot;&#123;&apos;a&apos;:123,&apos;b&apos;:456,&apos;c&apos;:789&#125;&quot; t = &apos;([1,3,5],[5,6,7,8,9],[123,456,789])&apos; print(type(l), type(eval(l))) print(type(d), type(eval(d))) print(type(t), type(eval(t)))evala() 结果为：123&lt;class &apos;str&apos;&gt; &lt;class &apos;list&apos;&gt; &lt;class &apos;str&apos;&gt; &lt;class &apos;dict&apos;&gt; &lt;class &apos;str&apos;&gt; &lt;class &apos;tuple&apos;&gt; locals()对象的值不能修改，globals()对象的值可以修改 1234567891011#test globals() and locals()z=0def f(): z = 1 print (locals()) locals()[&quot;z&quot;] = 2 print (locals()) f() globals()[&quot;z&quot;] = 2print (z) # 结果为&#123;&apos;z&apos;: 1&#125; &#123;&apos;z&apos;: 1&#125; 2 eval有安全性问题,比如用户恶意输入就会获得当前目录文件eval(&quot;__import__(&#39;os&#39;).system(&#39;dir&#39;)&quot;) 怎么避免安全问题？ 自行写检查函数； 使用ast.literal_eval tipsif __name__ == &quot;__main__&quot;: 第一开始不是很理解，今天经过学习知道了这句代码的作用。是编写私有化部分 ，这句代码以上的部分，可以被其它的调用，以下的部分只有这个文件自己可以看见，如果文件被调用了，其他人是无法看见私有化部分的。比如进行单元测试的时候会用到！他的原理是每个py文件都有name属性，如果当前属性和main一样，则值为真，否则为假，就不执行！]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多线程]]></title>
    <url>%2F2018%2F03%2F29%2Fpython%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[python多线程（假多线程） 如果你的代码是IO密集型，多线程可以明显提高效率。例如制作爬虫（我就不明白为什么Python总和爬虫联系在一起…不过也只想起来这个例子…），绝大多数时间爬虫是在等待socket返回数据。这个时候C代码里是有release GIL的，最终结果是某个线程等待IO的时候其他线程可以继续执行。如果你的代码是CPU密集型，多个线程的代码很有可能是线性执行的。所以这种情况下多线程是鸡肋，效率可能还不如单线程因为有context switch 一般网络传输类应用都是IO密集型，读写硬盘操作多也算是IO密集型 我们都知道，比方我有一个4核的CPU，那么这样一来，在单位时间内每个核只能跑一个线程，然后时间片轮转切换。但是Python不一样，它不管你有几个核，单位时间多个核只能跑一个线程，然后时间片轮转。看起来很不可思议？但是这就是GIL搞的鬼。任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。（Java中的多线程是可以利用多核的，这是真正的多线程！） 在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192 # 直接调用函数的多线程from threading import Threadimport timedef loop(name, seconds): print('start loop', name, 'at:', time.ctime()) time.sleep(1) print('end loop', name, 'at:', time.ctime())if __name__ == '__main__': loops = [2, 4] nloops = range(len(loops)) threads = [] print('start at:', time.ctime()) for i in nloops: t = Thread(target=loop, args=(i, loops[i],)) threads.append(t) for i in nloops: threads[i].start() for i in nloops: threads[i].join() print('all done at:', time.ctime()) # 使用可调用的类对象from threading import Threadimport timedef loop(name, seconds): print('start loop', name, 'at:', time.ctime()) time.sleep(1) print('end loop', name, 'at:', time.ctime())class ThreadFunc(object): def __init__(self, func, args, name=''): self.name = name self.func = func self.args = args def __call__(self): # 需要定义一个特殊的方法__call__ self.func(*self.args)if __name__ == '__main__': loops = [2, 4] nloops = range(len(loops)) threads = [] print('start at:', time.ctime()) for i in nloops: t = Thread(target=ThreadFunc(loop, (i, loops[i]), loop.__name__)) threads.append(t) for i in nloops: threads[i].start() for i in nloops: threads[i].join() print('all done at:', time.ctime())# 继承Thread类，也就是子类化from threading import Threadimport timedef loop(name, seconds): print('start loop', name, 'at:', time.ctime()) time.sleep(1) print('end loop', name, 'at:', time.ctime())class ThreadFunc(Thread): def __init__(self, func, args, name=''): super(ThreadFunc, self).__init__() self.name = name self.func = func self.args = args # 将上种方式中可调用的方法改为run方法，其实就是对Thread类中run方法的重写 def run(self): self.func(*self.args)if __name__ == '__main__': loops = [2, 4] nloops = range(len(loops)) threads = [] print('start at:', time.ctime()) for i in nloops: t = ThreadFunc(loop, (i, loops[i]), loop.__name__) threads.append(t) for i in nloops: threads[i].start() for i in nloops: threads[i].join() print('all done at:', time.ctime()) # 在一般推荐的方法中，我们用最后一种方式 线程同步如果多个线程共同对某个数据修改，则可能出现不可预料的结果，为了保证数据的正确性，需要对多个线程进行同步。使用Thread对象的Lock和Rlock可以实现简单的线程同步，这两个对象都有acquire方法和release方法，对于那些需要每次只允许一个线程操作的数据，可以将其操作放到acquire和release方法之间。如下：多线程的优势在于可以同时运行多个任务（至少感觉起来是这样）。但是当线程需要共享数据时，可能存在数据不同步的问题。考虑这样一种情况：一个列表里所有元素都是0，线程”set”从后向前把所有元素改成1，而线程”print”负责从前往后读取列表并打印。那么，可能线程”set”开始改的时候，线程”print”便来打印列表了，输出就成了一半0一半1，这就是数据的不同步。为了避免这种情况，引入了锁的概念。锁有两种状态——锁定和未锁定。每当一个线程比如”set”要访问共享数据时，必须先获得锁定；如果已经有别的线程比如”print”获得锁定了，那么就让线程”set”暂停，也就是同步阻塞；等到线程”print”访问完毕，释放锁以后，再让线程”set”继续。经过这样的处理，打印列表时要么全部输出0，要么全部输出1，不会再出现一半0一半1的尴尬场面。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import threadingimport timeclass myThread(threading.Thread): def __init__(self, threadID, name, counter): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter def run(self): print("Starting " + self.name) # 获得锁，成功获得锁定后返回True # 可选的timeout参数不填时将一直阻塞直到获得锁定 # 否则超时后将返回False threadLock.acquire() print_time(self.name, self.counter, 3) # 释放锁 threadLock.release()def print_time(threadName, delay, counter): while counter: time.sleep(delay) print("%s: %s" % (threadName, time.ctime(time.time()))) counter -= 1threadLock = threading.Lock()threads = []# 创建新线程thread1 = myThread(1, "Thread-1", 1)thread2 = myThread(2, "Thread-2", 2)# 开启新线程thread1.start()thread2.start()# 添加线程到线程列表threads.append(thread1)threads.append(thread2)# 等待所有线程完成for t in threads: t.join()print("Exiting Main Thread")]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[僵尸进程]]></title>
    <url>%2F2018%2F03%2F28%2F%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[僵尸进程 怎样产生僵尸进程的： 一个进程在调用exit命令结束自己的生命的时候，其实它并没有真正的被销毁，而是留下一个称为僵尸进程（Zombie）的数据结构（系统调用exit，它的作用是使进程退出，但也仅仅限于将一个正常的进程变成一个僵尸进程，并不能将其完全销毁）。在Linux进程的状态中，僵尸进程是非常特殊的一种，它已经放弃了几乎所有内存空间，没有任何可执行代码，也不能被调度，仅仅在进程列表中保留一个位置，记载该进程的退出状态等信息供其他进程收集，除此之外，僵尸进程不再占有任何内存空间。它需要它的父进程来为它收尸，如果他的父进程没安装SIGCHLD信号处理函数调用wait或waitpid()等待子进程结束，又没有显式忽略该信号，那么它就一直保持僵尸状态，如果这时父进程结束了，那么init进程自动会接手这个子进程，为它收尸，它还是能被清除的。但是如果如果父进程是一个循环，不会结束，那么子进程就会一直保持僵尸状态，这就是为什么系统中有时会有很多的僵尸进程。 网络原因有时也会引起僵尸进程， 僵尸进程有害吗？ 不会。由于僵尸进程并不做任何事情， 不会使用任何资源也不会影响其它进程， 因此存在僵尸进程也没什么坏处。 不过由于进程表中的退出状态以及其它一些进程信息也是存储在内存中的，因此存在太多僵尸进程有时也会是一些问题，比如会影响服务器的性能。signal(SIGCHLD, SIG_IGN); 忽略SIGCHLD信号，这是一个常用于提升并发服务器性能的技巧,因为并发服务器常常fork很多子进程，子进程终结之后需要服务器进程去wait清理资源。如果将此信号的处理方式设置为忽略，可让内核把僵尸进程转交给init进程去处理，省去了大量僵尸进程占用系统资源。 如何防止僵尸进程？ 让僵尸进程成为孤儿进程，由init进程回收；(手动杀死父进程) 调用fork()两次； 捕捉SIGCHLD信号，并在信号处理函数中调用wait函数；代码实例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869 #include &lt;signal.h&gt; #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;sys/types.h&gt; #include &lt;sys/wait.h&gt; #include &lt;unistd.h&gt; void sig_handler(int signo) &#123; printf(&quot;child process deaded, signo: %d\n&quot;, signo); wait(0);// 当捕获到SIGCHLD信号，父进程调用wait回收，避免子进程成为僵尸进程 &#125; void out(int n) &#123; int i; for(i = 0; i &lt; n; ++i) &#123; printf(&quot;%d out %d\n&quot;, getpid(), i); sleep(2); &#125; &#125; int main(void) &#123; // 登记一下SIGCHLD信号 if(signal(SIGCHLD, sig_handler) == SIG_ERR) &#123; perror(&quot;signal sigchld error&quot;); &#125; pid_t pid = fork(); if(pid &lt; 0) &#123; perror(&quot;fork error&quot;); exit(1); &#125; else if(pid &gt; 0) &#123; // parent process out(100); &#125; else &#123; // child process out(10); &#125; return 0; &#125; ``` 4. **如何找出僵尸进程呢？** `ps aux | grep Z`or `ps -ef | grep defunct`5. **解决方法：*** 设置SIGCLD信号为SIG_IGN，系统将不产生僵死进程。* 用两次fork()，而且使紧跟的子进程直接退出，是的孙子进程成为孤儿进程，从而init进程将负责清除这个孤儿进程。* 重启服务器电脑，这个是最简单，最易用的方法，但是如果你服务器电脑上运行有其他的程序，那么这个方法，代价很大。所以，尽量使用下面一种方法。* 找到该defunct僵尸进程的父进程，将该进程的父进程杀掉，则此defunct进程将自动消失。 如何找到defunct僵尸进程的父进程？很简单，一句命令就够了：`ps -ef | grep defunct_process_pid`* 正常情况下我们可以用 SIGKILL 信号来杀死进程，但是僵尸进程已经死了， 你不能杀死已经死掉的东西。 因此你需要输入的命令应该是`kill -s SIGCHLD pid` 。 将这里的 pid 替换成父进程的进程 id，这样父进程就会删除所有以及完成并死掉的子进程了。### 在服务器上发现了僵尸```shellroot@ubuntu16-desktop:~# ps aux | grep ZUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1354 0.0 0.0 14228 944 pts/8 S+ 06:13 0:00 grep --color=auto Zroot 3572 0.0 0.0 0 0 ? Z Mar05 0:00 [create-topics.s] &lt;defunct&gt;root 4120 0.0 0.0 0 0 ? Z Mar05 0:00 [create-topics.s] &lt;defunct&gt;root 4696 0.0 0.0 0 0 ? Z Mar05 0:00 [create-topics.s] &lt;defunct&gt; 这是不是我起了kafka的docker容器之后才出现的，出现原因我现在还不知道，待补充,pid竟然还会变？？ 我试了下，既然处理僵尸进程的方法是把父进程关掉，就行了，我一看这三个僵尸进程的父进程就是三个kafka容器，那么我docker-compose down关掉就ok了，试了试，果然这三个僵尸进程没有了，重启之后，发现还是有这三个僵尸进程。。 经过测试发现，这是这个docker镜像本身的问题，所以我像官方提了issue。https://github.com/wurstmeister/kafka-docker/issues/497,大意是说因为create-topic这个脚本放在了后台运行，能不能用disown这个命令处理，有待进一步实验。 那么这样又引来另外一个问题，如果不能关闭父进程怎么办？ 实现一个内核线程，专门实现模块init函数的逻辑，需要干掉的僵尸进程号通过procfs传入内核，然后在write例程中唤醒回收僵尸进程的内核线程； 实现一个用户态进程U，挂载一个信号A的处理函数，内部实现waitpid，通过procfs传入或者通过netlink传入内核的僵尸进程号代表的进程过继给用户态进程U，然后向U发送信号A； /dev/mem的机器码编程或者直接释放僵尸进程的task_t。 在/proc//目录中加入kill-if-jiangshi文件，写入1如果该进程是僵尸，那么就调用上述模块的逻辑杀死它 参考https://blog.csdn.net/dog250/article/details/6451989]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[哈希表]]></title>
    <url>%2F2018%2F03%2F28%2F%E5%93%88%E5%B8%8C%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[哈希表 哈希表的特点：关键字在表中位置和它之间存在一种确定的关系。hash : 翻译为“散列”，就是把任意长度的输入，通过散列算法，变成固定长度的输出，该输出就是散列值。不同的输入可能会散列成相同的输出，所以不可能从散列值来唯一的确定输入值。 实际工作中需视不同的情况采用不同的哈希函数，通常考虑的因素有： 计算哈希函数所需时间 关键字的长度 哈希表的大小 关键字的分布情况 记录的查找频率 由于哈希表高效的特性，查找或者插入的情况在大多数情况下可以达到O(1)，时间主要花在计算hash上，当然也有最坏的情况就是hash值全都映射到同一个地址上，这样哈希表就会退化成链表，查找的时间复杂度变成O(n)，但是这种情况比较少，只要不要把hash计算的公式外漏出去并且有人故意攻击（用兴趣的人可以搜一下基于哈希冲突的拒绝服务攻击），一般也不会出现这种情况。 下面这个程序是用C++实现的一个包含hash算法的小程序，可以让电话或者姓名作为key，从而查找记录。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526 // MyHashTable.cpp : 定义控制台应用程序的入口点。////设计哈希表实现电话号码查询系统//说明：一是从文件old.txt中读取的数据自己在程序运行前建立，// 二是由系统随机生成数据，在程序运行由随机数产生器生成，并且将产生的记录保存到 new.txt文件。//存在的问题：使用随机产生的文件，在显示时出现乱码// #include &lt;stdafx.h&gt; #include&lt;fstream&gt;//文件流#include&lt;iostream&gt;#include &lt;string&gt;#include&lt;stdlib.h&gt;using namespace std;const int D[] = &#123;3,5,8,11,13,14,19,21&#125;;//预定再随机数const int HASH_MAXSIZE = 50;//哈希表长度//记录信息类型class DataInfo&#123;public: DataInfo();//默认构造函数 friend ostream&amp; operator&lt;&lt;(ostream&amp; out, const DataInfo&amp; dataInfo); //重载输出操作符 //friend class HashTable;//private: string name;//姓名 string phone;//电话号码 string address;//地址 char sign;//冲突的标志位，&apos;1&apos;表示冲突，&apos;0&apos;表示无冲突&#125;;DataInfo::DataInfo():name(&quot;&quot;), phone(&quot;&quot;), address(&quot;&quot;), sign(&apos;0&apos;)&#123;&#125;ostream&amp; operator&lt;&lt;(ostream&amp; out, const DataInfo&amp; dataInfo) //重载输出操作符&#123; cout &lt;&lt; &quot;姓名：&quot; &lt;&lt; dataInfo.name &lt;&lt; &quot; 电话：&quot; &lt;&lt; dataInfo.phone &lt;&lt; &quot; 地址：&quot; &lt;&lt; dataInfo.address &lt;&lt; endl; return out;&#125;//存放记录的哈希表类型class HashTable&#123;public: HashTable();//默认构造函数 ~HashTable();//析构函数 int Random(int key, int i);// 伪随机数探测再散列法处理冲突 void Hashname(DataInfo *dataInfo);//以名字为关键字建立哈希表 int Rehash(int key, string str);// 再哈希法处理冲突 注意处理冲突还有链地址法等 void Hashphone(DataInfo *dataInfo);//以电话为关键字建立哈希表 void Hash(char *fname, int n);// 建立哈希表 //fname 是数据储存的文件的名称，用于输入数据，n是用户选择的查找方式 int Findname(string name);// 根据姓名查找哈希表中的记录对应的关键码 int Findphone(string phone);// 根据电话查找哈希表中的记录对应的关键码 void Outhash(int key);// 输出哈希表中关键字码对应的一条记录 void Outfile(string name, int key);// 在没有找到时输出未找到的记录 void Rafile();// 随机生成文件，并将文件保存在 new.txt文档中 void WriteToOldTxt();//在运行前先写入数据 //private: DataInfo *value[HASH_MAXSIZE]; int length;//哈希表长度&#125;;HashTable::HashTable():length(0)//默认构造函数&#123; //memset(value, NULL, HASH_MAXSIZE*sizeof(DataInfo*)); for (int i=0; i&lt;HASH_MAXSIZE; i++) &#123; value[i] = new DataInfo(); &#125;&#125;HashTable::~HashTable()//析构函数&#123; delete[] *value;&#125;void HashTable::WriteToOldTxt()&#123; ofstream openfile(&quot;old.txt&quot;); if (openfile.fail()) &#123; cout &lt;&lt; &quot;文件打开错误！&quot; &lt;&lt; endl; exit(1); &#125; string oldname; string oldphone; string oldaddress; for (int i=0; i&lt;30; i++) &#123; cout &lt;&lt; &quot;请输入第&quot; &lt;&lt; i+1 &lt;&lt; &quot;条记录:&quot; &lt;&lt; endl; cin &gt;&gt; oldname ; cin &gt;&gt; oldphone; cin &gt;&gt; oldaddress; openfile &lt;&lt; oldname &lt;&lt; &quot; &quot; &lt;&lt; oldphone &lt;&lt; &quot; &quot; &lt;&lt; oldaddress &lt;&lt; &quot;,&quot; &lt;&lt; endl; &#125; openfile.close();&#125;int HashTable::Random(int key, int i)// 伪随机数探测再散列法处理冲突&#123;//key是冲突时的哈希表关键码，i是冲突的次数，N是哈希表长度 //成功处理冲突返回新的关键码，未进行冲突处理则返回-1 int h; if(value[key]-&gt;sign == &apos;1&apos;)//有冲突 &#123; h = (key + D[i]) % HASH_MAXSIZE; return h; &#125; return -1;&#125;void HashTable::Hashname(DataInfo *dataInfo)//以名字为关键字建立哈希表&#123;//利用除留取余法建立以名字为关键字建立的哈希函数，在发生冲突时调用Random函数处理冲突 int i = 0; int key = 0; for (int t=0; dataInfo-&gt;name[t]!=&apos;\0&apos;; t++) &#123; key = key + dataInfo-&gt;name[t]; &#125; key = key % 42; while(value[key]-&gt;sign == &apos;1&apos;)//有冲突 &#123; key = Random(key, i++);//处理冲突 &#125; if(key == -1) exit(1);//无冲突 length++;//当前数据个数加 value[key]-&gt;name = dataInfo-&gt;name; value[key]-&gt;address = dataInfo-&gt;address; value[key]-&gt;phone = dataInfo-&gt;phone; value[key]-&gt;sign = &apos;1&apos;;//表示该位置有值 //cout &lt;&lt; value[key]-&gt;name &lt;&lt; &quot; &quot; &lt;&lt; value[key]-&gt;phone &lt;&lt; &quot; &quot; &lt;&lt; value[key]-&gt;address &lt;&lt; endl;&#125;int HashTable::Rehash(int key, string str)// 再哈希法处理冲突&#123;//再哈希时使用的是折叠法建立哈希函数 int h; int num1 = (str[0] - &apos;0&apos;) * 1000 + (str[1] - &apos;0&apos;) * 100 + (str[2] - &apos;0&apos;) * 10 + (str[3] - &apos;0&apos;); int num2 = (str[4] - &apos;0&apos;) * 1000 + (str[5] - &apos;0&apos;) * 100 + (str[6] - &apos;0&apos;) * 10 + (str[7] - &apos;0&apos;); int num3 = (str[8] - &apos;0&apos;) * 100 + (str[9] - &apos;0&apos;) * 10 + (str[10] - &apos;0&apos;); h = num1 + num2 + num3; h = (h + key) % HASH_MAXSIZE; return h;&#125;void HashTable::Hashphone(DataInfo *dataInfo)//以电话为关键字建立哈希表&#123;//利用除留取余法建立以电话为关键字建立的哈希函数，在发生冲突时调用Rehash函数处理冲突 int key = 0; int t; for(t=0; dataInfo-&gt;phone[t] != &apos;\0&apos;; t++) &#123; key = key + dataInfo-&gt;phone[t]; &#125; key = key % 42; while(value[key]-&gt;sign == &apos;1&apos;)//有冲突 &#123; key = Rehash(key, dataInfo-&gt;phone); &#125; length++;//当前数据个数加 value[key]-&gt;name = dataInfo-&gt;name; value[key]-&gt;address = dataInfo-&gt;address; value[key]-&gt;phone = dataInfo-&gt;phone; value[key]-&gt;sign = &apos;1&apos;;//表示该位置有值 &#125;void HashTable::Outfile(string name, int key)//在没有找到时输出未找到的记录&#123; ofstream fout; if((key == -1)||(value[key]-&gt;sign == &apos;0&apos;))//判断哈希表中没有记录 &#123; fout.open(&quot;out.txt&quot;,ios::app);//打开文件 if(fout.fail()) &#123; cout &lt;&lt; &quot;文件打开失败!&quot; &lt;&lt; endl; exit(1); &#125; fout &lt;&lt; name &lt;&lt; endl;//将名字写入文件,有个问题，每次写入的时候总是将原来的内容替换了 fout.close(); &#125;&#125;void HashTable::Outhash(int key)//输出哈希表中关键字码对应的记录&#123; if((key==-1)||(value[key]-&gt;sign==&apos;0&apos;)) cout &lt;&lt; &quot;没有找到这条记录！&quot; &lt;&lt; endl; else &#123; for(unsigned int i=0; value[key]-&gt;name[i]!=&apos;\0&apos;; i++) &#123; cout &lt;&lt; value[key]-&gt;name[i]; &#125; for(unsigned int i=0; i&lt;10; i++) &#123; cout &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; value[key]-&gt;phone; for(int i=0; i&lt;10; i++) &#123; cout &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; value[key]-&gt;address &lt;&lt; endl; &#125;&#125;void HashTable::Rafile()//随机生成文件，并将文件保存在new.txt文档中&#123; ofstream fout; fout.open(&quot;new.txt&quot;);//打开文件，等待写入 if(fout.fail()) &#123; cout &lt;&lt; &quot;文件打开失败！&quot; &lt;&lt; endl; exit(1); &#125; for(int j=0; j&lt;30; j++) &#123; string name = &quot;&quot;; for(int i=0; i&lt;20; i++)//随机生成长个字的名字 &#123; name += rand() % 26 + &apos;a&apos;;//名字是由个字母组成 &#125; fout &lt;&lt; name &lt;&lt; &quot; &quot;;//将名字写入文件 string phone = &quot;&quot;; for(int i=0; i&lt;11; i++)//随机生成长位的电话号码 &#123; phone += rand() % 10 + &apos;0&apos;;//电话号码是纯数字 &#125; fout &lt;&lt; phone &lt;&lt; &quot; &quot;;//将电话号码写入文件 string address = &quot;&quot;; for(int i=0; i&lt;29; i++)//随机生成长个字的名字 &#123; address += rand() % 26 + &apos;a&apos;;//地址是由个字母组成 &#125; address += &apos;,&apos;; fout &lt;&lt; address &lt;&lt; endl;//将地址写入文件 &#125; fout.close();&#125;void HashTable::Hash(char *fname, int n)//建立哈希表//fname是数据储存的文件的名称，用于输入数据，n是用户选择的查找方式//函数输入数据，并根据选择调用Hashname或Hashphone函数进行哈希表的建立&#123; ifstream fin; int i; fin.open(fname);//读文件流对象 if(fin.fail()) &#123; cout &lt;&lt; &quot;文件打开失败！&quot; &lt;&lt; endl; exit(1); &#125; while(!fin.eof())//按行读入数据 &#123; DataInfo *dataInfo = new DataInfo(); char* str = new char[100]; fin.getline(str, 100, &apos;\n&apos;);//读取一行数据 if(str[0] == &apos;*&apos;)//判断数据结束 &#123; break; &#125; i = 0;//记录字符串数组的下标 //a-z:97-122 A-Z:65-90 //本程序的姓名和地址都使用小写字母 while((str[i] &lt; 97) || (str[i] &gt; 122))//读入名字 &#123; i++; &#125; for(; str[i]!=&apos; &apos;; i++) &#123; dataInfo-&gt;name += str[i]; &#125; while(str[i] == &apos; &apos;) &#123; i++; &#125; for(int j=0; str[i]!=&apos; &apos;; j++,i++)//读入电话号码 &#123; dataInfo-&gt;phone += str[i]; &#125; while(str[i] == &apos; &apos;) &#123; i++; &#125; for(int j=0; str[i]!=&apos;,&apos;; j++,i++)//读入地址 &#123; dataInfo-&gt;address += str[i]; &#125; if(n == 1) &#123; Hashname(dataInfo); &#125; else &#123; Hashphone(dataInfo);//以电话为关键字 &#125; delete []str; delete dataInfo; &#125; fin.close();&#125;int HashTable::Findname(string name)//根据姓名查找哈希表中的记录对应的关键码&#123; int i = 0; int j = 1; int t; int key = 0; for(key=0, t=0; name[t] != &apos;\0&apos;; t++) &#123; key = key + name[t]; &#125; key = key % 42; while((value[key]-&gt;sign == &apos;1&apos;) &amp;&amp; (value[key]-&gt;name != name)) &#123; key = Random(key, i++); j++; if(j &gt;= length) return -1; &#125; return key;&#125;int HashTable::Findphone(string phone)//根据电话查找哈希表中的记录对应的关键码&#123; int key = 0; int t; for(t=0; phone[t] != &apos;\0&apos; ; t++) &#123; key = key + phone[t]; &#125; key = key % 42; int j = 1; while((value[key]-&gt;sign == &apos;1&apos;) &amp;&amp; (value[key]-&gt;phone != phone)) &#123; key = Rehash(key, phone); j++; if(j &gt;= length) &#123; return -1; &#125; &#125; return key;&#125;int main()&#123; //WriteToOldTxt(); int k; int ch; char *Fname; HashTable *ht = new HashTable; while(1) &#123; system(&quot;cls&quot;);//cls命令清除屏幕上所有的文字 cout &lt;&lt; &quot;欢迎使用本系统！&quot; &lt;&lt; endl &lt;&lt; endl; cout &lt;&lt; &quot;请选择数据&quot; &lt;&lt; endl; cout &lt;&lt; &quot;1.使用已有数据文件&quot; &lt;&lt; endl; cout &lt;&lt; &quot;2.随机生成数据文件&quot; &lt;&lt; endl; cout &lt;&lt; &quot;0.结束&quot; &lt;&lt; endl; cout &lt;&lt; &quot;输入相应序号选择功能：&quot;; cin &gt;&gt; k; switch(k) &#123; case 0: return 0; case 1: Fname = &quot;old.txt&quot;;//从数据文件old.txt(自己现行建好)中读入各项记录 break; case 2: ht-&gt;Rafile(); Fname = &quot;new.txt&quot;;//由系统随机产生各记录，并且把记录保存到new.txt文件中 break; default: cout &lt;&lt; &quot;输入序号有误，退出程序。&quot; &lt;&lt; endl; return 0; &#125; do &#123; system(&quot;cls&quot;); cout &lt;&lt; &quot; 请选择查找方式&quot; &lt;&lt; endl; cout &lt;&lt; &quot;1.通过姓名查找&quot; &lt;&lt; endl; cout &lt;&lt; &quot;2.通过电话查找&quot; &lt;&lt; endl; cout &lt;&lt; &quot;输入相应序号选择功能：&quot;; cin &gt;&gt; ch; if((ch != 1) &amp;&amp; (ch != 2)) cout &lt;&lt; &quot;输入序号有误！&quot; &lt;&lt; endl; &#125;while((ch != 1) &amp;&amp; (ch != 2)); ht-&gt;Hash(Fname, ch); while(ch == 1) &#123; int choice; cout &lt;&lt; endl &lt;&lt; &quot;请选择功能&quot; &lt;&lt; endl; cout &lt;&lt; &quot;1.输入姓名查找数据&quot; &lt;&lt; endl; cout &lt;&lt; &quot;2.显示哈希表&quot; &lt;&lt; endl; cout &lt;&lt; &quot;0.退出&quot;&lt;&lt;endl; cout &lt;&lt; &quot;输入相应序号选择功能：&quot;; cin &gt;&gt; choice; switch(choice) &#123; case 1: &#123;//注意此处应该加上大括号 int key1; string name; cout &lt;&lt; &quot;请输入姓名：&quot;; cin &gt;&gt; name; key1 = ht-&gt;Findname(name); ht-&gt;Outfile(name, key1); ht-&gt;Outhash(key1); &#125; break; case 2: &#123; for(int i=0; i&lt;HASH_MAXSIZE; i++) &#123; if(ht-&gt;value[i]-&gt;sign!=&apos;0&apos;) &#123; ht-&gt;Outhash(i); &#125; &#125; &#125; break; default: cout &lt;&lt; endl &lt;&lt; &quot;您的输入有误！&quot; &lt;&lt; endl; &#125; if(choice == 0) &#123; return 0; &#125; &#125; while(ch == 2) &#123; int choice; cout &lt;&lt; endl &lt;&lt; &quot;请选择功能&quot; &lt;&lt; endl; cout &lt;&lt; &quot;1.输入电话查找数据&quot; &lt;&lt; endl; cout &lt;&lt; &quot;2.显示哈希表&quot;&lt;&lt;endl; cout &lt;&lt; &quot;0.退出&quot;&lt;&lt;endl; cout &lt;&lt; &quot;输入相应序号选择功能：&quot;; cin &gt;&gt; choice; switch(choice) &#123; case 1: &#123; int key2; string phone; cout &lt;&lt; &quot;请输入11位的电话号码：&quot;; do &#123; cin &gt;&gt; phone; if(phone.length() != 11) &#123; cout &lt;&lt; &quot;电话号码应为11位！\n请重新输入：&quot;; &#125; &#125;while(phone.length() != 11); key2 = ht-&gt;Findphone(phone); ht-&gt;Outfile(phone, key2); ht-&gt;Outhash(key2); &#125; break; case 2: &#123; for(int i=0; i&lt;HASH_MAXSIZE; i++) &#123; if(ht-&gt;value[i]-&gt;sign != &apos;0&apos;) &#123; ht-&gt;Outhash(i); &#125; &#125; &#125; break; default: cout &lt;&lt; endl &lt;&lt; &quot;您的输入有误！&quot; &lt;&lt; endl; &#125; if(choice == 0) &#123; return 0; &#125; &#125; while((ch != 1) &amp;&amp; (ch != 2)) &#123; cout &lt;&lt; &quot;您的输入有误！请输入相应需要选择功能：&quot;; &#125; &#125; system(&quot;pause&quot;); return 0;&#125;]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>DataStructure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种树]]></title>
    <url>%2F2018%2F03%2F27%2F%E5%90%84%E7%A7%8D%E6%A0%91%2F</url>
    <content type="text"><![CDATA[红黑树，AVL树简单来说都是用来搜索的呗。 AVL树：平衡二叉树，一般是用平衡因子差值决定并通过旋转来实现，左右子树树高差不超过1，那么和红黑树比较它是严格的平衡二叉树，平衡条件非常严格（树高差只有1），只要插入或删除不满足上面的条件就要通过旋转来保持平衡。由于旋转是非常耗费时间的。我们可以推出AVL树适合用于插入删除次数比较少，但查找多的情况。 红黑树：平衡二叉树，通过对任何一条从根到叶子的简单路径上各个节点的颜色进行约束，确保没有一条路径会比其他路径长2倍，因而是近似平衡的。所以相对于严格要求平衡的AVL树来说，它的旋转保持平衡次数较少。用于搜索时，插入删除次数多的情况下我们就用红黑树来取代AVL。（现在部分场景使用跳表来替换红黑树，可搜索“为啥 redis 使用跳表(skiplist)而不是使用 red-black？”） B树，B+树：它们特点是一样的，是多路查找树，一般用于数据库系统中，为什么，因为它们分支多层数少呗，都知道磁盘IO是非常耗时的，而像大量数据存储在磁盘中所以我们要有效的减少磁盘IO次数避免磁盘频繁的查找。B+树是B树的变种树，有n棵子树的节点中含有n个关键字，每个关键字不保存数据，只用来索引，数据都保存在叶子节点。是为文件系统而生的。b+树是b树的一个变种，只在leaf node存储数据，可以方便地遍历所有数据。b树相较于b+树可以把热点数据放在internal node中以便更快地查找。 B+树本来就是为 SQL 实现的，它比 B 树的优势在于范围查询更快，因为数据都在叶子节点上，所有的叶子节点又在同一底层上。 磁盘中的B+树以文件的形式将整体都存放磁盘当中，使用时只在内存中缓存部份结构。 先了解下相关的硬件知识，才能很好的了解为什么需要B~tree这种外存数据结构。 B-tree就是指的B树。 B+-tree的查询效率更加稳定 由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 数据库索引采用B+树的主要原因是 B树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。正是为了解决这个问题，B+树应运而生。B+树只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作（或者说效率太低）。 Trie树：又名单词查找树，一种树形结构，常用来操作字符串,比如在百度搜索时，会给出提示。它是不同字符串的相同前缀只保存一份。相对直接保存字符串肯定是节省空间的，但是它保存大量字符串时会很耗费内存（是内存）。类似的有前缀树(prefix tree)，后缀树(suffix tree)，radix tree(patricia tree, compact prefix tree)，crit-bit tree（解决耗费内存问题），以及前面说的double array trie。简单的补充下我了解应用前缀树：字符串快速检索，字符串排序，最长公共前缀，自动匹配前缀显示后缀。后缀树：查找字符串s1在s2中，字符串s1在s2中出现的次数，字符串s1,s2最长公共部分，最长回文串。radix tree：linux内核，nginx。]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>DataStructure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机通过桥接模式上网（补充）]]></title>
    <url>%2F2018%2F03%2F24%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%80%9A%E8%BF%87%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F%E4%B8%8A%E7%BD%91%EF%BC%88%E8%A1%A5%E5%85%85%EF%BC%89%2F</url>
    <content type="text"><![CDATA[虚拟机通过桥接模式上网（补充）（上次设置的桥接模式并不具备上外网的能力，这次重新梳理了一下过程） 通过brctl show命令可以看到原来已经有网桥br1，我想通过brctl delbr br1试着删除该网桥，但是提示不能删除，经过分析，原因可能有二，其一还有网卡挂在其上，其二，该网桥没有关闭，执行ifconfig br1 down命令关闭网桥,然后再进行删除。 重新建立一个网桥，brctl addbr br-ex（让linux知道网桥，首先告诉它，我们想要一个虚拟的以太网桥接口） 把物理网卡enp11s0挂载到网桥上，brctl addif br-ex enp11s0（以太网物理接口变成了网桥上的逻辑端口。那物理接口过去存在，未来也不会消失。） 此时，ifconfig发现网卡和网桥均有ip（不正常，上不去网） 执行删去网卡ip的命令id addr del dev enp11s0 192.168.1.50/24（为什么可以删去，因为物理网卡成了逻辑网桥设备的一部分了，所以不再需要IP地址。虽然不把原有的网卡地址释放掉，网桥也能工作！但是，为了更规范，或者说为了避免有什幺莫名其妙的问题，最好还是按要求做） 更改/etc/network/interface文件，添加配置，static模式 此时再ifconfig发现物理网卡已没有ip，只有br-ex有，此时可以上网。 登录到学校网关（如果登不上，可能DNS解析出现问题，去/etc/resolv.conf更改配置，加上nameserver 172.21.0.21 ，此为学校的DNS服务器） 此时一切完好，服务器设置已经完毕！这时虚拟机即可以通过桥接模式连接外网，但请注意，如果重启服务器的话，这些配置会消失，因为这些事通过命令行设置的，如果想永久生效的话，可能需要通过配置文件进行修改（我并没有尝试）注：ping 172.21.4.254可以检查是否可以访问外网。 下图显示的是一些配置网桥常用的命令 注意的问题： brctl show中有STP选项，这个其实是生成树协议！因为实验室目前只有一个路由器，是绝对不可能形成一个环的。可以关闭这个功能，这样也可以减少网络环境的数据包污染。 服务器可以上网但是ping百度不通，可能是由于很多网站在服务器都设置了阻止ping数据包，一是安全考虑，二是如果每天都有大量的ping数据包向服务器请求，那会给服务器带来很大负担，如果人家恶意向你的服务器发送大量这种数据包，你的服务器将会无暇顾及其他数据包，那你的网站别人将无法访问，或者打开非常慢，经过测试，确实是这样的！]]></content>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机通过代理上外网]]></title>
    <url>%2F2018%2F03%2F24%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%80%9A%E8%BF%87%E4%BB%A3%E7%90%86%E4%B8%8A%E5%A4%96%E7%BD%91%2F</url>
    <content type="text"><![CDATA[虚拟机通过代理连接外网 因为实验室的服务器没有连接外网，而如果又需要连网的需求，那么可以通过代理的方式，有一款软件CCProxy，需要有校园IP ，服务器的ip网段172.21.4.x，校园网的ip网段为172.21.6.x， 所以虚拟机可以通过有校园网IP的电脑作为Proxy。 下载CCproxy，主界面如下，点击账号管理。 新建一个用户 注意：输入用户名，勾选密码，自定义一个密码，取消勾选IP地址/IP地址段 回到主界面，点击设置，按如下方式设置，注意“请选择本机局域网IP地址默认为自动检测，如果虚拟机不能联网，可尝试手动修改其为本机当前IP地址” 在虚拟机中输入如下命令：export http_proxy=http://username:passwd@ip:port 之后就可以使用apt-get 等功能安装软件包，升级系统。注意：代理设置只在当前ssh连接有效，如果你断开过ssh连接，重新连接后，需要重新在ubuntu中通过命令设置代理。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell知识点]]></title>
    <url>%2F2018%2F03%2F18%2FShell%E7%9F%A5%E8%AF%86%E7%82%B9-1%2F</url>
    <content type="text"><![CDATA[使用read读取行 read可以一次读取所有的值到多个变量里，使用$IFS分割，比如应用于/etc/passwd文件。 12345#!/bin/bashwhile IFS=: read user pass uid gid fullname homedir Shelldo echo $userdone &lt; /etc/passwd 当到文件末尾时，read会以非0退出，终止while循环。可能会觉得把/etc/passwd的重定向放到末尾有些奇怪，不过这是必须的。如果这样写，就不会终止！因为每次循环，Shell都会打开/etc/passwd一次，且read只读取文件的第一行。 12345#!/bin/bashwhile IFS=: read user pass uid gid &lt; /etc/passwd fullname homedir Shelldo echo $userdone make 1&gt; results 2&gt; ERRS 该命令将标准错误输出到ERRS，将标准输出传给results。 命令替换：就是指Shell执行命令并将命令替换为执行该命令后的结果。可以用两个反引号，不过这种方式容易混淆，所以还有一种方式就是使用$(...) ,现在多用此。 1234567$# 传递到脚本的参数个数$* 以一个单字符串显示所有向脚本传递的参数$$ 脚本运行的当前进程ID号$! 后台运行的最后一个进程的ID号$@ 与$#相同，但是使用时加引号，并在引号中返回每个参数。$- 显示Shell使用的当前选项，与set命令功能相同。$? 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误]]></content>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机IP]]></title>
    <url>%2F2018%2F03%2F15%2F%E8%99%9A%E6%8B%9F%E6%9C%BAIP%2F</url>
    <content type="text"><![CDATA[如何不进去虚拟机看到其IP 在linux上玩过kvm的朋友基本都晓得，在宿主机上运行了虚拟主机以后，我们无法直接看到某一个虚拟主机IP地址。比如 如果我们想知道test这个虚拟机的IP地址，那么是无法直接看到的。但是我们可以通过一个小技巧间接得到其IP，也就是利用ARP。 编辑虚拟主机配置文件。查看虚拟机的配置文件test.xml，（或者通过virsh dumpxml test）找到MAC标签，可以得到该虚拟机的MAC地址，记录下mac后退出，然后通过arp -a判定虚拟机IP地址，由此可以发现此虚拟机IP为192.168.122.118，由此实现了不登陆虚拟机即可得到其IP。 (注意这里一定要加上-i 忽略大小写。不然因为大小写问题有可能查不到)说明：这里只根据通信缓存记录的mac 、IP地址手段做排查。也有可能找不到。最好的办法是自己写一个脚本跟网段内的所有服务器都ping一次，记录下mac、ip地址以后再查找就没问题,因为这个方法就根据ARP缓存得到的。 虚拟机指定固定IP (方法1) 由于bridge模式默认创建虚拟机的时候用的DHCP方式，分配给虚拟机的IP可能会发生改变，有时候很麻烦。那么如何指定其IP呢？进去虚拟机，编辑/etc/network/interface文件，由变成然后输入 systemctl restart networking或者实在不行重启，即可更改为192.168.1.111的地址。 虚拟机指定固定IP (方法2) 把MAC与IP之间进行绑定,配置DHCP的配置文件 12345678910111213141516171819202122232425262728293031323334353637# dhcpd.conf## Sample configuration file for ISC dhcpd# # Use this to enble / disable dynamic dns updates globally.ddns-update-style none; ignore client-updates; # If this DHCP server is the official DHCP server for the local# network, the authoritative directive should be uncommented.#authoritative; # Use this to send dhcp log messages to a different log file (you also# have to hack syslog.conf to complete the redirection).#log-facility local7; # No service will be given on this subnet, but declaring it helps the# DHCP server to understand the network topology.# This is a very basic subnet declaration. # A slightly different configuration for an internal subnet.subnet 192.168.0.0 netmask 255.255.255.0 &#123; range 192.168.0.30 192.168.0.39; option domain-name-servers 192.168.0.31; option domain-name &quot;wan.hust.china&quot;; option routers 192.168.0.1; option broadcast-address 192.168.0.255; default-lease-time 21600; max-lease-time 43200; host pc001 &#123; hardware ethernet 66:66:66:66:66:0b; fixed-address 192.168.0.88; &#125;&#125; 如今就能够通过在创建虚拟机时指定MAC地址来间接指定IP地址了： 1/usr/local/qemu-kemari-v0.2.14/bin/qemu-system-x86_64 -m 1024 /images/test2.img -net nic,mac=66:66:66:66:66:0b -net tap,ifname=tap1,script=/etc/qemu-ifup,downscript=no -vnc :6 -enable-kvm 注意的问题这里要注意：当两台虚拟机都指定同一个与特定IP绑定的MAC地址时。DHCP并不报错。而是给两台虚拟机都分配这个特定的IP。以下是我在网上找到的一些帖子。来解释同一个网段内能够有两台电脑拥有同样的IP和MAC地址： 事实上是能够的，你全然能够把两台电脑的IP 和MAC改成一样。不但能够上网并且还没IP冲突。这样的方法不但能够突破路由封锁用在ADSL共享上网。并且还能够用在IEEE802.1X认证上网的环境中。可是前提必需要用同样的帐号来拨号上网（前提认证server没设验证帐号的反复性）。我的机子是通过学校校园网接入internet的，client採用802.1x认证client软件“STAR Supplicant拨号软件”来拨号上网，在我们学校里能够将两台机子的IP和MAC改成一样然后用同样的一个帐号来达到共享上网的目的，只是在我们学校仅仅能够在同一个宿舍的两台机子才干够共享上网，由于我们学校的server不单止验证帐号,ip,MAC并且还验证接入serverIP（NASIP），和接入serverport（NAS port)，不同的宿舍接在学校交换机不同的port，所以仅仅限于同一个宿舍用这样的法共享上网。 至于为什么不会引起IP冲突并且还能上网,这是由于ARP工作的缺陷，系统之所以会发现网上有相的IP的而提示“IP冲突”，是由于系统在启动时,TCP/IP中的ARP会广播一个免费ARP(free arp)请求包到网段上，这个ARP(free arp)包包括自己的IP和MAC。假设网段上有机子回应了这个包，这台发广播的机子就会觉得局域网有别的机子使用和自己同样的IP。 比如：PC A和PC B的IP和MAC全然一样，PC A的系统启动时会广播一个包括自己IP和MAC的免费ARP(free arp)请求包到网段上，假设PC B回应了这个请求。PC A会觉得自己的IP和网络上的IP有冲突并发出提示（这就是为什么IP冲突一般发生系统刚启动完毕时），问题是PC B根本不会回应这个请求包。这是由于这个请求包的IP和MAC和PC B自己的全然一样，而PC B会觉得是自己发的包。所以不会回应，既然不会回应自然不会发生IP冲突了。 好了。让我来解释下一个问题。就是两台机子的IP和MAC一样究竟会不会导致不能上网： 既然能够。那么网络上的硬件设备是如何区份这些数据究竟是哪台机的呢？？大家都知道局域网内是用硬件地址来通迅的，局域网的二层设备(如交换机）维护着一张地址表，地址表记录着本设备每一个port所相应的MAC（注：不是port的MAC，而是port所连设备的MAC）。设备要经过地址学习状态才干知道这些port所相应的MAC，当一个帧经过设备的某介port时，设备会检查该帧源地址和目的地址,然后再对比自己的地址表，看地址表中是否存在该源地址的相应项。若不存在则port会变为“地址学习状态”，将该地址保存在地址表中组成一个新的表项。假设PCA和PCB都连在同一个交换机上。则交换机经过“地址学习状态”后，地址表中存在两个同样的地址项，只是它们所相应的port是不同的。当交换机在外部接收到一个目的地址为该地址（PCA和PCB同样的MAC地址）的帧时，则会检查地址表。检查地址表后会发现存在两个同样地址的表项。于是交换机会将该帧转发到这两个表项所相应的port，(至于交换机是用组播的方式还是说用一个帧发两遍的方式转发给这两个port我就不太清楚了)。 路由器也一样。不同是的路由器的地址表是路由表。存放的是IP而不是硬件地址。 连接这两个port的PCA和PCB都会收到相同的帧，既然会收到相同的帧，那么计算机如何才知道哪些帧才是自己想要的呢？这取决于工作在TCP/ip上层协议。尽管网卡是接收了这个帧，可是上层的协议进行进一步的分用，也能够说成是过滤，当TCP/IP的网络接口层(也叫链路层)收到一个帧，会检查帧头中的帧类型，假设是ARP类型的就交给ARP协议去处理，假设是RARP类型就会交给RARP协议处理。假设是IP类型会去掉帧头并把这个帧传给上一层（即网络层来处理）。网络层会依据包头（去掉帧头就叫IP包了）中的协议类型来分用。如是TCMP类型就交给ICMP协议处理。假设是IGMP类型就交给IGMP协议处理。假设是TCP或UDP就把包头去掉并交给上一层（即传输层）来片理，去掉IP包头后就叫做报文分段了（传输层的单位），相同传输层也会对报文分段的头部进行检查从而进行进一步的分用，假设是TCP类型的交给TCP协议处理，假设是UDP类型就交给UDP协议处理，TCP或UDP会依据报文分段的头部中的“目的port号”来交给应用层（交给应用层前会把报文分段的头部去掉），然后应用层的用户进程会依据该“port号”来决是否接收这个数据，比如QQ某个进程打开了UDP 1324这个port，传输层的UDP协议会把全部接收到的且“目的port号”为1324的报文分段交给QQ的这个进程。 这样就完毕接收数据的整个过程。尽管两台电脑都会接收到不是属于自己的数据帧。可是在把帧交给上层协议片理时有可能会被丢充。就如应用层的QQ进程不会接到除“目的port号”为1324以外的其他数据包，由于这些数据在应用层前已经被丢弃。 在同一个局域网内理论上是同意同样MAC地址存在的。为什么使用同样的MAC地址的PC都能在同一网段内执行呢？首先。我们要明白的是，局域网内的通信是以帧为基础的，也就是我们通常说的MAC地址。而不是IP地址。其次，路由器（特指平时我们家庭用的那种路由，如tplink等）或交换机（如cisco）在局域网内是依据mac-table进行数据的交换。并且这些表都有特定的生存期。不是静态的。 如今如果，有两台PC（PC1和PC2）的MAC地址同样。分别接在路由器（或交换机）的两个端口（port1，port2）上，pc1首先发起连接魔兽游戏的server的请求，那么在路由器（或交换机）上就会在mac-table加入PC1的mac地址到port1上。当魔兽游戏的server反应请求后，路由器也会把信息转发到port1上给PC1.同理。当PC2也要登陆魔兽游戏server时。过程也一样。 可是。路由器的mac-table是动态的，当pc1请求连接并且被路由器记录这个mac地址相应的端口为port1时，pc2突然发起连接魔兽server的请求，那么路由器的mac-table就会更改次MAC地址的相应端口，把这个mac地址的端口改为port2。那么pc1的响应消息就会直接发给PC2,造成pc1不能上网。 当然。发生这样的情况比較少。由于，请求响应都是在几秒甚至几十毫秒内完毕的。 所以这也解释了，为什么当中一台PC接收大量数据时，还有一台会断网的原因啦。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM配置虚拟机桥接（bridge）模式]]></title>
    <url>%2F2018%2F03%2F14%2FKVM%E9%85%8D%E7%BD%AE%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%A1%A5%E6%8E%A5%EF%BC%88bridge%EF%BC%89%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[主机：Ubuntu14.04 64bit虚拟机：Ubuntu14.04 64bitVMM:KVM 基础知识：默认创建的网桥virbr0用于NAT模式，如果想用brigde模式，需要自己创建一个br0。Bridge方式即虚拟网桥的网络连接方式，是客户机和子网里面的机器能够互相通信。可以使虚拟机成为网络中具有独立IP的主机。 桥接网络（也叫物理设备共享）被用作把一个物理设备复制到一台虚拟机。网桥多用作高级设置，特别是主机多个网络接口的情况。 修改网络配置文件 /etc/network/interfaces ，此处是DHCP方式，也可以static方式。（Ubuntu系统，centos在etc/sysconfig里）输入 brctl addbr br0 建立一个网桥br0，此时在Linux内核里创建虚拟网卡br0，enp11s0 是host主机的网卡。 重新启动网络服务便可重启计算机或者sudo systemctl restart networking / sudo systemctl restart network-manager，此时输入ifconfig，显示如图。（据说br0与enp11s0只能有一个有ip地址，此处不是很明白，但是目前并不影响使用。） 输入该命令，可以看到此时已有br0的网桥。 创建虚拟机时，指定—bridge=br0。（–bridge与—network只能二选一，不能同时指定） 1virt-install --virt-type kvm --name test2 --ram 1024 --vcpus 1 --bridge=br0 \ --cdrom=/home/lib206/vmimage/ubuntu-16.04.1-server-amd64.iso \--disk path=/home/lib206/vmimage/Test.img,size=20,format=raw --graphics vnc,port=5901 --os-type=linux --boot cdrom 创建成功后，如果启动虚拟机时出现不能引导Booting from No bootable device，按下图所示设置，可能是引导顺序的原因。 此时虚拟机即处于桥接模式，ip由路由器DHCP随即分配。通过以上步骤的设置KVM的桥接问题解决了，此时可以查看一下虚拟机网络IP地址，应该跟主机的IP地址处于同一个网段；但是还是有问题的， 无线网卡桥接是不成功的，默认的是有线网卡！ 如果想指定该虚拟机的ip地址，按另一篇文章设置。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell知识点]]></title>
    <url>%2F2018%2F03%2F11%2FShell%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[/dev/null 位桶（bit bucket）传送到此文件的数据会被丢掉,可以用来测试一个文件是否包含某个pattern。 1234if grep pattern myfile &gt; /dev/nullthen ... 找到模式else ...没找到模式fi Shell脚本的命令行参数超多9时，用数字框起来${10} ^$ 用来匹配空字符串或行列，cc -E foo.c | grep -v &#39;^$&#39;&gt; foo.out 用来排除空行 实时查看当前进程中使用的shell种类ps | grep $$ | awk &#39;{print $4}&#39; 正则表达式对于程序执行时的locale环境相当敏感；方括号表达式里的范围应避免使用，该用字符集，例如[[:alnum:]]比较好。 -k选项指定排序字段，-t选择字段界定符 12345678910sort -t_ -k1,1 -k2,2 &lt;&lt; EOF&gt; one_two&gt; one_two_three&gt; ont_two_four&gt; ont_two_five&gt; EOFone_twoone_two_threeont_two_fiveont_two_four 可以看出sort并不稳定，因为输入与输出不一致，但我们可以通过–stable选项补救该问题。 为什么看不到/etc/passwd,因为默认是隐藏的。 awk与cut是提取字段的好工具。 awk -F: &apos;{print $5}&apos; or cut -d: -f5 2&gt; /dev/null 是丢弃标准错误信息的输出 myvar=赋值并不会将myvar删除，只不过是将其设为null字符串，unset myvar则会完全删除它。 rm -fr /$MYPROGRAM 若MYPROGRAM未定义，就会有灾难发生！！！ POSIX标准化字符串长度运算符：返回字符长度。 12x=supercalifraglistcexpialidoucius 著名的特殊单词echo There are $&#123;#x&#125; characters in $x set命令 如果未給任何选项，会设置位置参数的值，并将之前存在的任何值丢弃。 12345678910111213141516171819202122232425262728293031323334[root@localhost ~]# set -- hello &quot;hi there&quot; greetings[root@localhost ~]# echo there are $# total arguments 计数there are 3 total arguments[root@localhost ~]# for i in $*&gt; do echo i is $i&gt; donei is hello 注意内嵌的空白已经消失i is hii is therei is greetings[root@localhost ~]# for i in $@ 没有双引号 $*和$@一样&gt; do echo i is $i&gt; donei is helloi is hii is therei is greetings[root@localhost ~]# for i in &quot;$*&quot;&gt; do echo i is $i&gt; donei is hello hi there greetings[root@localhost ~]# for i in &quot;$@&quot;; do echo i is $i; donei is helloi is hi therei is greetings[root@localhost ~]# shift 截去第一个参数[root@localhost ~]# echo there are now $# total argumentsthere are now 2 total arguments 证明消失[root@localhost ~]# for i in &quot;$@&quot;; do echo i is $i; donei is hi therei is greetings[root@localhost ~]# shift[root@localhost ~]# echo there are now $# total argumentsthere are now 1 total arguments 特殊变量$$ 可在编写脚本时用来建立具有唯一性的文件名，多半是临时的，根据Shell的进程编号建立文件名，不过，mktemp也能做。 set -e当命令以非零状态退出时，则退出shell。主要作用是，当脚本执行出现意料之外的情况时，立即退出，避免错误被忽略，导致最终结果不正确。说明set -e 选项对set.sh起作用。脚本作为一个进程去描述set -e选项的范围应该是：set -e选项只作用于当前进行，不作用于其创建的子进程。set -e 命令用法总结如下：1.当命令的返回值为非零状态时，则立即退出脚本的执行。2.作用范围只限于脚本执行的当前进行，不作用于其创建的子进程。3.另外，当想根据命令执行的返回值，输出对应的log时，最好不要采用set -e选项，而是通过配合exit 命令来达到输出log并退出执行的目的。 shell 脚本中set-x 与set+x的区别linux shell 脚本编写好要经过漫长的调试阶段，可以使用sh -x 执行。但是这种情况在远程调用脚本的时候，就有诸多不便。又想知道脚本内部执行的变量的值或执行结果，这个时候可以使用在脚本内部用 set -x 。set去追踪一段代码的显示情况，执行后在整个脚本有效，set -x 开启，set +x关闭，set -o 查看 shell if条件判断中的-z到-d的意思 123456789101112131415161718192021222324252627282930313233[ -a FILE ] 如果 FILE 存在则为真。[ -b FILE ] 如果 FILE 存在且是一个块特殊文件则为真。[ -c FILE ] 如果 FILE 存在且是一个字特殊文件则为真。[ -d FILE ] 如果 FILE 存在且是一个目录则为真。[ -e FILE ] 如果 FILE 存在则为真。[ -f FILE ] 如果 FILE 存在且是一个普通文件则为真。[ -g FILE ] 如果 FILE 存在且已经设置了SGID则为真。[ -h FILE ] 如果 FILE 存在且是一个符号连接则为真。[ -k FILE ] 如果 FILE 存在且已经设置了粘制位则为真。[ -p FILE ] 如果 FILE 存在且是一个名字管道(F如果O)则为真。[ -r FILE ] 如果 FILE 存在且是可读的则为真。[ -s FILE ] 如果 FILE 存在且大小不为0则为真。[ -t FD ] 如果文件描述符 FD 打开且指向一个终端则为真。[ -u FILE ] 如果 FILE 存在且设置了SUID (set user ID)则为真。[ -w FILE ] 如果 FILE 如果 FILE 存在且是可写的则为真。[ -x FILE ] 如果 FILE 存在且是可执行的则为真。[ -O FILE ] 如果 FILE 存在且属有效用户ID则为真。[ -G FILE ] 如果 FILE 存在且属有效用户组则为真。[ -L FILE ] 如果 FILE 存在且是一个符号连接则为真。[ -N FILE ] 如果 FILE 存在 and has been mod如果ied since it was last read则为真。[ -S FILE ] 如果 FILE 存在且是一个套接字则为真。[ FILE1 -nt FILE2 ] 如果 FILE1 has been changed more recently than FILE2, or 如果 FILE1 exists and FILE2 does not则为真。[ FILE1 -ot FILE2 ] 如果 FILE1 比 FILE2 要老, 或者 FILE2 存在且 FILE1 不存在则为真。[ FILE1 -ef FILE2 ] 如果 FILE1 和 FILE2 指向相同的设备和节点号则为真。[ -o OPTIONNAME ] 如果 shell选项 “OPTIONNAME” 开启则为真。[ -z STRING ] “STRING” 的长度为零则为真。[ -n STRING ] or [ STRING ] “STRING” 的长度为非零 non-zero则为真。[ STRING1 == STRING2 ] 如果2个字符串相同。 “=” may be used instead of “==” for strict POSIX compliance则为真。[ STRING1 != STRING2 ] 如果字符串不相等则为真。[ STRING1 &lt; STRING2 ] 如果 “STRING1” sorts before “STRING2” lexicographically in the current locale则为真。[ STRING1 &gt; STRING2 ] 如果 “STRING1” sorts after “STRING2” lexicographically in the current locale则为真。[ ARG1 OP ARG2 ] “OP” is one of -eq, -ne, -lt, -le, -gt or -ge. These arithmetic binary operators return true if “ARG1” is equal to, not equal to, less than, less than or equal to, greater than, or greater than or equal to “ARG2”, respectively. “ARG1” and “ARG2” are integers. Shell变量表达式 举个栗子：12345678910111213#!/bin/bashstr=&quot;a b c d e f g h i j&quot;echo &quot;the source string is &quot;$&#123;str&#125; #源字符串echo &quot;the string length is &quot;$&#123;#str&#125; #字符串长度echo &quot;the 6th to last string is &quot;$&#123;str:5&#125; #截取从第五个后面开始到最后的字符echo &quot;the 6th to 8th string is &quot;$&#123;str:5:2&#125; #截取从第五个后面开始的2个字符echo &quot;after delete shortest string of start is &quot;$&#123;str#a*f&#125; #从开头删除a到f的字符echo &quot;after delete widest string of start is &quot;$&#123;str##a*&#125; #从开头删除a以后的字符echo &quot;after delete shortest string of end is &quot;$&#123;str%f*j&#125; #从结尾删除f到j的字符echo &quot;after delete widest string of end is &quot;$&#123;str%%*j&#125; #从结尾删除j前面的所有字]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[qemu-kvm-io线程]]></title>
    <url>%2F2018%2F03%2F11%2Fqemu-kvm-io%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[qemu-kvm一般会有4个线程，主线程是io thread，还有一个vcpu thread，一个signal thread。对于io thread，可以从kvm_main_loop(qemu-kvm.c)开始看：两个pipe是比较重要的一点： 它调用qemu_eventfd创建一个event pipe，pipe read fd用qemu_set_fd _hadnler2注册一个read fd handler，也就是说如果fd read ready，io thread将通过某种机制调用这个handler，即io_thread_wakeup。换句话说，这个event pipe读的操作应该是在io_thread_wakeup()函数中，那么是由谁执行写操作的呢？这个还不是很清楚，只知道write fd记录到了全局变量io_thread_fd中，也就是说在qemu-kvm任何地方都可以引用io_thread_fd向pipe写东西。 它还调用qemu_signalfd创建一个signal thread，不过现在还不清楚为什么要单独创建一个这样的线程。qemu_signalfd中还会创建一个signal pipe，read fd同样用qemu_set_fd _hadnler2注册一个read fd handler——sigfd_handler，而write fd作为参数传递给signal thread执行函数，也就是说写操作应该是由signal thread负责的。为什么要创建一个signal thread和signal pipe呢？ 上面提到用qemu_set_fd_handler2注册的handler在fd ready时，io thread会通过某种机制调用相应地handler。这个机制是main_loop_wait函数实现的。main_loop_wait函数中将所有注册的r/w fd都加入到对应地r/w fd_set中，然后调用select系统调用，这个系统调用将检查相应fd_set中的fd是否有已ready的，如果没有一个ready，将使得调用线程阻塞直到超时，否则将返回，并将原来fd_set修改，将没有ready的fd从原fd_set中删除。返回后，通过遍历handler队列，检测其fd是否在fd_set中，并调用相应的handler。 我记得中说过，发送给这个qemu process的信号可以由process内任意的thread来处理， 但是每个线程都可以设置mask来选择是否处理某信号。那么为什么要单独设置一个signal thread来接受信号呢？实际上，用signal thread这种机制是将原本对指定信号的处理由异步方式改成同步方式。 默认异步方式时，信号发送给本qemu process后，内核将检测是否有pending signal，如果有内核将调用相应的handler，而这个执行实体可以是process中任意的thread。这样，thread有可能因为信号的到来，而被临时打断，去执行handler。 同步方式是这样的，首先sigprocmask系统调用屏蔽掉指定的signal，这样process将不再接受这个signal(当然，每个thread都有屏蔽码的，但是应该是调用pthread_sigmask，调用sigprocmask将使整个process不接受)。然后，创建一个signal thread，其执行代码调用sigwaitinfo函数，等待指定的信号，如果无信号则signal thread阻塞在这个系统调用中，否则返回，将siginfo_t写入signal pipe中。最后，io thread通过select系统调用检测到signal pipe read_fd ready，然后调用相应的read_fd handler，fd handler再调用相应的signal handler。 可以看到，同步方式虽然代价比异步方式大得多，需要多创建一个signal pipe和signal thread，但是它不会突然打断thread执行，对于程序响应性有利。这个跟interrupt和polling之间的比较类似，当外部事情来时，执行内核的中断服务例程，interrupt handler有可能发送一个信号给要处理事件的process。 之前发现一个很奇怪的现象：在我的电脑上跑的qemu-kvm有4个线程，而其他同学电脑上跑只有2个线程。我原本觉得奇怪，按照前面的论述，qemu-kvm至少得有3个线程才是，只有2个线程的话，那么signal thread去哪里了呢？带着这个问题，对两种情况的代码做了一下trace，结果发现是在qemu_signalfd (compatfd.c)函数中出现了差异—如果系统有定义CONFIG_SIGNALFD宏，那么就不会创建signal thread，否则就创建。不太理解的是，我的装的是32位CentOS，结果没有SIGNALFD，而其他装的是64位CentOS，是有SIGNALFD的。那么signalfd是什么呢？简单的说，就是一个线程可以创建一个signalfd，指定哪些signal可以由它来接受，当信号来时，就不是像原来那样去调用signal handler，而是将siginfo_t写入signalfd，线程通过read或select或poll就可以知道signalfd是否接受到了新的signal。这样，在由signalfd情况下，就没有必要创建signal thread了。这样，就减少了一点开销。原来是signal thread通过sigwaitinfo检测到signal，然后将siginfo_t写入signal pipe，io thread通过select检测到signal pipe read fd ready，然后读出siginfo_t，再调用handler；现在是，内核直接将siginfo_t写入signalfd，io thread通过select检测到signalfd ready，然后读出读出siginfo_t，再调用handler。很显然，省去了额外signal thread的开销。总之，只要先用sigprocmask将可能接受的signal block掉，再用select+signalfd或者是sigwaitinfo都可以讲异步signal转变为同步signal。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
      </tags>
  </entry>
</search>
